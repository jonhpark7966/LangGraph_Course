{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Contents</p> <ul> <li>Concept guide</li> <li> <p>LangGraph</p> <ul> <li>Agentic System</li> <li>Tool</li> <li>Memory</li> <li>Human-in-the-loop</li> <li>Customizing</li> </ul> </li> <li> <p>Agentic Patterns</p> <ul> <li>Reflection</li> <li>Planning</li> <li>Multi-Agent</li> </ul> </li> <li> <p>RAG</p> <ul> <li>Self RAG reflection</li> <li>Adaptive RAG</li> </ul> </li> </ul>"},{"location":"#1-day-course-curriculum","title":"1-Day Course Curriculum","text":"<ul> <li> <p>\ud559\uc2b5 \ubaa9\ud45c \u00a0  - Agentic \uad6c\ud604 \uae30\ubc95\uc744 \ud559\uc2b5\ud558\uc5ec \uc790\uc720\ub3c4 \ub192\uc740 LLM \uc5b4\ud50c\ub9ac\ucf00\uc774\uc158 \uc791\uc131 \u00a0  - LangGraph \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc0ac\uc6a9\ubc95 \uc2b5\ub4dd</p> </li> <li> <p>\ucee4\ub9ac\ud058\ub7fc</p> <ul> <li>(1H) \uc778\ud2b8\ub85c: Agentic \uc2dc\uc2a4\ud15c \ucee8\uc149<ul> <li>LLM\uc744 \ub514\uc2dc\uc804 \uba54\uc774\ucee4\ub85c \uc0ac\uc6a9\ud558\uae30</li> <li>\uc0ac\ub840\ud0d0\uad6c: OpenAI o1 \ubaa8\ub378, ChatGPT\uc758 \ub3c4\uad6c\ub4e4</li> </ul> </li> <li>(2H) LangGraph \ub85c Agent \uad6c\ud604\ud558\uae30<ul> <li>Tool \uc950\uc5b4\uc8fc\uae30 (ex. Web Browsing)</li> <li>\uae30\uc5b5\ub825 \ucd94\uac00\ud558\uae30</li> <li>Human in the loop, \uc0ac\ub78c\uc774 \uc9c1\uc811 \uac1c\uc785\ud558\ub3c4\ub85d \uc694\uccad\ud558\uae30</li> <li>Agent State \ucee4\uc2a4\ud130\ub9c8\uc774\uc9d5</li> </ul> </li> <li>(2H) Multi-agent<ul> <li>\uc5ec\ub7ec Agent\ub4e4\uc774 \uac01\uc790 \uc791\uc5c5\ud558\uace0, \uacb0\uacfc\ub97c \ud569\uccd0 \ud655\uc778\ud558\uae30 (Map-Reduce)</li> <li>LangGraph Studio \uc0ac\uc6a9\ud558\uae30</li> </ul> </li> <li>(2H) Adaptive RAG<ul> <li>Reflection, LLM\uc758 \uc140\ud504 \uccb4\ud06c, \uad50\uc815</li> <li>Agentic RAG, Agent \ud328\ud134\uc744 \ud65c\uc6a9\ud558\uc5ec RAG \uac1c\uc120\ud558\uae30</li> </ul> </li> </ul> </li> </ul>"},{"location":"intros/Agent/","title":"Agentic Application","text":"<p>\uc5d0\uc774\uc804\ud2b8 (Agent) \ub294 LLM (\ub610\ub294 LMM, FM) \uc744 \uc798 \uc0ac\uc6a9\ud558\ub294 \uac00\uc7a5 \ub300\ud45c\uc801\uc778 \ubc29\ubc95\uc785\ub2c8\ub2e4. \uae00\uc744 \uc791\uc131\ud558\uace0 \uc788\ub294 \ud604\uc2dc\uc810 \uae30\uc900\uc758 (24.09) ChatGPT \uc11c\ube44\uc2a4\ub3c4 \ud558\ub098\uc758 Agent \ub77c\uace0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc0ac\uc6a9\uc790\uc640 \ub300\ud654\ub97c \ud558\uba74\uc11c \ud544\uc694\uc5d0 \ub530\ub77c \uc6f9\ube0c\ub77c\uc6b0\uc9d5\ub3c4 \ud558\uace0, \ucf54\ub4dc\ub97c \uc791\uc131\ud574\uc11c \uc218\ud589\ud574\ubcf4\uae30\ub3c4 \ud558\uace0, dalle \ub97c \ud1b5\ud574 \uadf8\ub9bc\uc744 \uadf8\ub824\uc8fc\uae30\ub3c4 \ud558\uc8e0. GPT \ub77c\ub294 LLM \uc5d0 Agent \uac1c\ub150\uc774 \ucd94\uac00\ub418\uc5b4 ChatGPT\ub77c\ub294 \ubcf4\ub2e4 \ub611\ub611\ud55c \uc11c\ube44\uc2a4\ub97c \ub9cc\ub4e0 \uac83\uc785\ub2c8\ub2e4.  </p> <p>LangGraph \uc5d0\uc11c\ub294 \uc774\ub97c Agent \ub77c\uace0 \uce6d\ud558\uae30 \ubcf4\ub2e4\ub294 \"Agentic\" \ud55c \uc2dc\uc2a4\ud15c\uc774\ub2e4 \ub77c\uace0 \ud45c\ud604\ud569\ub2c8\ub2e4. \uba85\ud655\ud55c Agent \ub77c\ub294 \uae30\uc900\uc740 \uc5c6\uace0, LLM \uc5d0 Agentic \ud55c \ub3d9\uc791\ub4e4\uc744 \ucd94\uac00\ud55c \uc2dc\uc2a4\ud15c \ub4e4\uc774 \uc788\ub294 \uac83\uc774\uc8e0.  </p> <p>Agentic \ud55c \uc2dc\uc2a4\ud15c\uc740 LLM \uc774 \uc9c1\uc811 \ud589\ub3d9\uc744 \ud310\ub2e8\ud558\uace0 \uc218\ud589\ud558\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \ub300\ud45c\uc801\uc73c\ub85c\ub294 \uc544\ub798\uc640 \uac19\uc740 \uc608\uc2dc\uac00 \uc788\uaca0\ub124\uc694. - LLM \uc744 \uc774\uc6a9\ud55c \ub77c\uc6b0\ud305  - LLM \uc774 \uc9c1\uc811 \uc5b4\ub5a4 \ub3c4\uad6c\ub97c \uc0ac\uc6a9\ud560\uc9c0 \uacb0\uc815 - \uc0dd\uc131\ub41c \ub2f5\ubcc0\uc774 \ucda9\ubd84\ud55c\uc9c0 \uc544\ub2cc\uc9c0 LLM \uc774 \ub2e4\uc2dc \ud310\ub2e8</p> <p>\uc608\ub97c \ub4e4\uc790\uba74, \uc0ac\uc6a9\uc790\uac00 \"\uc9c0\uad6c \ud45c\uba74\uc801 \uad6c\ud574\ubd10\" \ub77c\uace0 \ud588\uc744 \ub54c, LLM \uc774 \"xxx \uc774\ub2e4\" \ub77c\uace0 \ub300\ub2f5\ud588\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc774 \ub2f5\ubcc0\uc744 LLM \uc774 \ub2e4\uc2dc \uc77d\uc5b4\ubcf4\uace0 \ud2c0\ub838\ub294\uc9c0 \ud310\ub2e8\ud558\uace0, \uc6f9\ube0c\ub77c\uc6b0\uc9d5\uc744 \ud560\uc9c0, \uc5b4\ub5a4 \ub370\uc774\ud130\ubca0\uc774\uc2a4\ub97c \ucc3e\uc544\ubcfc\uae30 \uacb0\uc815\ud558\uace0, \ub2e4\uc2dc \uc62c\ubc14\ub978 \ub2f5\ubcc0\uc744 \ud558\ub294 \ud5f9\uc704\uac00 \ubaa8\ub450 Agentic\ud55c \ub3d9\uc791\uc785\ub2c8\ub2e4.  </p> <p>\uc704 \ud589\uc704\ub4e4\uc774 loop \uc744 \ub3cc\uba74\uc11c \ub3d9\uc791\ud55c\ub2e4\uba74, \ub354\uc6b1 Agentic \ud558\ub2e4\uace0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.  </p> <p>LangGraph \uc5d0\uc11c \uc815\uc758\ud55c Agentic \ud55c \ub3d9\uc791\ub4e4\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.  </p> <ul> <li>Tool calling: \ubb34\uc5c7\uc774\ub4e0 (ex. \uc6f9\ube0c\ub77c\uc6b0\uc9d5, API \ud638\ucd9c) \ub2e4\ub978 \ud589\uc704\ub97c \ud558\uace0 \uc635\ub2c8\ub2e4.</li> <li>Memory: \ub300\ud654 \ub0b4\uc6a9\uc774\ub098 \uc55e\uc120 \ub3d9\uc791\ub4e4\uc758 \ub9e5\ub77d \uc815\ubcf4\ub4e4\uc744 \uae30\uc5b5\ud569\ub2c8\ub2e4.</li> <li>Planning: LLM \uc774 \uc5b4\ub5a4 \ub3d9\uc791\uc744 \ud560\uc9c0 \uacc4\ud68d\ud569\ub2c8\ub2e4, \uc774\uac83\ub3c4 LLM \uc774 \uc0dd\uc131\ud569\ub2c8\ub2e4.</li> </ul>"},{"location":"intros/Agent/#langgraph","title":"LangGraph \uc758 \ud2b9\uc9d5","text":"<p>LangGraph\ub294 Agentic \uc2dc\uc2a4\ud15c\uc744 \uc0ac\uc6a9\uc790\ub4e4\uc774 \uc27d\uac8c \ub9cc\ub4e4 \uc218 \uc788\ub3c4\ub85d LangChain \uc5d0\uc11c \ub9cc\ub4e0 \ud504\ub808\uc784\uc6cc\ud06c\uc785\ub2c8\ub2e4.  </p> <p>\ud544\uc790\uc758 \uc758\uacac\uc744 \ucca8\uc5b8\ud558\uc790\uba74, - LangChain\uc774 \uc560\ucd08\uc5d0 LLM application \uc744 \uc798 \ub9cc\ub4e4\uae30 \uc704\ud55c \ud504\ub808\uc784\uc6cc\ud06c\uc774\uace0, \ub2f9\uc5f0\ud788 Agent \uad6c\ud604\uc744 \uc9c0\uc6d0\ud588\uc2b5\ub2c8\ub2e4. - \uadf8\ub7f0\ub370 LangChain \uc758 AgentExecuter \ub294 \uc81c\uc57d\uc774 \ub9ce\uc774 \uc788\uc5b4\uc11c product level \uc5d0\uc11c \uc0ac\uc6a9\ud558\uae30\uac00 \uc5b4\ub824\uc6e0\uc2b5\ub2c8\ub2e4. \ud2b9\ud788 \uc911\uac04 \uac1c\uc785\uc774 \uc548\ub418\uc5b4 \ucd08\uae30 \uad6c\ud604\uc740 \ub9e4\uc6b0 \uc26c\uc6e0\uc73c\ub098, \ub514\ud14c\uc77c\ud55c \ucee8\ud2b8\ub864\uc774 \ubd88\uac00\ud588\uc8e0. - \uc560\ucd08\uc5d0 LangChain\uc774 GPT3 \ucd9c\uc2dc\uc640 ChatGPT \ucd9c\uc2dc \uc0ac\uc774\uc5d0 \ub098\uc628 \ub3c4\uad6c\uc774\uace0, \uc0ac\uc6a9\uc790\ub4e4\uc758 \uc9c4\uc785\uc7a5\ubcbd\uc744 \ub0ae\ucdb0\uc8fc\ub294 \ub3c4\uad6c\uc784\uc744 \uac10\uc548\ud558\uba74 \uc774\ud574\uac00 \ub418\ub294 \uc601\uc5ed\uc785\ub2c8\ub2e4. - \uc0ac\uc6a9\uc790\ub4e4\uc740 \uc88b\uc740 Agentic \uc2dc\uc2a4\ud15c\uc744 \ub9cc\ub4e4\uae30 \uc704\ud574 LangChain \uc744 \uc774\ud0c8\ud588\uace0, \uc774\ub97c \ubcf4\uc644\ud558\uae30 \uc704\ud574 \ucd9c\uc2dc\ud55c \ud504\ub808\uc784\uc6cc\ud06c\uac00 LangGraph \uc785\ub2c8\ub2e4. - LangChain\uc744 \ub354 \uc774\uc0c1 \uc4f0\uc9c0 \uc54a\ub294 \uc774\uc720 - \ud574\ucee4\ub274\uc2a4 \uc4f0\ub808\ub4dc \ub97c \ucc38\uc870\ud558\uc2dc\uba74, \ub2e4\ub978 \uc0ac\uc6a9\uc790\ub4e4\uc758 \uc758\uacac\uacfc LangChain \uc758 \ucc3d\uc5c5\uc790\uc778 Harrison\uc758 \uc0dd\uac01/\ubc29\ud5a5\uc131 \uc744 \ubcf4\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc9c1\uc811 harrison\uc774 \ub313\uae00\ub85c \ud574\uba85(?) \ud558\ub294 \ub0b4\uc6a9\uc744 \ubcf4\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. </p> <p>LangGraph\uc5d0\uc11c \uc8fc\uc7a5\ud558\ub294 \ubcf8\uc778\ub4e4\uc758 \uc7a5\uc810 (Agentic \uc2dc\uc2a4\ud15c\uc744 \ub9cc\ub4e4\uae30 \uc704\ud55c \ud2b9\uc7a5\uc810) \uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.</p> <ul> <li>Controllability</li> <li>Human-in-the-Loop</li> <li>Streaming First</li> </ul>"},{"location":"intros/Agent/#controllabillty","title":"Controllabillty","text":"<p>\uc55e\uc11c \ubf51\uc740 LangChain \uc758 \ubd80\uc871\ud55c \uc810\uc774 \ub514\ud14c\uc77c\ud55c \ucee8\ud2b8\ub864\uc774 \ubcf4\uc644\ub418\uc5b4 \uc654\uc2b5\ub2c8\ub2e4. High-Level API\ub97c \uc9c0\uc6d0\ud558\ub294 LangChain\uacfc \ub2e4\ub974\uac8c, LangGraph\ub294 \"Extremely Low Level\" \uc774\ub77c\uace0 \ud45c\ud604\ud569\ub2c8\ub2e4. \uadf8\ub798\uc11c \uc0ac\uc6a9\uc790 (\uac1c\ubc1c\uc790) \ub294 \uc544\uc8fc \uc790\uc138\ud558\uac8c \ucee8\ud2b8\ub864\uc774 \uac00\ub2a5\ud558\uace0 reliable \ud55c \uc2dc\uc2a4\ud15c\uc744 \uac1c\ubc1c\ud560 \uc218 \uc788\uc8e0. </p>"},{"location":"intros/Agent/#human-in-the-loop","title":"Human-in-the-loop","text":"<p>\uc0ac\ub78c\uc774 \uc911\uac04\uc5d0 \uac1c\uc785\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uac1c\ubc1c\uc790\ub4e4\uc774 \ubf51\uc740 LangChain \uc5d0\uc11c \uac00\uc7a5 \uc544\uc26c\uc6e0\ub358 \uc810 \uc911 \ud558\ub098\uac00 \ubc14\ub85c \uc774 \"\uac1c\uc785\" \uc785\ub2c8\ub2e4. \ub54c\ub54c\ub85c AI \ub294 \uc0ac\ub78c\uc758 \uac1c\uc785\uc744 \uc694\ud569\ub2c8\ub2e4.  \uac80\uc0c9 \uacb0\uacfc\uac00 \uc798 \ub098\uc654\ub294\uc9c0 \uc0ac\ub78c\uc774 \ud655\uc778\uc744 \ud574 \uc8fc\uac70\ub098, \ud639\uc740 \ud398\ub974\uc18c\ub098\ub97c \ubc14\uafb8\ub3c4\ub85d \uc9c0\uc815\uc744 \ud574\uc8fc\uac70\ub098, \uc5ec\ub7ec\uac00\uc9c0 \ubc29\uba74\uc73c\ub85c \ud65c\uc6a9\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. Agent\uc758 \ub3d9\uc791\uc744 \uba48\ucd94\uace0, State\ub97c \ub9ac\ubdf0\ud558\uace0, \uc2b9\uc778/\uac70\uc808\uc744 \ud558\uac70\ub098, \uace0\uce58\uac70\ub098 \uc5ec\ub7ec\uac00\uc9c0 \uc751\uc6a9\uc744 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  </p>"},{"location":"intros/Agent/#streaming","title":"Streaming","text":"<p>Agentic \uc2dc\uc2a4\ud15c\uc740 \ub2f9\uc5f0\ud788 \uc5ec\ub7ec \uc77c\uacfc \uacfc\uc815\uc744 \uac70\uce58\ubbc0\ub85c \uc624\ub798 \uae30\ub2e4\ub824\uc57c\ud560 \uacbd\uc6b0\uac00 \ub9ce\uc2b5\ub2c8\ub2e4. \uc720\uc800\ub294 Agent\uc758 \ud589\uc704\ub97c \uc9c0\ucf1c \ubcfc \uc218 \uc788\uc5b4\uc57c \uaca0\uc8e0. \uc911\uac04\uacfc\uc815\ub4e4\uc744 \uc2a4\ud2b8\ub9ac\ubc0d\ud558\uc5ec \uc774\ub97c \uad00\ucc30\uac00\ub2a5\ud1a0\ub85d \ud569\ub2c8\ub2e4. </p>"},{"location":"intros/Agent/#langgraph-studio","title":"LangGraph Studio","text":"<p>\ud544\uc790\uac00 \ubf51\uc740 LangGraph \uc758 \uac00\uc7a5 \ud070 \uc7a5\uc810\uc785\ub2c8\ub2e4. \u00a0LangGraph Studio\u00a0\ub77c\ub294 IDE\uac00 \uc788\uc2b5\ub2c8\ub2e4. Agent\ub97c \uc704\ud55c UI/UX\uc640 \ucf54\ub4dc\ub97c \uc218\uc815\ud560 \uc218 \uc788\ub294 vscode \uc5f0\ub3d9, \uc190\uc26c\uc6b4 \ubc30\ud3ec \ubc84\ud2bc \uae4c\uc9c0 \uad6c\ube44\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 UI/UX \uac00 \uc815\ub9d0 \ub108\ubb34 \ud3b8\ud569\ub2c8\ub2e4. Agentic \uc2dc\uc2a4\ud15c\uc740 LLM \uc758 \ud310\ub2e8\uc5d0 \ub530\ub77c \uc774\ub9ac\uc800\ub9ac \ub3d9\uc791\uc774 \ub2ec\ub77c\uc9c0\uae30 \ub54c\ubb38\uc5d0, \ub514\ubc84\uae45\uc774 \ub9e4\uc6b0\ub9e4\uc6b0\ub9e4\uc6b0 \ud798\ub4e4\ub354\uad70\uc694. LangGraph \uc2a4\ud29c\ub514\uc624\ub294 Agent\uc758 \ub3d9\uc791\uc744 \uadf8\ub798\ud504 UI \ub85c \ubcf4\uc5ec\uc8fc\uace0, \uc0ac\uc6a9\uc790\uac00 \uc9c1\uc811 State \uc758 \ub370\uc774\ud130\ub97c \ubcc0\uacbd\ud558\ub294 \ucc3d\uc744 \uae30\ubcf8\uc801\uc73c\ub85c \uc81c\uacf5\ud569\ub2c8\ub2e4. IDE\uc5d0 \ubd99\uc5b4\uc788\ub294 \ub514\ubc84\uac70 \uac19\uc8e0. \uc544\uc8fc \uc720\uc6a9\ud569\ub2c8\ub2e4.  </p> <p></p>"},{"location":"tutorial/1_langgraph_start/","title":"LangGraph \uc2dc\uc791\ud558\uae30","text":"In\u00a0[\u00a0]: Copied! <pre>%%capture --no-stderr\n%pip install -U langgraph langsmith\n\n# \ubcf8 \uc2e4\uc2b5\uc744 \uc704\ud574\uc11c \ub2e4\uc6b4\uc744 \ubc1b\uc2b5\ub2c8\ub2e4.\n%pip install -U langchain-openai\n</pre> %%capture --no-stderr %pip install -U langgraph langsmith  # \ubcf8 \uc2e4\uc2b5\uc744 \uc704\ud574\uc11c \ub2e4\uc6b4\uc744 \ubc1b\uc2b5\ub2c8\ub2e4. %pip install -U langchain-openai <p>API key \ub97c \uc14b\ud305\ud569\ub2c8\ub2e4. OPENAI \uc5d0\uc11c \ubc1c\uae09\ubc1b\uc544\uc624\uc138\uc694</p> In\u00a0[\u00a0]: Copied! <pre>import getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")\n</pre> import getpass import os   def _set_env(var: str):     if not os.environ.get(var):         os.environ[var] = getpass.getpass(f\"{var}: \")   _set_env(\"OPENAI_API_KEY\") <pre>OPENAI_API_KEY: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> <p>LangSmith \ub3c4 \uc14b\uc5c5\uc744 \ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc55e\uc73c\ub85c \uc791\uc131\ud560 LangGraph \uc758 \ub3d9\uc791\uc744 \ucd94\uc801\ud558\uc5ec \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>LangSmith \uc5d0 \ub300\ud55c \ub0b4\uc6a9\uc740 \ud29c\ud1a0\ub9ac\uc5bc \ubb38\uc11c \ub97c \ucc38\uc870\ud558\uc138\uc694</p> In\u00a0[\u00a0]: Copied! <pre>os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n\n_set_env(\"LANGCHAIN_API_KEY\")\n</pre> os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"  _set_env(\"LANGCHAIN_API_KEY\") <pre>LANGCHAIN_API_KEY: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[\u00a0]: Copied! <pre>from typing import Annotated\n\nfrom langchain_openai import ChatOpenAI\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph.message import add_messages\n\n\n# 1. State \ub97c \uc815\uc758\ud569\ub2c8\ub2e4.\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\ngraph_builder = StateGraph(State)\n\n# 2. \ub178\ub4dc\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\ndef chatbot(state: State):\n    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n\n# 3. \uadf8\ub798\ud504\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.\n# The first argument is the unique node name\n# The second argument is the function or object that will be called whenever\n# the node is used.\ngraph_builder.add_node(\"chatbot\", chatbot)\ngraph_builder.set_entry_point(\"chatbot\")\ngraph_builder.set_finish_point(\"chatbot\")\ngraph = graph_builder.compile()\n</pre> from typing import Annotated  from langchain_openai import ChatOpenAI from typing_extensions import TypedDict  from langgraph.graph import StateGraph from langgraph.graph.message import add_messages   # 1. State \ub97c \uc815\uc758\ud569\ub2c8\ub2e4. class State(TypedDict):     messages: Annotated[list, add_messages]  graph_builder = StateGraph(State)  # 2. \ub178\ub4dc\ub97c \uc815\uc758\ud569\ub2c8\ub2e4. llm = ChatOpenAI(model=\"gpt-4o-mini\") def chatbot(state: State):     return {\"messages\": [llm.invoke(state[\"messages\"])]}  # 3. \uadf8\ub798\ud504\ub97c \uc815\uc758\ud569\ub2c8\ub2e4. # The first argument is the unique node name # The second argument is the function or object that will be called whenever # the node is used. graph_builder.add_node(\"chatbot\", chatbot) graph_builder.set_entry_point(\"chatbot\") graph_builder.set_finish_point(\"chatbot\") graph = graph_builder.compile() <p>\ucc38\uace0</p> <p>\uac00\uc7a5 \ucc98\uc74c \ud574\uc57c\ud560 \uc77c\uc744 <code>State</code> \ub97c \uc815\uc758\ud558\ub294 \uac83 \uc785\ub2c8\ub2e4. <code>State</code> \ub294 \uadf8\ub798\ud504\uc758 \uc2a4\ud0a4\ub9c8\ub97c \uac00\uc9c0\uace0 \uc788\uace0, \uc0c1\ud0dc\ub97c \uc5c5\ub370\uc774\ud2b8\ub791 \ub9ac\ub4c0\uc11c \ud568\uc218\ub4e4\uc744 \ubd99\uc785\ub2c8\ub2e4.</p> <p>\uc774\ubc88 \uc608\uc2dc\uc5d0\uc11c\ub294 <code>messages</code>\ub9cc \uac00\uc9c0\uace0 \uc788\ub294<code>TypedDict</code> \uac00 <code>State</code> \uc785\ub2c8\ub2e4. <code>messages</code>  \ub294 add_messages \ub9ac\ub4c0\uc11c \ud568\uc218\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. LangGraph \uc5d0\uc11c\ub294 \uc0c8\ub85c\uc6b4 \uba54\uc138\uc9c0\ub97c \ucd94\uac00\ud558\uac70\ub098 \ub36e\uc5b4\uc4f0\ub294 \uac83\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> <p>\uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ucee8\uc149\uac00\uc774\ub4dc\ub97c \ucc38\uace0\ud558\uc138\uc694.</p> <ol> <li>\ubaa8\ub4e0 <code>node</code> \ub294 \uc55e\uc73c\ub85c \ud604\uc7ac <code>State</code> \ub97c \ubc1b\uc544\uc11c \uc5c5\ub370\uc774\ud2b8\ud558\uace0 \ub9ac\ud134\ud569\ub2c8\ub2e4.</li> <li><code>messages</code>\ub294 \ud604\uc7ac \ub9ac\uc2a4\ud2b8\uc5d0 appended \ub420 \uac83\uc785\ub2c8\ub2e4, <code>add_messages</code> \ud568\uc218\ub97c <code>Annotated</code> \uc5d0 \ucd94\uac00 \ud588\uae30 \ub54c\ubb38\uc774\uace0\uc694, \ucc57\ubd07\uc774\ub2c8\uae4c \uba54\uc138\uc9c0\uac00 \uc313\uc5ec\uc57c \uaca0\uc8e0.</li> </ol> <p>\uc774\uc81c \"<code>chatbot</code>\" \ub178\ub4dc\ub97c \ucd94\uac00\ud569\ub2c8\ub2e4.</p> <p><code>chatbot</code> \ub178\ub4dc\ub294 \ud604\uc7ac <code>State</code> \ub97c \uc778\ud48b\uc73c\ub85c \ubc1b\uc544\uc11c \uc5c5\ub370\uc774\ud2b8\ud558\uace0 \ub9ac\ud134\ud569\ub2c8\ub2e4. \uc2e4\uc81c\ub85c\ub294 'State' \uc758 'messages' \uc2a4\ud0a4\ub9c8\uac00 \uc5c5\ub370\uc774\ud2b8\uac00 \ub418\uaca0\uc8e0. LangGraph \uc758 \uae30\ubcf8 \ud328\ud134\uc785\ub2c8\ub2e4.</p> <p>\uc774\uc81c <code>entry</code> \uc640 'finish' \ud3ec\uc778\ud2b8\ub97c \ucd94\uac00\ud574\uc11c \uc2dc\uc791\uc810\uacfc \ub05d\uc744 \ub9cc\ub4e4\uc5b4 \uc90d\ub2c8\ub2e4.</p> <p><code>finish</code> \ud3ec\uc778\ud2b8\ub294 \uadf8\ub798\ud504\uac00 \"\uc5b8\uc81c\ub098 \uc774 \ub178\ub4dc\uac00 \uc218\ud589\uc911\uc77c\ub54c \ub05d\ub0bc \uc218 \uc788\uc74c\uc744 \uc54c\ub9bd\ub2c8\ub2e4. (any time this node is run, you can exit.)\"</p> <p>\"<code>compile()</code>\" \uae4c\uc9c0 \ubd88\ub7ec\uc918\uc57c \uc0ac\uc6a9 \uac00\ub2a5 \ud569\ub2c8\ub2e4. \"<code>CompiledGraph</code>\" \uac00 \ub418\uc5b4\uc57c \ud638\ucd9c\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> <p>\uc2dc\uac01\ud654\ud574\uc11c \uadf8\ub798\ud504\ub97c \ubcfc \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. <code>get_graph</code>&amp; <code>draw</code> \ud568\uc218\ub85c \uadf8\ub824\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Image, display\n\ntry:\n    display(Image(graph.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    pass\n</pre> from IPython.display import Image, display  try:     display(Image(graph.get_graph().draw_mermaid_png())) except Exception:     # This requires some extra dependencies and is optional     pass <p>\uc774\uc81c \uc218\ud589\uc2dc\ucf1c \ubcf4\uc8e0.</p> <p>Tip:  \"quit\", \"exit\", or \"q\". \ub97c \uc785\ub825\ud574\uc11c \ub05d\ub0bc \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>while True:\n    user_input = input(\"User: \")\n    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n        print(\"Goodbye!\")\n        break\n    for event in graph.stream({\"messages\": (\"user\", user_input)}):\n        for value in event.values():\n            print(\"Assistant:\", value[\"messages\"][-1].content)\n</pre> while True:     user_input = input(\"User: \")     if user_input.lower() in [\"quit\", \"exit\", \"q\"]:         print(\"Goodbye!\")         break     for event in graph.stream({\"messages\": (\"user\", user_input)}):         for value in event.values():             print(\"Assistant:\", value[\"messages\"][-1].content) <pre>User: \uc800\ub141 \uba54\ub274 \ucd94\ucc9c \uc880\nAssistant: \uc800\ub141 \uba54\ub274\ub85c \uba87 \uac00\uc9c0 \ucd94\ucc9c\ud574\ub4dc\ub9b4\uac8c\uc694!\n\n1. **\ubd88\uace0\uae30**: \ub2ec\ucf64\ud558\uace0 \uc9ed\uc9e4\ud55c \uc18c\uace0\uae30 \ubd88\uace0\uae30\ub97c \uc308 \ucc44\uc18c\uc640 \ud568\uaed8 \ub4dc\uc2dc\uba74 \uc88b\uc2b5\ub2c8\ub2e4.\n2. **\ube44\ube54\ubc25**: \ub2e4\uc591\ud55c \uc57c\ucc44\uc640 \uace0\uae30\ub97c \ub123\uace0 \uace0\ucd94\uc7a5\uacfc \ud568\uaed8 \ube44\ubcbc \uba39\ub294 \ube44\ube54\ubc25\uc740 \uc601\uc591\uac00\ub3c4 \ub192\uace0 \ub9db\uc788\uc5b4\uc694.\n3. **\uae40\uce58\ucc0c\uac1c**: \uad6c\uc218\ud55c \uae40\uce58\ucc0c\uac1c\uc5d0 \ubc25 \ud55c \uacf5\uae30\uba74 \uc18d\uc774 \ub4e0\ub4e0\ud574\uc9d1\ub2c8\ub2e4.\n4. **\uce58\ud0a8**: \ubc14\uc0ad\ud55c \ud504\ub77c\uc774\ub4dc \uce58\ud0a8\uc774\ub098 \ub9e4\ucf64\ud55c \uc591\ub150 \uce58\ud0a8\ub3c4 \uc88b\uc740 \uc120\ud0dd\uc785\ub2c8\ub2e4.\n5. **\ud574\ubb3c\ud30c\uc804**: \ud574\ubb3c\uacfc \ud30c\uac00 \uac00\ub4dd\ud55c \uc804\uc740 \uac04\ub2e8\ud558\uba74\uc11c\ub3c4 \ub9db\uc788\ub294 \uc800\ub141 \uba54\ub274\uc785\ub2c8\ub2e4.\n6. **\ud30c\uc2a4\ud0c0**: \ud06c\ub9bc \uc18c\uc2a4\ub098 \ud1a0\ub9c8\ud1a0 \uc18c\uc2a4\ub85c \uac04\ub2e8\ud558\uac8c \ub9cc\ub4e4 \uc218 \uc788\ub294 \ud30c\uc2a4\ud0c0\ub3c4 \ucd94\ucc9c\ud574\uc694.\n7. **\uc0d0\ub7ec\ub4dc**: \uac00\ubcbc\uc6b4 \uc800\ub141\uc744 \uc6d0\ud558\uc2e0\ub2e4\uba74 \uc2e0\uc120\ud55c \ucc44\uc18c\uc640 \ub2e8\ubc31\uc9c8\uc744 \uacc1\ub4e4\uc778 \uc0d0\ub7ec\ub4dc\ub3c4 \uc88b\uc2b5\ub2c8\ub2e4.\n\n\uc5b4\ub5a4 \uba54\ub274\uac00 \ub9c8\uc74c\uc5d0 \ub4dc\uc2dc\ub098\uc694?\nUser: q\nGoodbye!\n</pre> In\u00a0[\u00a0]: Copied! <pre>%%capture --no-stderr\n%pip install -U tavily-python\n%pip install -U langchain_community\n</pre> %%capture --no-stderr %pip install -U tavily-python %pip install -U langchain_community In\u00a0[\u00a0]: Copied! <pre>_set_env(\"TAVILY_API_KEY\")\n</pre> _set_env(\"TAVILY_API_KEY\") <pre>TAVILY_API_KEY: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> <p><code>tool</code> \uc744 \uc815\uc758\ud558\uace0, \uc0ac\uc6a9\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4</p> In\u00a0[\u00a0]: Copied! <pre>from langchain_community.tools.tavily_search import TavilySearchResults\n\ntool = TavilySearchResults(max_results=2)\ntools = [tool]\ntool.invoke(\"What's a 'node' in LangGraph?\")\n</pre> from langchain_community.tools.tavily_search import TavilySearchResults  tool = TavilySearchResults(max_results=2) tools = [tool] tool.invoke(\"What's a 'node' in LangGraph?\") Out[\u00a0]: <pre>[{'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141',\n  'content': 'Nodes: Nodes are the building blocks of your LangGraph. Each node represents a function or a computation step. You define nodes to perform specific tasks, such as processing input, making ...'},\n {'url': 'https://saksheepatil05.medium.com/demystifying-langgraph-a-beginner-friendly-dive-into-langgraph-concepts-5ffe890ddac0',\n  'content': 'Nodes (Tasks): Nodes are like the workstations on the assembly line. Each node performs a specific task on the product. In LangGraph, nodes are Python functions that take the current state, do some work, and return an updated state. Next, we define the nodes, each representing a task in our sandwich-making process.'}]</pre> <p>\uac80\uc0c9\uacb0\uacfc \ubbf8\ub514\uc5c4\uc5d0\uc11c \uad00\ub828 \uae00\uc744 \ucc3e\uc558\uace0, \ub0b4\uc6a9\uc744 parsing \ud574\uc11c text \uc758 \ud615\ud0dc\ub85c \ub9ac\ud134\ud574 \uc90d\ub2c8\ub2e4.</p> <p>\uc774\uc81c Tavily search tool \uc744 \uc704\ucabd 1\ubc88 \ud30c\ud2b8\uc5d0\uc11c \ub9cc\ub4e0 chatbot langgraph \uc5d0 \uc774\uc5b4 \ubd99\uc5ec\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <ol> <li><code>tool</code>\uc744 \uc815\uc758\ud569\ub2c8\ub2e4.</li> <li><code>llm.bind_tools()</code> \ub97c \uc774\uc6a9\ud574\uc11c tool \uc744 \ucd94\uac00\ud574\uc90d\ub2c8\ub2e4.</li> <li><code>ToolNode</code>\ub97c \uc815\uc758\ud558\uace0 \ub178\ub4dc\ub85c \ucd94\uac00\ud574\uc90d\ub2c8\ub2e4.</li> <li><code>add_conditional_edges()</code> &amp; <code>tools_condition</code> \uc744 \ucd94\uac00\ud574\uc11c LLM\uc774 \uc9c1\uc811 tool \uc744 \uc0ac\uc6a9\ud560\uc9c0 \uc548\ud560\uc9c0 \ud310\ub2e8\ud558\ub3c4\ub85d \ud569\ub2c8\ub2e4.</li> </ol> <ul> <li><code>add_conditional_edge()</code> \ub294 end point \uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uae30 \ub54c\ubb38\uc5d0, \uc9c1\uc811 \uc9c0\uc815\ud560 \ud544\uc694\uac00 \uc5c6\uc2b5\ub2c8\ub2e4.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># \ud30c\ud2b8 1 \uc5d0\uc11c \uc774\uc5b4\uc9c0\ub294 \ucf54\ub4dc \uc785\ub2c8\ub2e4\ngraph_builder = StateGraph(State)\n\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langgraph.graph import START\n\n\ntool = TavilySearchResults(max_results=2)\ntools = [tool]\nllm_with_tools = llm.bind_tools(tools)\n\ndef chatbot(state: State):\n    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n\ngraph_builder.add_node(\"chatbot\", chatbot)\n\ntool_node = ToolNode(tools=[tool])\ngraph_builder.add_node(\"tools\", tool_node)\n\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    tools_condition,\n)\n\ngraph_builder.add_edge(\"tools\", \"chatbot\")\n\n#graph_builder.set_entry_point(\"chatbot\")\ngraph_builder.add_edge(START, \"chatbot\")\n\n# conditional_edges() \uac00 \uc54c\uc544\uc11c \ub05d\ub0bc\uc218 \uc788\uae30 \ub54c\ubb38\uc5d0 \ub05d\uc744 \uc9c0\uc815 \uc548 \ud574\uc918\ub3c4 \ub429\ub2c8\ub2e4.\n# graph_builder.set_finish_point(\"chatbot\")\n\ngraph = graph_builder.compile()\n</pre> # \ud30c\ud2b8 1 \uc5d0\uc11c \uc774\uc5b4\uc9c0\ub294 \ucf54\ub4dc \uc785\ub2c8\ub2e4 graph_builder = StateGraph(State)  from langchain_community.tools.tavily_search import TavilySearchResults from langgraph.prebuilt import ToolNode, tools_condition from langgraph.graph import START   tool = TavilySearchResults(max_results=2) tools = [tool] llm_with_tools = llm.bind_tools(tools)  def chatbot(state: State):     return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}  graph_builder.add_node(\"chatbot\", chatbot)  tool_node = ToolNode(tools=[tool]) graph_builder.add_node(\"tools\", tool_node)  graph_builder.add_conditional_edges(     \"chatbot\",     tools_condition, )  graph_builder.add_edge(\"tools\", \"chatbot\")  #graph_builder.set_entry_point(\"chatbot\") graph_builder.add_edge(START, \"chatbot\")  # conditional_edges() \uac00 \uc54c\uc544\uc11c \ub05d\ub0bc\uc218 \uc788\uae30 \ub54c\ubb38\uc5d0 \ub05d\uc744 \uc9c0\uc815 \uc548 \ud574\uc918\ub3c4 \ub429\ub2c8\ub2e4. # graph_builder.set_finish_point(\"chatbot\")  graph = graph_builder.compile() <p>\uc774\ub807\uac8c \uc0dd\uc131\ub41c \uadf8\ub798\ud504\ub97c \ubcf4\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ub429\ub2c8\ub2e4. conditional edge \uac00 \uc0dd\uaca8\uc11c tool\uc744 \uc0ac\uc6a9\ud558\uac70\ub098 \uc548\ud558\uac70\ub098, \uacb0\uc815\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Image, display\n\ntry:\n    display(Image(graph.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    pass\n</pre> from IPython.display import Image, display  try:     display(Image(graph.get_graph().draw_mermaid_png())) except Exception:     # This requires some extra dependencies and is optional     pass <p>chatbot \uc758 \ud615\ud0dc\ub85c \ucc44\ud305\uc744 \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uacfc\uc5f0 tool\uc744 \uc798 \uc0ac\uc6a9\ud574\uc11c \ub300\ub2f5\ud560 \uc218 \uc788\uc744\uae4c\uc694?</p> In\u00a0[\u00a0]: Copied! <pre>from langchain_core.messages import BaseMessage\n\nwhile True:\n    user_input = input(\"User: \")\n    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n        print(\"Goodbye!\")\n        break\n    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n        for value in event.values():\n            if isinstance(value[\"messages\"][-1], BaseMessage):\n                print(\"Assistant:\", value[\"messages\"][-1].content)\n</pre> from langchain_core.messages import BaseMessage  while True:     user_input = input(\"User: \")     if user_input.lower() in [\"quit\", \"exit\", \"q\"]:         print(\"Goodbye!\")         break     for event in graph.stream({\"messages\": [(\"user\", user_input)]}):         for value in event.values():             if isinstance(value[\"messages\"][-1], BaseMessage):                 print(\"Assistant:\", value[\"messages\"][-1].content) <pre>User: \uc800\ub141 \uba54\ub274 \ucd94\ucc9c \uc880\nAssistant: \uc800\ub141 \uba54\ub274\ub85c\ub294 \uc5ec\ub7ec \uac00\uc9c0\uac00 \uc788\uaca0\uc9c0\ub9cc, \uba87 \uac00\uc9c0 \ucd94\ucc9c\ub4dc\ub9b4\uac8c\uc694:\n\n1. **\ube44\ube54\ubc25**: \ub2e4\uc591\ud55c \ucc44\uc18c\uc640 \uace0\uae30, \uacc4\ub780\uc744 \uc11e\uc5b4 \uba39\ub294 \uac74\uac15\ud55c \ud55c \uadf8\ub987 \uc694\ub9ac\uc785\ub2c8\ub2e4.\n2. **\uae40\uce58\ucc0c\uac1c**: \ub728\ub048\ud558\uace0 \ub9e4\ucf64\ud55c \uae40\uce58\ucc0c\uac1c\ub294 \ubc25\uacfc \ud568\uaed8 \uba39\uae30 \uc88b\uc740 \uba54\ub274\uc785\ub2c8\ub2e4.\n3. **\ubd88\uace0\uae30**: \uc591\ub150\ud55c \uc18c\uace0\uae30\ub97c \uad6c\uc6cc\uc11c \uba39\ub294 \ubd88\uace0\uae30\ub294 \uac04\ub2e8\ud558\uba74\uc11c\ub3c4 \ub9db\uc788\ub294 \uc800\ub141\uc73c\ub85c \uc88b\uc2b5\ub2c8\ub2e4.\n4. **\ud574\ubb3c\ud30c\uc804**: \ud574\ubb3c\uacfc \ud30c\ub97c \ub123\uc5b4 \ubd80\uce5c \uc804\uc744 \ub9c9\uac78\ub9ac\uc640 \ud568\uaed8 \uc990\uae30\uba74 \uc88b\uc2b5\ub2c8\ub2e4.\n5. **\ub2ed\uac08\ube44**: \ub9e4\ucf64\ud55c \uc591\ub150\uc758 \ub2ed\uace0\uae30\uc640 \ucc44\uc18c\ub97c \ubcf6\uc544 \uba39\ub294 \ub2ed\uac08\ube44\ub3c4 \ud6cc\ub96d\ud55c \uc120\ud0dd\uc785\ub2c8\ub2e4.\n\n\uc5b4\ub5a4 \uba54\ub274\uac00 \ub04c\ub9ac\uc2dc\ub098\uc694? \ucd94\uac00\uc801\uc73c\ub85c \ub2e4\ub978 \ucd94\ucc9c\uc774 \ud544\uc694\ud558\uc2dc\uba74 \ub9d0\uc500\ud574 \uc8fc\uc138\uc694!\nUser: \uc800\ub141 \uba54\ub274 \uc694\uc998 \uc720\ud589\ud558\ub294 \uac83\uc744 \uc6f9\uc5d0\uc11c \uac80\uc0c9\ud574\uc11c \ucd94\ucc9c\ud574\uc918\nAssistant: \nAssistant: [{\"url\": \"https://ranknews.co.kr/\uc800\ub141\uba54\ub274-\ucd94\ucc9c-best20/\", \"content\": \"\uc624\ub298\uc740 \uc800\ub141 \uba54\ub274 \ucd94\ucc9c BEST 20\uc5d0 \ub300\ud574\uc11c \uc54c\ub824\ub4dc\ub9ac\ub824\uace0 \ud569\ub2c8\ub2e4. \uc0ac\uc2e4 \uc800\ub3c4 \ud1f4\uadfc\ud558\uace0 \ub098\uba74 \ub9cc\uc0ac\uac00 \uadc0\ucc2e\uc544\uc9c8 \ub54c\uac00 \ub9ce\uc544\uc11c, \uac00\uc7a5 \ub9cc\uc871\uc2a4\ub7ec\uc6b4 \uc800\ub141 \uba54\ub274\ub97c \ube60\ub974\uac8c \uace0\ub97c \uc218 \uc788\ub3c4\ub85d \uc900\ube44\ud574 \ubcf4\uc558\uc2b5\ub2c8\ub2e4. \uc790\ucde8 5\ub144 \ucc28\uc774\ub2c8 \uc800\ub97c \ubbff\uc5b4\ubcf4\uc154\ub3c4 \uc88b\uc2b5\ub2c8\ub2e4. \ubc30\ub2ec\uc758 \ubbfc\uc871\uc774\ub77c\ub294 ...\"}, {\"url\": \"https://m.blog.naver.com/next200208/222957667466\", \"content\": \"2023 \uc678\uc2dd\uc5c5\ud2b8\ub80c\ub4dc\ub294 2020\ub144\ubd80\ud130 \uc9c4\ud589\ud574\uc628 2022 \ubc30\ubbfc \uc678\uc2dd\uc5c5 \ucee8\ud37c\ub7f0\uc2a4\uc5d0\uc11c \ucd5c\ucd08 \uacf5\uac1c\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc62c\ud574 \ud589\uc0ac\ub294 '22.12.13 (\ud654)~12.14 (\uc218) \uc591\uc77c\uac04, \ubc14\uc05c \uc0ac\uc7a5\ub2d8\ub4e4\uc774 \uc27d\uac8c \uc811\uadfc\ud558\uace0\uc790 \uc5ed\uc2dc \uc628\ub77c\uc778\uc73c\ub85c \uc9c4\ud589\ub418\uc5c8\ub294\ub370\uc694. \uc774\ubc88 \ucee8\ud37c\ub7f0\uc2a4\uc5d0 \uae40\ub09c\ub3c4 \uad50\uc218\uac00 \uc9c1\uc811 \ub098\uc640, \ubb34\ub824 80\ubd84 ...\"}]\nAssistant: \uc694\uc998 \uc720\ud589\ud558\ub294 \uc800\ub141 \uba54\ub274 \ucd94\ucc9c\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \ucc3e\uc558\uc2b5\ub2c8\ub2e4. \uc544\ub798 \ub9c1\ud06c\ub4e4\uc744 \ud1b5\ud574 \ub2e4\uc591\ud55c \ucd94\ucc9c \uba54\ub274\ub97c \ud655\uc778\ud574\ubcf4\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n1. [\uc800\ub141 \uba54\ub274 \ucd94\ucc9c BEST 20](https://ranknews.co.kr/\uc800\ub141\uba54\ub274-\ucd94\ucc9c-best20/): \uc5ec\ub7ec \uac00\uc9c0 \uc800\ub141 \uba54\ub274\ub97c \uc18c\uac1c\ud558\uba70, \ud2b9\ud788 \ud1f4\uadfc \ud6c4 \uc27d\uac8c \ub9cc\ub4e4 \uc218 \uc788\ub294 \uba54\ub274\ub4e4\uc744 \uc81c\uc548\ud569\ub2c8\ub2e4.\n\n2. [2023 \uc678\uc2dd\uc5c5 \ud2b8\ub80c\ub4dc](https://m.blog.naver.com/next200208/222957667466): 2023\ub144 \uc678\uc2dd\uc5c5 \ud2b8\ub80c\ub4dc\ub97c \ub2e4\ub8e8\uace0 \uc788\uc73c\uba70, \uc62c\ud574\uc758 \uc778\uae30 \uba54\ub274\uc640 \uc678\uc2dd \uc5c5\uacc4\uc758 \ubcc0\ud654\uc5d0 \ub300\ud55c \uc815\ubcf4\ub3c4 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc774 \ub9c1\ud06c\ub4e4\uc744 \ucc38\uace0\ud558\uc5ec \uc694\uc998 \uc778\uae30 \uc788\ub294 \uc800\ub141 \uba54\ub274\ub97c \uc120\ud0dd\ud574 \ubcf4\uc138\uc694!\nUser: q\nGoodbye!\n</pre> <ol> <li>\uccab \uc9c8\ubb38\uc778 \uc800\ub141 \uba54\ub274 \ucd94\ucc9c \uc880 \uc774\ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub300\ud574\uc11c\ub294 tavily \uc758 \uac80\uc0c9 \uc5c6\uc774 \ub300\ub2f5\uc744 \ud588\uc2b5\ub2c8\ub2e4.</li> </ol> <ul> <li>\uc544\ub798 Langsmith \ub85c \ucd94\uc801\ub41c \uacb0\uacfc\ub97c \ucca8\ubd80\ud569\ub2c8\ub2e4.</li> </ul> <p></p> <ol> <li>\ucd94\uac00 \uc9c8\ubb38\uc778 \uc800\ub141 \uba54\ub274 \uc694\uc998 \uc720\ud589\ud558\ub294 \uac83\uc744 \uc6f9\uc5d0\uc11c \uac80\uc0c9\ud574\uc11c \ucd94\ucc9c\ud574\uc918 \uc5d0 \ub300\ud574\uc11c\ub294 tavily \uac80\uc0c9 \uacb0\uacfc\uc640 \ud568\uaed8 \ub2f5\ubcc0\uc744 \ud574 \uc92c\uc2b5\ub2c8\ub2e4.</li> </ol> <ul> <li>\uc5ed\uc2dc\ub098, \uc544\ub798 Langsmith \ub85c \ucd94\uc801\ub41c \uacb0\uacfc\ub97c \ucca8\ubd80\ud569\ub2c8\ub2e4.</li> </ul> <p></p> <p>\uac80\uc0c9\uacb0\uacfc platum.kr \ub3c4\uba54\uc778\uc758 \uc678\uc2dd\uc5c5 \ud2b8\ub80c\ub4dc \uae00\uc744 \ucc38\uace0\ud574\uc11c \ub300\ub2f5\uc744 \ud588\uc2b5\ub2c8\ub2e4. \uc815\ub9d0\ub85c \ub9cc\uc871\uc2a4\ub7ec\uc6b4 \uc800\ub141 \uba54\ub274\ub97c \ucd94\ucc9c\ud574\uc92c\ub294\uc9c0\ub294 \ubaa8\ub974\uaca0\uc9c0\ub9cc, \uc5b4\uca0c\ub4e0 \ub3c4\uad6c\uc758 \ub3c4\uc6c0\uacfc \ud568\uaed8 \ub2f5\ubcc0\uc744 \ud588\uc2b5\ub2c8\ub2e4.</p> <p>\uc6d0\ud558\ub358\ub300\ub85c \uc6f9\uc11c\uce6d \ub3c4\uad6c\ub97c \uc798 \uc950\uc5b4 \uc8fc\uc5c8\uad70\uc694!</p> <p>\uc774\ub7f0\uac83\uc774 \uc5b4\ub5bb\uac8c \uac00\ub2a5\ud560\uae4c\uc694...??</p> <ul> <li>\ucd94\uc801\uc744 \ubcf4\uba74, tavily \uc5d0 \ub0a0\ub9b4 \ucffc\ub9ac\ub97c LLM (\uc5ec\uae30\uc11c\ub294 gpt-4o-mini) \uc774 \uc54c\uc544\uc11c \ubf51\uc544\uc11c \uc694\uccad\ud558\uace0 \uc635\ub2c8\ub2e4.</li> <li>\uc6b0\ub9ac\uac00 \uc0ac\uc6a9\ud55c LLM\uc740 ChatOpenAI, LangChain \uc758 chat \ubaa8\ub378\uc785\ub2c8\ub2e4. \uc5ec\uae30\uc11c tool calling \uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4.</li> <li>\uc544\ub798 \ub9c1\ud06c\ub97c \ubcf4\uba74, chat \ubaa8\ub378\ub9c8\ub2e4 tool calling \uc758 \uc9c0\uc6d0 \uc5ec\ubd80\ub97c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.<ul> <li>\ucc38\uc870 - https://python.langchain.com/docs/integrations/chat/?ref=blog.langchain.dev</li> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>from langgraph.checkpoint.memory import MemorySaver\n\nmemory = MemorySaver()\n</pre> from langgraph.checkpoint.memory import MemorySaver  memory = MemorySaver() <p>\ucc38\uace0 \uc774 \ubb38\uc11c\uc5d0\uc11c\ub294 In-memory checkpointer \ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc11c\uc758 \"in-memory\" \ub294 \ud504\ub85c\uc138\uc2a4\uc758 \uba54\ubaa8\ub9ac, RAM \uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \ud29c\ud1a0\ub9ac\uc5bc \uc790\ub8cc\ub85c\uc11c\ub294 \uc774\uc815\ub3c4\ub85c \ucda9\ubd84\ud558\uc9c0\ub9cc, \ub2f9\uc5f0\ud788 \ud504\ub85c\uc138\uc2a4\uac00 \uaebc\uc9c0\uba74, \uc815\ubcf4\uac00 \ubaa8\ub450 \ub0a0\uc544\uac11\ub2c8\ub2e4. \uc0c1\uc5c5 (production) \ub808\ubca8\uc5d0\uc11c\ub294 <code>SqliteSaver</code> \ub098 <code>PostgresSaver</code>\ub97c \uc0ac\uc6a9\ud558\uace0 DB \uc5d0 \uc5f0\uacb0\ud558\ub294 \uac83\uc744 \ucd94\ucc9c\ud569\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre># \ud30c\ud2b82 \uc5d0\uc11c \uc791\uc131\ud55c \uadf8\ub798\ud504\uc5d0 checkpointer \ub9cc \ucd94\uac00\ud558\uc5ec \ub2e4\uc2dc \ub9cc\ub4e7\ub2c8\ub2e4.\n\ngraph = graph_builder.compile(checkpointer=memory)\n</pre> # \ud30c\ud2b82 \uc5d0\uc11c \uc791\uc131\ud55c \uadf8\ub798\ud504\uc5d0 checkpointer \ub9cc \ucd94\uac00\ud558\uc5ec \ub2e4\uc2dc \ub9cc\ub4e7\ub2c8\ub2e4.  graph = graph_builder.compile(checkpointer=memory) <p>\uadf8\ub798\ud504\ub97c \uc2dc\uac01\ud654\ud574\ubcf4\uba74, \ub178\ub4dc\uac00 \ub2ec\ub77c\uc9c4 \uac83\uc740 \uc5c6\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Image, display\n\ntry:\n    display(Image(graph.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    pass\n</pre> from IPython.display import Image, display  try:     display(Image(graph.get_graph().draw_mermaid_png())) except Exception:     # This requires some extra dependencies and is optional     pass <p>\ub300\ud654\uc5d0 \uc4f0\ub808\ub4dc \uc544\uc774\ub514\ub97c \ud560\ub2f9\ud558\uace0 \ucc57\ubd07\uc744 \uc0ac\uc6a9\ud574 \ubd05\uc2dc\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>config = {\"configurable\": {\"thread_id\": \"1\"}}\n</pre> config = {\"configurable\": {\"thread_id\": \"1\"}} In\u00a0[\u00a0]: Copied! <pre>user_input = \"\uc548\ub155\uc548\ub155, \ub098\ub294 \uc810\uc2ec\uc5d0 \uacbd\uc591\uc2dd \ub3c8\uae4c\uc4f0\ub97c \uba39\uc5c8\uc5b4.\"\n\n# The config is the **second positional argument** to stream() or invoke()!\nevents = graph.stream(\n    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n)\nfor event in events:\n    event[\"messages\"][-1].pretty_print()\n</pre> user_input = \"\uc548\ub155\uc548\ub155, \ub098\ub294 \uc810\uc2ec\uc5d0 \uacbd\uc591\uc2dd \ub3c8\uae4c\uc4f0\ub97c \uba39\uc5c8\uc5b4.\"  # The config is the **second positional argument** to stream() or invoke()! events = graph.stream(     {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\" ) for event in events:     event[\"messages\"][-1].pretty_print() <pre>================================ Human Message =================================\n\n\uc548\ub155\uc548\ub155, \ub098\ub294 \uc810\uc2ec\uc5d0 \uacbd\uc591\uc2dd \ub3c8\uae4c\uc4f0\ub97c \uba39\uc5c8\uc5b4.\n================================== Ai Message ==================================\n\n\uc548\ub155\ud558\uc138\uc694! \uacbd\uc591\uc2dd \ub3c8\uae4c\uc2a4\ub97c \ub4dc\uc168\ub2e4\ub2c8 \ub9db\uc788\uaca0\ub124\uc694. \ub3c8\uae4c\uc2a4\ub294 \ubc14\uc0ad\ud55c \ud280\uae40\uacfc \ubd80\ub4dc\ub7ec\uc6b4 \uace0\uae30\uac00 \uc870\ud654\ub97c \uc774\ub8e8\uc5b4\uc11c \uc815\ub9d0 \ub9db\uc788\uc8e0. \uc5b4\ub5a4 \uc18c\uc2a4\uc640 \ud568\uaed8 \ub4dc\uc168\ub098\uc694?\n</pre> <p>Note: The config was provided as the second positional argument when calling our graph. It importantly is not nested within the graph inputs (<code>{'messages': []}</code>).</p> <p>Let's ask a followup: see if it remembers your name.</p> In\u00a0[\u00a0]: Copied! <pre>user_input = \"\ub098 \uc810\uc2ec\uc5d0 \ubb50 \uba39\uc5c8\uc9c0?\"\n\n# The config is the **second positional argument** to stream() or invoke()!\nevents = graph.stream(\n    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n)\nfor event in events:\n    event[\"messages\"][-1].pretty_print()\n</pre> user_input = \"\ub098 \uc810\uc2ec\uc5d0 \ubb50 \uba39\uc5c8\uc9c0?\"  # The config is the **second positional argument** to stream() or invoke()! events = graph.stream(     {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\" ) for event in events:     event[\"messages\"][-1].pretty_print() <pre>================================ Human Message =================================\n\n\ub098 \uc810\uc2ec\uc5d0 \ubb50 \uba39\uc5c8\uc9c0?\n================================== Ai Message ==================================\n\n\uc810\uc2ec\uc5d0 \uacbd\uc591\uc2dd \ub3c8\uae4c\uc2a4\ub97c \ub4dc\uc168\ub2e4\uace0 \ud558\uc168\uc2b5\ub2c8\ub2e4! \uc544\uc8fc \ub9db\uc788\ub294 \uc120\ud0dd\uc774\uc5c8\ub124\uc694. \ub2e4\ub978 \uba54\ub274\ub3c4 \uace0\ub824\ud558\uc168\ub098\uc694?\n</pre> <p>\uc55e\uc120 \ub300\ud654\ub97c \uc798 \uae30\uc5b5\uc744 \ud588\uc2b5\ub2c8\ub2e4. LangSmith trace \ub97c \ubcf4\uc2dc\uba74, \ub300\ud654\uac00 \uc774\uc5b4\uc838 \uc788\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub9c1\ud06c\uc758 \ud654\uba74\uc740 \ucea1\ucc98\ud574\uc11c \uc544\ub798 \ucca8\ubd80\ud569\ub2c8\ub2e4.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre># The only difference is we change the `thread_id` here to \"2\" instead of \"1\"\nevents = graph.stream(\n    {\"messages\": [(\"user\", user_input)]},\n    {\"configurable\": {\"thread_id\": \"2\"}},\n    stream_mode=\"values\",\n)\nfor event in events:\n    event[\"messages\"][-1].pretty_print()\n</pre> # The only difference is we change the `thread_id` here to \"2\" instead of \"1\" events = graph.stream(     {\"messages\": [(\"user\", user_input)]},     {\"configurable\": {\"thread_id\": \"2\"}},     stream_mode=\"values\", ) for event in events:     event[\"messages\"][-1].pretty_print() <pre>================================ Human Message =================================\n\n\ub098 \uc810\uc2ec\uc5d0 \ubb50 \uba39\uc5c8\uc9c0?\n================================== Ai Message ==================================\n\n\uc8c4\uc1a1\ud558\uc9c0\ub9cc, \ub2f9\uc2e0\uc774 \uc810\uc2ec\uc5d0 \ubb34\uc5c7\uc744 \uba39\uc5c8\ub294\uc9c0 \uc54c \uc218 \uc788\ub294 \ubc29\ubc95\uc774 \uc5c6\uc2b5\ub2c8\ub2e4. \ud639\uc2dc \uae30\uc5b5\uc774 \ub098\uc9c0 \uc54a\uac70\ub098 \ub3c4\uc6c0\uc774 \ud544\uc694\ud55c \ub2e4\ub978 \uc9c8\ubb38\uc774 \uc788\uc73c\uc2dc\uba74 \ub9d0\uc500\ud574 \uc8fc\uc138\uc694!\n</pre> <p><code>thread_id</code> \ub97c \ubc14\uafd4\uc8fc\ub2c8, \ub300\ud654\ub97c \uae30\uc5b5\uc744 \ubabb\ud558\ub124\uc694.</p> <p>Checkpoint \uc5d0\uc11c \ubb34\uc2a8 \uc77c\uc774 \uc77c\uc5b4\ub098\uace0 \uc788\ub294\uc9c0 \ubcf4\uace0 \uc2f6\ub2e4\uba74, graph\uc758 <code>state</code> \ub97c <code>get_state(config)</code> \ud568\uc218 \ud638\ucd9c\ud574\uc11c \ubcf4\uba74 \ub429\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>snapshot = graph.get_state(config)\nsnapshot\n</pre> snapshot = graph.get_state(config) snapshot Out[\u00a0]: <pre>StateSnapshot(values={'messages': [HumanMessage(content='\uc548\ub155\uc548\ub155, \ub098\ub294 \uc810\uc2ec\uc5d0 \uacbd\uc591\uc2dd \ub3c8\uae4c\uc4f0\ub97c \uba39\uc5c8\uc5b4.', additional_kwargs={}, response_metadata={}, id='e4e5df43-1506-4ed6-b174-c7c46df52c7b'), AIMessage(content='\uc548\ub155\ud558\uc138\uc694! \uacbd\uc591\uc2dd \ub3c8\uae4c\uc2a4\ub97c \ub4dc\uc168\ub2e4\ub2c8 \ub9db\uc788\uaca0\ub124\uc694. \ub3c8\uae4c\uc2a4\ub294 \ubc14\uc0ad\ud55c \ud280\uae40\uacfc \ubd80\ub4dc\ub7ec\uc6b4 \uace0\uae30\uac00 \uc870\ud654\ub97c \uc774\ub8e8\uc5b4\uc11c \uc815\ub9d0 \ub9db\uc788\uc8e0. \uc5b4\ub5a4 \uc18c\uc2a4\uc640 \ud568\uaed8 \ub4dc\uc168\ub098\uc694?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 99, 'total_tokens': 152, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-26d8ef2f-aedc-4593-b0c7-73bd66d01b2e-0', usage_metadata={'input_tokens': 99, 'output_tokens': 53, 'total_tokens': 152}), HumanMessage(content='\ub098 \uc810\uc2ec\uc5d0 \ubb50 \uba39\uc5c8\uc9c0?', additional_kwargs={}, response_metadata={}, id='82514da5-863d-4a44-a5d7-cd75be780098'), AIMessage(content='\uc810\uc2ec\uc5d0 \uacbd\uc591\uc2dd \ub3c8\uae4c\uc2a4\ub97c \ub4dc\uc168\ub2e4\uace0 \ud558\uc168\uc2b5\ub2c8\ub2e4! \uc544\uc8fc \ub9db\uc788\ub294 \uc120\ud0dd\uc774\uc5c8\ub124\uc694. \ub2e4\ub978 \uba54\ub274\ub3c4 \uace0\ub824\ud558\uc168\ub098\uc694?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 168, 'total_tokens': 201, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-4826567c-c492-4da9-8a70-daaf7b4bbad1-0', usage_metadata={'input_tokens': 168, 'output_tokens': 33, 'total_tokens': 201})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef7578b-53c1-62a8-8004-1000dc8fcbeb'}}, metadata={'source': 'loop', 'writes': {'chatbot': {'messages': [AIMessage(content='\uc810\uc2ec\uc5d0 \uacbd\uc591\uc2dd \ub3c8\uae4c\uc2a4\ub97c \ub4dc\uc168\ub2e4\uace0 \ud558\uc168\uc2b5\ub2c8\ub2e4! \uc544\uc8fc \ub9db\uc788\ub294 \uc120\ud0dd\uc774\uc5c8\ub124\uc694. \ub2e4\ub978 \uba54\ub274\ub3c4 \uace0\ub824\ud558\uc168\ub098\uc694?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 168, 'total_tokens': 201, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-4826567c-c492-4da9-8a70-daaf7b4bbad1-0', usage_metadata={'input_tokens': 168, 'output_tokens': 33, 'total_tokens': 201})]}}, 'step': 4, 'parents': {}}, created_at='2024-09-18T04:44:36.891287+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef7578b-478e-668c-8003-6304d3ab0c68'}}, tasks=())</pre> In\u00a0[\u00a0]: Copied! <pre>snapshot.next  # (since the graph ended this turn, `next` is empty. If you fetch a state from within a graph invocation, next tells which node will execute next)\n</pre> snapshot.next  # (since the graph ended this turn, `next` is empty. If you fetch a state from within a graph invocation, next tells which node will execute next) Out[\u00a0]: <pre>()</pre> <p>\uc2a4\ub0c5\uc0f7\uc740 \ub118\uaca8\uc900 config (thread_id) \uc758 \ud604\uc7ac state \uac12\uc744 \uac00\uc9c0\uace0 \uc788\uace0\uc694, \ub2e4\uc74c\uc73c\ub85c \uc2e4\ud589\ud560 <code>next</code> \ub178\ub4dc\ub97c \uac00\ub9ac\ud0a4\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294  <code>__end__</code> \uc5d0 \ub3c4\ub2ec\ud574\uc11c <code>next</code> \uac00 \uc5c6\uc5b4\uc694.</p> <p>\uc5ec\uae30\uae4c\uc9c0 \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \ub300\ud654\uc758 \uae30\uc5b5\uc744 \uad6c\ud604\ud558\ub3c4\ub85d \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c\uc740 \uc0ac\ub78c\uc758 \uac1c\uc785 (Human in the loop) \uc744 \uad6c\ud604\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>\uc5ec\uae30\uc11c \uc7a0\uae50! LangGraph Studio \ub97c \uc0ac\uc6a9\ud558\uba74, \uc544\uc8fc \uc27d\uac8c \uc778\ud130\ub7fd\ud2b8\ub97c UX \uc640 \ud568\uaed8 \uad6c\ud604 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. 24.9.30 \uae30\uc900 MAC OS \ub9cc \ubca0\ud0c0\ub85c \uc9c0\uc6d0\ud569\ub2c8\ub2e4.</p> <p>\uadf8\ub798\ud504\ub97c \ucef4\ud30c\uc77c \ud560\ub54c, <code>interrupt_before</code>  \ub97c <code>tools</code> \ub178\ub4dc\uc5d0\ub2e4\uac00 \ub123\uaca0\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>graph = graph_builder.compile(\n    checkpointer=memory,\n    # This is new!\n    interrupt_before=[\"tools\"],\n\n    # Note: can also interrupt __after__ tools, if desired.\n    # interrupt_after=[\"tools\"]\n)\n</pre> graph = graph_builder.compile(     checkpointer=memory,     # This is new!     interrupt_before=[\"tools\"],      # Note: can also interrupt __after__ tools, if desired.     # interrupt_after=[\"tools\"] ) In\u00a0[\u00a0]: Copied! <pre>user_input = \"\ub098\ub294 \uc9c0\uae08 \ub7ad\uadf8\ub798\ud504\ub97c \ubc30\uc6b0\uace0 \uc788\ub294\ub370, \ub300\uc2e0 \uc870\uc0ac \uc880 \ud574\uc904\ub798?\"\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n# The config is the **second positional argument** to stream() or invoke()!\nevents = graph.stream(\n    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n</pre> user_input = \"\ub098\ub294 \uc9c0\uae08 \ub7ad\uadf8\ub798\ud504\ub97c \ubc30\uc6b0\uace0 \uc788\ub294\ub370, \ub300\uc2e0 \uc870\uc0ac \uc880 \ud574\uc904\ub798?\" config = {\"configurable\": {\"thread_id\": \"1\"}} # The config is the **second positional argument** to stream() or invoke()! events = graph.stream(     {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\" ) for event in events:     if \"messages\" in event:         event[\"messages\"][-1].pretty_print() <pre>================================ Human Message =================================\n\n\ub098\ub294 \uc9c0\uae08 \ub7ad\uadf8\ub798\ud504\ub97c \ubc30\uc6b0\uace0 \uc788\ub294\ub370, \ub300\uc2e0 \uc870\uc0ac \uc880 \ud574\uc904\ub798?\n================================== Ai Message ==================================\nTool Calls:\n  tavily_search_results_json (call_56EmA9jt3tcBF09z4sVslldv)\n Call ID: call_56EmA9jt3tcBF09z4sVslldv\n  Args:\n    query: \ub7ad\uadf8\ub798\ud504(Leung graph) \uac1c\ub150 \ubc0f \ud65c\uc6a9\n</pre> <p><code>get_state()</code> \ub97c \ud574\uc11c \ub2e4\uc74c \uc2a4\ud0ed\uc744 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>snapshot = graph.get_state(config)\nsnapshot.next\n</pre> snapshot = graph.get_state(config) snapshot.next Out[\u00a0]: <pre>('tools',)</pre> <p>Notice \uc544\uae4c \uc704\uc5d0\uc11c\ub294 next \uac00 \ube44\uc5b4\uc788\uc5c8\uc8e0. \uc774\ubc88\uc5d0\ub294 next \uc5d0  'tools' \uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc911\uac04 \ub2e8\uacc4\uc5d0\uc11c \uc798 \uba48\ucdc4\ub2e4\ub294 \ub73b\uc774\uc8e0.</p> In\u00a0[\u00a0]: Copied! <pre>existing_message = snapshot.values[\"messages\"][-1]\nexisting_message.tool_calls\n</pre> existing_message = snapshot.values[\"messages\"][-1] existing_message.tool_calls Out[\u00a0]: <pre>[{'name': 'tavily_search_results_json',\n  'args': {'query': '\ub7ad\uadf8\ub798\ud504(Leung graph) \uac1c\ub150 \ubc0f \ud65c\uc6a9'},\n  'id': 'call_56EmA9jt3tcBF09z4sVslldv',\n  'type': 'tool_call'}]</pre> <p>Tools \uc778 Tavily \ub85c \uac80\uc0c9\uc744 \ub0a0\ub9b4 \uc608\uc815\uc778\ub370, \ucffc\ub9ac\ub97c \ub0b4\ubcf4\ub0bc \uac80\uc0c9\uc5b4\uac00 \ub098\uc058\uc9c0 \uc54a\uc740 \uac83 \uac19\uc544\uc694, \ubcc4\ub2e4\ub978 \uc218\uc815\uc5c6\uc774 \uadf8\ub300\ub85c \uc9c4\ud589 \uc2dc\ucf1c \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre># `None` will append nothing new to the current state, letting it resume as if it had never been interrupted\nevents = graph.stream(None, config, stream_mode=\"values\")\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n</pre> # `None` will append nothing new to the current state, letting it resume as if it had never been interrupted events = graph.stream(None, config, stream_mode=\"values\") for event in events:     if \"messages\" in event:         event[\"messages\"][-1].pretty_print() <pre>================================== Ai Message ==================================\nTool Calls:\n  tavily_search_results_json (call_56EmA9jt3tcBF09z4sVslldv)\n Call ID: call_56EmA9jt3tcBF09z4sVslldv\n  Args:\n    query: \ub7ad\uadf8\ub798\ud504(Leung graph) \uac1c\ub150 \ubc0f \ud65c\uc6a9\n================================= Tool Message =================================\nName: tavily_search_results_json\n\n[{\"url\": \"https://gunn.kim/langgraph-vs-langchain-evolution-of-llm-development-frameworks\", \"content\": \"\ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM) \uae30\ubc18 \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uac1c\ubc1c\uc774 \uae09\uc99d\ud558\uba74\uc11c \ub7ad\uccb4\uc778(LangChain)\uacfc \ub7ad\uadf8\ub798\ud504(LangGraph)\uac00 \uc8fc\ubaa9\ubc1b\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uae00\uc5d0\uc11c\ub294 \ub450 \ud504\ub808\uc784\uc6cc\ud06c\uc758 \ud2b9\uc9d5, \ucc28\uc774\uc810, \uadf8\ub9ac\uace0 \uc2e4\uc81c \uc801\uc6a9 \uc0ac\ub840\ub97c \uc0b4\ud3b4\ubd05\ub2c8\ub2e4. AutoGPT\uc640\uc758 \uc720\uc0ac\uc131\uc744 \ud1b5\ud574 \uac01 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc774\ud574\ud558\uace0, \ub098\uc2a4\ub2e5 \uc9c0\uc218 \ubd84\uc11d \ubc0f \ud3ec\ud2b8\ud3f4\ub9ac\uc624 \ucd5c\uc801\ud654 ...\"}, {\"url\": \"https://livewiki.com/ko/content/learn-langgraph-easy-way\", \"content\": \"LangGraph \uc571\uc744 \uc0ac\uc6a9\ud558\uba74 \uadf8\ub798\ud504 \uad6c\uc870\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc0ac\uc6a9\uc790 \ucffc\ub9ac\ub97c \ucc98\ub9ac\ud558\uace0 \uc751\ub2f5\ud558\ub294 \uc5d0\uc774\uc804\ud2b8 \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \ub610\ub294 LLM \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\"}]\n================================== Ai Message ==================================\n\n\ub7ad\uadf8\ub798\ud504(Leung graph)\uc5d0 \ub300\ud55c \uc815\ubcf4\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:\n\n1. **\ub7ad\uadf8\ub798\ud504\uc640 \ub7ad\uccb4\uc778**:\n   - \ucd5c\uadfc \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM) \uae30\ubc18 \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uac1c\ubc1c\uc774 \ud65c\ubc1c\ud574\uc9c0\uba74\uc11c \ub7ad\uccb4\uc778(LangChain)\uacfc \ub7ad\uadf8\ub798\ud504(LangGraph)\uac00 \uc8fc\ubaa9\ubc1b\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ub450 \ud504\ub808\uc784\uc6cc\ud06c\ub294 \uac01\uac01\uc758 \ud2b9\uc9d5\uacfc \ucc28\uc774\uc810\uc774 \uc788\uc73c\uba70, \uc2e4\uc81c \uc801\uc6a9 \uc0ac\ub840\ub97c \ud1b5\ud574 \uc774\ud574\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. [\ub354 \uc54c\uc544\ubcf4\uae30](https://gunn.kim/langgraph-vs-langchain-evolution-of-llm-development-frameworks)\n\n2. **LangGraph \uc571**:\n   - LangGraph \uc571\uc744 \uc0ac\uc6a9\ud558\uba74 \uadf8\ub798\ud504 \uad6c\uc870\ub97c \ud65c\uc6a9\ud558\uc5ec \uc0ac\uc6a9\uc790 \ucffc\ub9ac\ub97c \ucc98\ub9ac\ud558\uace0 \uc751\ub2f5\ud558\ub294 \uc5d0\uc774\uc804\ud2b8 \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \ub610\ub294 LLM \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ub370\uc774\ud130 \uad6c\uc870\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ubc29\ubc95\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. [\uc790\uc138\ud788 \ubcf4\uae30](https://livewiki.com/ko/content/learn-langgraph-easy-way)\n\n\ub354 \uad81\uae08\ud55c \uc810\uc774 \uc788\uc73c\uba74 \ub9d0\uc500\ud574 \uc8fc\uc138\uc694!\n</pre> <p>\ucca8\uc5b8</p> <p>\uac80\uc0c9\uacb0\uacfc\uac00 \ub9e4\uc6b0 \ub9c8\uc74c\uc5d0 \uc548\ub4e4\uae30\ub294 \ud569\ub2c8\ub2e4... \ud3ec\uc2a4\ud2b8\uac00 AI generated content \uc778 \uac83 \uac19\uad70\uc694. \ud53c\uaddc\uc5b4\ub3c4 \uae68\uc838\uc788\uace0, \uc790\uc138\ud788 \uc77d\uc5b4\ubcf4\uba74 \ud2c0\ub9b0 \ub0b4\uc6a9\ub3c4 \ub9ce\uc2b5\ub2c8\ub2e4. \uc544\ub9c8 \ub300\ubd80\ubd84\uc740 \ub208\uce58\ucc44\uc9c0 \ubabb\ud560\ud14c\uc9c0\ub9cc\uc694. Human-in-the-loop \uc774 \uc774\ub7f0 \uac80\uc0c9 \uacb0\uacfc\uc5d0\uc11c \ub9ac\uc81d\ud558\uace0 \uc2f6\uc740 \uac83\ub4e4\uc744 \uac70\ub974\uae30 \uc704\ud55c \ubc29\ubc95\uc73c\ub85c\ub3c4 \uc798 \uc4f0\uc77c \uac83 \uac19\uc740\ub370, After Tool \uc5d0 \uc778\ud130\ub7fd\ud2b8\ub97c \uac78\uc5b4\uc11c \ud55c\ubc88 \uac70\uc808\ud558\ub3c4\ub85d \ud574\ubcfc\uae4c\uc694?</p> In\u00a0[\u00a0]: Copied! <pre>graph_builder = StateGraph(State)\n\ngraph_builder.add_node(\"chatbot\", chatbot)\n\ntool_node = ToolNode(tools=[tool])\ngraph_builder.add_node(\"tools\", tool_node)\n\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    tools_condition,\n)\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.add_edge(START, \"chatbot\")\nmemory = MemorySaver()\ngraph = graph_builder.compile(\n    checkpointer=memory,\n    # This is new!\n    interrupt_before=[\"tools\"],\n    # Note: can also interrupt **after** actions, if desired.\n    # interrupt_after=[\"tools\"]\n)\n\nuser_input = \"\ub098\ub294 \uc9c0\uae08 \ub7ad\uadf8\ub798\ud504\ub97c \ubc30\uc6b0\uace0 \uc788\ub294\ub370, \ub300\uc2e0 \uc870\uc0ac \uc880 \ud574\uc904\ub798?\"\nconfig = {\"configurable\": {\"thread_id\": \"3\"}}\n# The config is the **second positional argument** to stream() or invoke()!\nevents = graph.stream({\"messages\": [(\"user\", user_input)]}, config)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n</pre> graph_builder = StateGraph(State)  graph_builder.add_node(\"chatbot\", chatbot)  tool_node = ToolNode(tools=[tool]) graph_builder.add_node(\"tools\", tool_node)  graph_builder.add_conditional_edges(     \"chatbot\",     tools_condition, ) graph_builder.add_edge(\"tools\", \"chatbot\") graph_builder.add_edge(START, \"chatbot\") memory = MemorySaver() graph = graph_builder.compile(     checkpointer=memory,     # This is new!     interrupt_before=[\"tools\"],     # Note: can also interrupt **after** actions, if desired.     # interrupt_after=[\"tools\"] )  user_input = \"\ub098\ub294 \uc9c0\uae08 \ub7ad\uadf8\ub798\ud504\ub97c \ubc30\uc6b0\uace0 \uc788\ub294\ub370, \ub300\uc2e0 \uc870\uc0ac \uc880 \ud574\uc904\ub798?\" config = {\"configurable\": {\"thread_id\": \"3\"}} # The config is the **second positional argument** to stream() or invoke()! events = graph.stream({\"messages\": [(\"user\", user_input)]}, config) for event in events:     if \"messages\" in event:         event[\"messages\"][-1].pretty_print() In\u00a0[\u00a0]: Copied! <pre>snapshot = graph.get_state(config)\nexisting_message = snapshot.values[\"messages\"][-1]\nexisting_message.pretty_print()\n</pre> snapshot = graph.get_state(config) existing_message = snapshot.values[\"messages\"][-1] existing_message.pretty_print() <pre>================================== Ai Message ==================================\nTool Calls:\n  tavily_search_results_json (call_N9E5wxGaTBpZudPU5IkiRBac)\n Call ID: call_N9E5wxGaTBpZudPU5IkiRBac\n  Args:\n    query: \ub7ad\uadf8\ub798\ud504 \ub7ad\ud0b9 \uadf8\ub798\ud504 \uacf5\ubd80\n</pre> In\u00a0[\u00a0]: Copied! <pre>snapshot\n</pre> snapshot Out[\u00a0]: <pre>StateSnapshot(values={'messages': [HumanMessage(content='\ub098\ub294 \uc9c0\uae08 \ub7ad\uadf8\ub798\ud504\ub97c \ubc30\uc6b0\uace0 \uc788\ub294\ub370, \ub300\uc2e0 \uc870\uc0ac \uc880 \ud574\uc904\ub798?', additional_kwargs={}, response_metadata={}, id='e7ce0efa-ef12-41f0-b5f5-eb662a0e8902'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_N9E5wxGaTBpZudPU5IkiRBac', 'function': {'arguments': '{\"query\":\"\ub7ad\uadf8\ub798\ud504 \ub7ad\ud0b9 \uadf8\ub798\ud504 \uacf5\ubd80\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 97, 'total_tokens': 124, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_54e2f484be', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-76b993a2-cfcd-42a6-870a-70bb44080fd4-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': '\ub7ad\uadf8\ub798\ud504 \ub7ad\ud0b9 \uadf8\ub798\ud504 \uacf5\ubd80'}, 'id': 'call_N9E5wxGaTBpZudPU5IkiRBac', 'type': 'tool_call'}], usage_metadata={'input_tokens': 97, 'output_tokens': 27, 'total_tokens': 124})]}, next=('tools',), config={'configurable': {'thread_id': '3', 'checkpoint_ns': '', 'checkpoint_id': '1ef757b6-a154-658c-8001-2fe079d4b550'}}, metadata={'source': 'loop', 'writes': {'chatbot': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_N9E5wxGaTBpZudPU5IkiRBac', 'function': {'arguments': '{\"query\":\"\ub7ad\uadf8\ub798\ud504 \ub7ad\ud0b9 \uadf8\ub798\ud504 \uacf5\ubd80\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 97, 'total_tokens': 124, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_54e2f484be', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-76b993a2-cfcd-42a6-870a-70bb44080fd4-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': '\ub7ad\uadf8\ub798\ud504 \ub7ad\ud0b9 \uadf8\ub798\ud504 \uacf5\ubd80'}, 'id': 'call_N9E5wxGaTBpZudPU5IkiRBac', 'type': 'tool_call'}], usage_metadata={'input_tokens': 97, 'output_tokens': 27, 'total_tokens': 124})]}}, 'step': 1, 'parents': {}}, created_at='2024-09-18T05:03:59.298067+00:00', parent_config={'configurable': {'thread_id': '3', 'checkpoint_ns': '', 'checkpoint_id': '1ef757b6-9943-61cf-8000-3d5416a18823'}}, tasks=(PregelTask(id='b54345af-217f-26eb-d082-49863f437ded', name='tools', path=('__pregel_pull', 'tools'), error=None, interrupts=(), state=None),))</pre> <p>\uc704\uc640 \uac19\uc774 tavily \uac00 query \ub97c \ub0a0\ub9ac\ub824\uace0 \ud569\ub2c8\ub2e4, \uadf8\ub7f0\ub370 query \uac00 \ub9d8\uc5d0 \uc548\ub4dc\ub124\uc694...?</p> <p>\"\uac80\uc0c9\uc744 \uc0dd\ub7b5\ud558\uace0 \ubc14\ub85c \ub2f5\ubcc0\uc744 \ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\"</p> In\u00a0[\u00a0]: Copied! <pre>from langchain_core.messages import AIMessage, ToolMessage\n\nanswer = (\n    \"\ub7ad\uadf8\ub798\ud504\ub294 stateful, multi-actor LLM applications \ub97c \ub9cc\ub4dc\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac \ub780\ub2e4\"\n)\nnew_messages = [\n    # The LLM API expects some ToolMessage to match its tool call. We'll satisfy that here.\n    ToolMessage(content=answer, tool_call_id=existing_message.tool_calls[0][\"id\"]),\n    # And then directly \"put words in the LLM's mouth\" by populating its response.\n    AIMessage(content=answer),\n]\n\nnew_messages[-1].pretty_print()\ngraph.update_state(\n    # Which state to update\n    config,\n    # The updated values to provide. The messages in our `State` are \"append-only\", meaning this will be appended\n    # to the existing state. We will review how to update existing messages in the next section!\n    {\"messages\": new_messages},\n)\n\nprint(\"\\n\\nLast 2 messages;\")\nprint(graph.get_state(config).values[\"messages\"][-2:])\n</pre> from langchain_core.messages import AIMessage, ToolMessage  answer = (     \"\ub7ad\uadf8\ub798\ud504\ub294 stateful, multi-actor LLM applications \ub97c \ub9cc\ub4dc\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac \ub780\ub2e4\" ) new_messages = [     # The LLM API expects some ToolMessage to match its tool call. We'll satisfy that here.     ToolMessage(content=answer, tool_call_id=existing_message.tool_calls[0][\"id\"]),     # And then directly \"put words in the LLM's mouth\" by populating its response.     AIMessage(content=answer), ]  new_messages[-1].pretty_print() graph.update_state(     # Which state to update     config,     # The updated values to provide. The messages in our `State` are \"append-only\", meaning this will be appended     # to the existing state. We will review how to update existing messages in the next section!     {\"messages\": new_messages}, )  print(\"\\n\\nLast 2 messages;\") print(graph.get_state(config).values[\"messages\"][-2:]) <pre>================================== Ai Message ==================================\n\n\ub7ad\uadf8\ub798\ud504\ub294 stateful, multi-actor LLM applications \ub97c \ub9cc\ub4dc\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac \ub780\ub2e4\n\n\nLast 2 messages;\n[ToolMessage(content='\ub7ad\uadf8\ub798\ud504\ub294 stateful, multi-actor LLM applications \ub97c \ub9cc\ub4dc\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac \ub780\ub2e4', id='69859f3c-907d-4679-8a07-aae03651d304', tool_call_id='call_N9E5wxGaTBpZudPU5IkiRBac'), AIMessage(content='\ub7ad\uadf8\ub798\ud504\ub294 stateful, multi-actor LLM applications \ub97c \ub9cc\ub4dc\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac \ub780\ub2e4', additional_kwargs={}, response_metadata={}, id='262a50f2-1c44-41f6-a34b-9d20353c23a4')]\n</pre> In\u00a0[\u00a0]: Copied! <pre>graph.get_state(config).next\n</pre> graph.get_state(config).next Out[\u00a0]: <pre>()</pre> <p>\uadf8\ub798\ud504\ub294 \ub05d\ub0ac\uc2b5\ub2c8\ub2e4. next \uac00 \ube44\uc5b4\uc788\uc8e0. \uac15\uc81c\ub85c \uc791\uc131\ud55c \ub2f5\ubcc0\uc744 \ub0b4\ubc49\uace0 \ub9c8\ubb34\ub9ac\uac00 \ub418\uc5c8\uace0\uc694. State \uc5c5\ub370\uc774\ud2b8\uac00 graph step (\uc5ec\uae30\uc11c\ub294 tool\ub85c \uc9c0\uc815\ub41c tavily \uac80\uc0c9) \uc744 \uc2dc\ubbac\ub808\uc774\uc158 \ud55c \uac83\uc774\uc8e0. LangSmith \uc5d0 \ucc0d\ud78c trace \ub3c4 \ud55c\ubc88 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. LangSmith trace \uc5d0\uc11c <code>update_state()</code> \uc758 \ub3d9\uc791 \ub85c\uadf8\ub97c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>Notice</p> <p><code>State</code> \ud0c0\uc785\uc744 \ub2e4\uc2dc \ud55c\ubc88 \ubcf4\uba74, message \uac00 append \ub9cc \ub418\ub294 \ud615\ud0dc\uc785\ub2c8\ub2e4. overwrite, \ub36e\uc5b4\uc4f0\uae30\uac00 \ub418\ub294 \ud615\ud0dc\uac00 \uc544\ub2c8\uc8e0.</p> <pre>class State(TypedDict):\n    messages: Annotated[list, add_messages]\n</pre> <p><code>update_state</code> \ub3c4 \ub9c8\ucc2c\uac00\uc9c0 \uc785\ub2c8\ub2e4, \uadf8\ub798\uc11c tool \ub178\ub4dc\uc5d0 \uac00\uc57c\ud560 \ucc28\ub840\ub97c \uc0dd\ub7b5\ud558\uace0 \uc5c5\ub370\uc774\ud2b8\ub85c \uba54\uc138\uc9c0\ub97c \uc774\uc5b4 \ubd99\uc778 \uac83\uc774\uc8e0.</p> <p>\uae30\ubcf8\uc801\uc73c\ub85c\ub294 \uc5c5\ub370\uc774\ud2b8 \ud558\ub294 \ud589\uc704\ub294 \ub9c8\uc9c0\ub9c9 \ub178\ub4dc\uc5d0\uc11c \ub098\uc544\uac00\ub294 \uac83\uc73c\ub85c \ub3d9\uc791\ud558\uc9c0\ub9cc, \uc5b4\ub290 \ub178\ub4dc\uc758 \uacb0\uacfc\uc778\uc9c0 \ub9c8\uc74c\ub300\ub85c \uc9c0\uc815\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ubc88\uc5d0\ub294 \"chatbot\" \ub178\ub4dc\uc5d0\uc11c \uc2e4\ud589\ud55c \uac83\uc73c\ub85c \ucc98\ub9ac\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>graph.update_state(\n    config,\n    {\"messages\": [AIMessage(content=\"I'm an AI expert!\")]},\n    # Which node for this function to act as. It will automatically continue\n    # processing as if this node just ran.\n    as_node=\"chatbot\",\n)\n</pre> graph.update_state(     config,     {\"messages\": [AIMessage(content=\"I'm an AI expert!\")]},     # Which node for this function to act as. It will automatically continue     # processing as if this node just ran.     as_node=\"chatbot\", ) Out[\u00a0]: <pre>{'configurable': {'thread_id': '3',\n  'checkpoint_ns': '',\n  'checkpoint_id': '1ef7580d-2b61-6742-8003-204dc6472893'}}</pre> <p>LangSmith trace \uc5d0\uc11c \uacb0\uacfc\ub97c \ud655\uc778 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> <p>Notice \ud2b8\ub808\uc774\uc2a4\ub97c \ubcf4\uba74, \uc774\ubbf8 \ub2e4 \ub05d\ub09c \uadf8\ub798\ud504\uc5d0\uc11c \ucd94\uac00\ub85c \ub354 \uc9c4\ud589\ud588\uace0, \ub9c8\uc9c0\ub9c9\uc5d0 <code>tools_condition</code> \uc774 \uc218\ud589\ub418\uace0 \ub05d\ub0ac\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Image, display\n\ntry:\n    display(Image(graph.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    pass\n</pre> from IPython.display import Image, display  try:     display(Image(graph.get_graph().draw_mermaid_png())) except Exception:     # This requires some extra dependencies and is optional     pass In\u00a0[\u00a0]: Copied! <pre>snapshot = graph.get_state(config)\nprint(snapshot.values[\"messages\"][-3:])\nprint(snapshot.next)\n</pre> snapshot = graph.get_state(config) print(snapshot.values[\"messages\"][-3:]) print(snapshot.next) <pre>[ToolMessage(content='\ub7ad\uadf8\ub798\ud504\ub294 stateful, multi-actor LLM applications \ub97c \ub9cc\ub4dc\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac \ub780\ub2e4', id='69859f3c-907d-4679-8a07-aae03651d304', tool_call_id='call_N9E5wxGaTBpZudPU5IkiRBac'), AIMessage(content='\ub7ad\uadf8\ub798\ud504\ub294 stateful, multi-actor LLM applications \ub97c \ub9cc\ub4dc\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac \ub780\ub2e4', additional_kwargs={}, response_metadata={}, id='262a50f2-1c44-41f6-a34b-9d20353c23a4'), AIMessage(content=\"I'm an AI expert!\", additional_kwargs={}, response_metadata={}, id='1666e433-9d66-4fd2-a4ff-464a33564ca7')]\n()\n</pre> In\u00a0[\u00a0]: Copied! <pre>user_input = \"\ub098\ub294 \uc9c0\uae08 \ub7ad\uadf8\ub798\ud504\ub97c \ubc30\uc6b0\uace0 \uc788\ub294\ub370, \ub300\uc2e0 \uc870\uc0ac \uc880 \ud574\uc904\ub798?\"\nconfig = {\"configurable\": {\"thread_id\": \"4\"}}  # we'll use thread_id = 4 here\nevents = graph.stream(\n    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n</pre> user_input = \"\ub098\ub294 \uc9c0\uae08 \ub7ad\uadf8\ub798\ud504\ub97c \ubc30\uc6b0\uace0 \uc788\ub294\ub370, \ub300\uc2e0 \uc870\uc0ac \uc880 \ud574\uc904\ub798?\" config = {\"configurable\": {\"thread_id\": \"4\"}}  # we'll use thread_id = 4 here events = graph.stream(     {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\" ) for event in events:     if \"messages\" in event:         event[\"messages\"][-1].pretty_print() <pre>================================ Human Message =================================\n\n\ub098\ub294 \uc9c0\uae08 \ub7ad\uadf8\ub798\ud504\ub97c \ubc30\uc6b0\uace0 \uc788\ub294\ub370, \ub300\uc2e0 \uc870\uc0ac \uc880 \ud574\uc904\ub798?\n================================== Ai Message ==================================\nTool Calls:\n  tavily_search_results_json (call_85Z84vjd60rJ18pWRb00GuWI)\n Call ID: call_85Z84vjd60rJ18pWRb00GuWI\n  Args:\n    query: \ub7ad\uadf8\ub798\ud504 \ub7ad\ud06c\uadf8\ub798\ud504 \ubc30\uc6b0\uae30\n</pre> <p>Next, \uc774\ubc88\uc5d0\ub294 \ucffc\ub9ac\uac00 \uc774\ub807\uac8c \ub098\uc654\uc5b4\uc694.</p> <ul> <li>\"\ub7ad\uadf8\ub798\ud504 \ub7ad\ud06c\uadf8\ub798\ud504 \ubc30\uc6b0\uae30\"</li> </ul> <p>\ucffc\ub9ac\uac00 \ub9c8\uc74c\uc5d0 \uc548\ub4e7\ub2c8\ub2e4. \ubc14\uafd4\uc11c \uc774\uc5b4\uac00\uaca0\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>from langchain_core.messages import AIMessage\n\nsnapshot = graph.get_state(config)\nexisting_message = snapshot.values[\"messages\"][-1]\nprint(\"Original\")\nprint(\"Message ID\", existing_message.id)\nprint(existing_message.tool_calls[0])\nnew_tool_call = existing_message.tool_calls[0].copy()\nnew_tool_call[\"args\"][\"query\"] = \"\ub7ad\uadf8\ub798\ud504 Langgraph \uc0ac\uc6a9\ubc95\"\nnew_message = AIMessage(\n    content=existing_message.content,\n    tool_calls=[new_tool_call],\n    # Important! The ID is how LangGraph knows to REPLACE the message in the state rather than APPEND this messages\n    id=existing_message.id,\n)\n\nprint(\"Updated\")\nprint(new_message.tool_calls[0])\nprint(\"Message ID\", new_message.id)\ngraph.update_state(config, {\"messages\": [new_message]})\n\nprint(\"\\n\\nTool calls\")\ngraph.get_state(config).values[\"messages\"][-1].tool_calls\n</pre> from langchain_core.messages import AIMessage  snapshot = graph.get_state(config) existing_message = snapshot.values[\"messages\"][-1] print(\"Original\") print(\"Message ID\", existing_message.id) print(existing_message.tool_calls[0]) new_tool_call = existing_message.tool_calls[0].copy() new_tool_call[\"args\"][\"query\"] = \"\ub7ad\uadf8\ub798\ud504 Langgraph \uc0ac\uc6a9\ubc95\" new_message = AIMessage(     content=existing_message.content,     tool_calls=[new_tool_call],     # Important! The ID is how LangGraph knows to REPLACE the message in the state rather than APPEND this messages     id=existing_message.id, )  print(\"Updated\") print(new_message.tool_calls[0]) print(\"Message ID\", new_message.id) graph.update_state(config, {\"messages\": [new_message]})  print(\"\\n\\nTool calls\") graph.get_state(config).values[\"messages\"][-1].tool_calls <pre>Original\nMessage ID run-29a7fc2d-236d-44a3-80ec-bbda0576d745-0\n{'name': 'tavily_search_results_json', 'args': {'query': '\ub7ad\uadf8\ub798\ud504 Langgraph \uc0ac\uc6a9 \ubc95'}, 'id': 'call_85Z84vjd60rJ18pWRb00GuWI', 'type': 'tool_call'}\nUpdated\n{'name': 'tavily_search_results_json', 'args': {'query': '\ub7ad\uadf8\ub798\ud504 Langgraph \uc0ac\uc6a9\ubc95'}, 'id': 'call_85Z84vjd60rJ18pWRb00GuWI', 'type': 'tool_call'}\nMessage ID run-29a7fc2d-236d-44a3-80ec-bbda0576d745-0\n\n\nTool calls\n</pre> Out[\u00a0]: <pre>[{'name': 'tavily_search_results_json',\n  'args': {'query': '\ub7ad\uadf8\ub798\ud504 Langgraph \uc0ac\uc6a9\ubc95'},\n  'id': 'call_85Z84vjd60rJ18pWRb00GuWI',\n  'type': 'tool_call'}]</pre> <p>Notice</p> <p>'\ub7ad\uadf8\ub798\ud504 Langgraph \uc0ac\uc6a9\ubc95' \uc774\ub77c\uace0 \ucffc\ub9ac \ub0b4\uc6a9\uc744 \ubc14\uafd4 \ubd24\uc2b5\ub2c8\ub2e4.</p> <p>LangSmith trace \uc744 \uccb4\ud06c\ud574\ubcf4\uba74, \ub2e4\uc74c tool \ud638\ucd9c\uc5d0 \ub0a0\ub824\uc57c\ud560 \ucffc\ub9ac\uac00 \ubc14\ub010 \uac83\uc744 \ubcfc \uc218 \uc788\uc5b4\uc694.</p> <p>graph.stream \uc744 \uc774\uc5b4\uac00\uaca0\uc2b5\ub2c8\ub2e4. \uba54\uc138\uc9c0\ub294 \uc788\ub358 \uac83\uc744 \uad50\uccb4\ud588\uc73c\ub2c8 \ub2e4\uc2dc \ucd94\uac00\ub85c \ub123\uc744 \uac83\uc740 \uc5c6\uc2b5\ub2c8\ub2e4. None \uc744 \ucd94\uace0 \uc774\uc5b4\uac00\uaca0\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>events = graph.stream(None, config, stream_mode=\"values\")\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n</pre> events = graph.stream(None, config, stream_mode=\"values\") for event in events:     if \"messages\" in event:         event[\"messages\"][-1].pretty_print() <pre>================================== Ai Message ==================================\nTool Calls:\n  tavily_search_results_json (call_85Z84vjd60rJ18pWRb00GuWI)\n Call ID: call_85Z84vjd60rJ18pWRb00GuWI\n  Args:\n    query: \ub7ad\uadf8\ub798\ud504 Langgraph \uc0ac\uc6a9\ubc95\n================================= Tool Message =================================\nName: tavily_search_results_json\n\n[{\"url\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"content\": \"LangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner. It simplifies the development process by enabling the creation of cyclical graphs, which are essential for developing ...\"}, {\"url\": \"https://langchain-ai.github.io/langgraph/how-tos/\", \"content\": \"One of LangGraph's main benefits is that it makes human-in-the-loop workflows easy. These guides cover common examples of that. How to add breakpoints. How to add dynamic breakpoints. How to edit graph state. How to wait for user input. How to view and update past graph state. Review tool calls.\"}]\n================================== Ai Message ==================================\n\n\ub7ad\uadf8\ub798\ud504(Langgraph)\ub294 LangChain \uc0dd\ud0dc\uacc4 \ub0b4\uc758 \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c, \uc5ec\ub7ec LLM \uc5d0\uc774\uc804\ud2b8(\ub610\ub294 \uccb4\uc778)\ub97c \uc815\uc758\ud558\uace0 \uc870\uc815\ud558\uba70 \uc2e4\ud589\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4. \ub7ad\uadf8\ub798\ud504\ub97c \ud1b5\ud574 \uac1c\ubc1c\uc790\ub294 \uc21c\ud658 \uadf8\ub798\ud504\ub97c \uc27d\uac8c \uc0dd\uc131\ud560 \uc218 \uc788\uc5b4 \ubcf5\uc7a1\ud55c \uc791\uc5c5\uc744 \ubcf4\ub2e4 \uad6c\uc870\uc801\uc73c\ub85c \ucc98\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc5ec\uae30 \uba87 \uac00\uc9c0 \uc720\uc6a9\ud55c \uc790\ub8cc\ub97c \uc18c\uac1c\ud560\uac8c\uc694:\n\n1. [DataCamp\uc758 LangGraph \ud29c\ud1a0\ub9ac\uc5bc](https://www.datacamp.com/tutorial/langgraph-tutorial): LangGraph\uc758 \uae30\ubcf8 \uac1c\ub150\uacfc \uc0ac\uc6a9\ubc95\uc5d0 \ub300\ud574 \uc0c1\uc138\ud788 \uc124\uba85\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n2. [LangChain \uacf5\uc2dd \ubb38\uc11c\uc758 How-To \uac00\uc774\ub4dc](https://langchain-ai.github.io/langgraph/how-tos/): \uc778\uac04\uc758 \uac1c\uc785\uc774 \ud544\uc694\ud55c \uc791\uc5c5 \ud750\ub984\uc744 \uc27d\uac8c \ub9cc\ub4e4 \uc218 \uc788\ub3c4\ub85d \ub2e4\uc591\ud55c \uc608\uc81c\ub97c \ub2e4\ub8e8\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\ub294 \uc911\ub2e8\uc810 \ucd94\uac00, \ub3d9\uc801 \uc911\ub2e8\uc810 \ucd94\uac00, \uadf8\ub798\ud504 \uc0c1\ud0dc \ud3b8\uc9d1 \ub4f1\uc758 \ubc29\ubc95\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc774 \uc790\ub8cc\ub4e4\uc744 \ud1b5\ud574 \ub7ad\uadf8\ub798\ud504\uc5d0 \ub300\ud55c \uc774\ud574\ub97c \ub192\uc77c \uc218 \uc788\uc744 \uac83\uc785\ub2c8\ub2e4! \ucd94\uac00\uc801\uc778 \uc9c8\ubb38\uc774 \uc788\uc73c\uba74 \uc5b8\uc81c\ub4e0\uc9c0 \ubb3c\uc5b4\ubcf4\uc138\uc694.\n</pre> <p>\ucd5c\uc885 trace \ub97c \ubcf4\uba74, \uccab \uc9c8\ubb38\uc5d0 \uc774\uc5b4\uc11c \ucffc\ub9ac\uac00 \ubc14\ub00c\uace0 tool \uc774 \ud638\ucd9c\ub418\uace0, \uadf8 \uac80\uc0c9\uacb0\uacfc\ub85c \ub2f5\ubcc0\ud55c \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc774 \uadf8\ub798\ud504\uc5d0 \uc774\uc5b4\uc11c \uc9c8\ubb38\uc744 \ud574\ubcf4\uba74, memory \uc5d0 \ub4e4\uc5b4\uac04 \ub0b4\uc6a9\uc744 \ucc38\uc870\ud574\uc11c \ub2f5\ubcc0\uc744 \ud558\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>events = graph.stream(\n    {\n        \"messages\": (\n            \"user\",\n            \"\uc790 \ub108\ub294 \ubb58 \ubc30\uc6e0\uc9c0?\",\n        )\n    },\n    config,\n    stream_mode=\"values\",\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n</pre> events = graph.stream(     {         \"messages\": (             \"user\",             \"\uc790 \ub108\ub294 \ubb58 \ubc30\uc6e0\uc9c0?\",         )     },     config,     stream_mode=\"values\", ) for event in events:     if \"messages\" in event:         event[\"messages\"][-1].pretty_print() <pre>================================ Human Message =================================\n\n\uc790 \ub108\ub294 \ubb58 \ubc30\uc6e0\uc9c0?\n================================== Ai Message ==================================\n\n\ub7ad\uadf8\ub798\ud504(Langgraph)\uc5d0 \ub300\ud574 \ubc30\uc6b4 \ub0b4\uc6a9\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:\n\n1. **\uc815\uc758 \ubc0f \ubaa9\uc801**: \ub7ad\uadf8\ub798\ud504\ub294 LangChain \uc0dd\ud0dc\uacc4\uc758 \uc77c\ubd80\ub85c, \uc5ec\ub7ec LLM(\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378) \uc5d0\uc774\uc804\ud2b8\ub098 \uccb4\uc778\uc744 \uc815\uc758\ud558\uace0 \uc870\uc815\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc8fc\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc785\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ubcf5\uc7a1\ud55c \uc791\uc5c5\uc744 \uad6c\uc870\uc801\uc73c\ub85c \ucc98\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n2. **\uae30\ub2a5**: \ub7ad\uadf8\ub798\ud504\ub294 \uc21c\ud658 \uadf8\ub798\ud504\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\ub294 \uae30\ub2a5\uc744 \uc81c\uacf5\ud558\uc5ec \uac1c\ubc1c\uc790\uac00 \ub2e4\uc591\ud55c \uc791\uc5c5 \ud750\ub984\uc744 \uc27d\uac8c \uad6c\uc131\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4. \ub610\ud55c, \uc778\uac04\uc758 \uac1c\uc785\uc774 \ud544\uc694\ud55c \uc791\uc5c5 \ud750\ub984\uc744 \uc27d\uac8c \ub9cc\ub4e4 \uc218 \uc788\ub294 \ub2e4\uc591\ud55c \ub3c4\uad6c\uc640 \ubc29\ubc95\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4.\n\n3. **\uc720\uc6a9\ud55c \uc790\ub8cc**: \ub7ad\uadf8\ub798\ud504\uc5d0 \ub300\ud55c \ud29c\ud1a0\ub9ac\uc5bc\uacfc How-To \uac00\uc774\ub4dc\uac00 \uc81c\uacf5\ub418\uc5b4 \uc0ac\uc6a9\uc790\uac00 \uc27d\uac8c \ubc30\uc6b8 \uc218 \uc788\ub3c4\ub85d \ub3d5\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\ub294 \uc911\ub2e8\uc810 \ucd94\uac00, \uadf8\ub798\ud504 \uc0c1\ud0dc \ud3b8\uc9d1, \uacfc\uac70 \uadf8\ub798\ud504 \uc0c1\ud0dc \ubcf4\uae30 \ubc0f \uc5c5\ub370\uc774\ud2b8\ud558\uae30\uc640 \uac19\uc740 \uae30\ub2a5\uc774 \ud3ec\ud568\ub429\ub2c8\ub2e4.\n\n\uc774 \uc815\ubcf4\ub4e4\uc740 \ub7ad\uadf8\ub798\ud504\ub97c \ubc30\uc6b0\ub294 \ub370 \uc911\uc694\ud55c \uae30\ucd08\uac00 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub354 \uad81\uae08\ud55c \uc810\uc774\ub098 \ucd94\uac00\uc801\uc73c\ub85c \uc54c\uace0 \uc2f6\uc740 \ub0b4\uc6a9\uc774 \uc788\uc73c\uba74 \ub9d0\uc500\ud574 \uc8fc\uc138\uc694!\n</pre> <p>\uc790. \uc5ec\uae30\uae4c\uc9c0 Agent \uc218\ud589 \uc911 \uac1c\uc785\uc5d0 \uc131\uacf5\ud588\uc2b5\ub2c8\ub2e4. \uc6d0\ud558\ub294 \ub300\ub85c \uc911\uac04\uc5d0 \ub4e4\uc5b4\uac00\uc11c \uba48\ucd94\uace0, \ubc14\uafb8\uace0 \uc2f6\uc740 \uac83 \ubc14\uafb8\uace0, \uc774\uc5b4\uac00\uace0 \ubaa8\ub450 \ub2e4 \uac00\ub2a5\ud569\ub2c8\ub2e4. \uc774 \uc815\ub3c4\uba74 \uaf64\ub098 \uc790\uc720\ub3c4\uac00 \uc0dd\uaca8\uc11c \uc6d0\ud558\ub294 \uac83\ub4e4\uc744 \uad6c\ud604\ud558\ub294 \uac83\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> <p>\ub610 \ub2e4\uc2dc \uc81c \uc758\uacac \ucca8\uc5b8\ud558\uc790\uba74,</p> <ul> <li>Agent \uc758 \ubb38\uc81c\ub294 \ucee8\ud2b8\ub864\uc774 \ub108\ubb34 \ud798\ub4e4\ub2e4\ub294 \uc810 \uc774\uc5c8\uc2b5\ub2c8\ub2e4.<ul> <li>\uc77c\uc744 \uc798\ud558\ub294 \uac83 \uac19\uc73c\uba74\uc11c\ub3c4, \uac00\ub054\uc529 \uace0\uc7a5\ub098\uae30\ub3c4 \ud558\uace0, \ubcf5\uc7a1\ud55c \uac83\uc740 \uc798 \ubabb\ud558\uae30\ub3c4 \ud569\ub2c8\ub2e4.</li> </ul> </li> <li>\uc774\ub807\uac8c \uac1c\uc785 \ucc3d\uad6c\ub97c \ub9cc\ub4e4\uc5b4\ub450\uba74, \ub9ce\uc740 \ubcf4\uc644\uc774 \uac00\ub2a5\ud574\uc11c \uc9c4\uc9dc\ub85c! \ub9e4\uc6b0! \uc720\uc6a9\ud569\ub2c8\ub2e4.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>class State(TypedDict):\n    messages: Annotated[list, add_messages]\n    # This flag is new\n    ask_human: bool\n</pre> class State(TypedDict):     messages: Annotated[list, add_messages]     # This flag is new     ask_human: bool <p><code>BaseModel</code> \uc744 \uc0c1\uc18d\ubc1b\uc544\uc11c Human \uc5d0\uac8c \uc694\uccad\ud560 \ub54c \ubcf4\ub0bc \ud1b5\ub85c\ub97c \ub9cc\ub4e4\uc5b4\uc90d\ub2c8\ub2e4. \uc774 \ud074\ub798\uc2a4\ub294 <code>tool</code>  \uc790\ub9ac\uc785\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>from langchain_core.pydantic_v1 import BaseModel\n\n\nclass RequestAssistance(BaseModel):\n    \"\"\"Escalate the conversation to an expert. Use this if you are unable to assist directly or if the user requires support beyond your permissions.\n\n    To use this function, relay the user's 'request' so the expert can provide the right guidance.\n    \"\"\"\n\n    request: str\n</pre> from langchain_core.pydantic_v1 import BaseModel   class RequestAssistance(BaseModel):     \"\"\"Escalate the conversation to an expert. Use this if you are unable to assist directly or if the user requires support beyond your permissions.      To use this function, relay the user's 'request' so the expert can provide the right guidance.     \"\"\"      request: str <pre>/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n\nFor example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\nwith: `from pydantic import BaseModel`\nor the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n\n  exec(code_obj, self.user_global_ns, self.user_ns)\n</pre> <p><code>chatbot</code> \ub178\ub4dc\ub97c \ub2e4\uc2dc \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4.</p> <ul> <li><code>llm</code> \uc5d0 <code>bind_tools()</code> \ud560 \ub54c, \uc704\uc5d0\uc11c \ub9cc\ub4e0 \ubaa8\ub378\uc744 \ucd94\uac00\ud558\uace0,</li> <li><code>ask_human</code> \ud50c\ub798\uadf8\ub97c llm \uc774 \ubd80\ub974\ub824\uace0 \ud55c\ub2e4\uba74, <code>true</code> \ub85c \ucf1c\uc8fc\ub294 \ucf54\ub4dc\ub97c \ub123\uc2b5\ub2c8\ub2e4.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>tool = TavilySearchResults(max_results=2)\ntools = [tool]\n\n# We can bind the llm to a tool definition, a pydantic model, or a json schema\nllm_with_tools = llm.bind_tools(tools + [RequestAssistance])\n\n\ndef chatbot(state: State):\n    response = llm_with_tools.invoke(state[\"messages\"])\n    ask_human = False\n    if (\n        response.tool_calls\n        and response.tool_calls[0][\"name\"] == RequestAssistance.__name__\n    ):\n        ask_human = True\n    return {\"messages\": [response], \"ask_human\": ask_human}\n</pre> tool = TavilySearchResults(max_results=2) tools = [tool]  # We can bind the llm to a tool definition, a pydantic model, or a json schema llm_with_tools = llm.bind_tools(tools + [RequestAssistance])   def chatbot(state: State):     response = llm_with_tools.invoke(state[\"messages\"])     ask_human = False     if (         response.tool_calls         and response.tool_calls[0][\"name\"] == RequestAssistance.__name__     ):         ask_human = True     return {\"messages\": [response], \"ask_human\": ask_human} <p>\uc774\uc5b4\uc11c \uadf8\ub798\ud504\ub3c4 \ub9cc\ub4e4\uc5b4 \uc90d\ub2c8\ub2e4.  tools \ub178\ub4dc\ub294 tavily tool \uc744 \ub9cc\ub4e4\uc5b4\uc8fc\uace0\uc694, \uc774\ub530 \uc544\ub798\uc5d0\uc11c human \ub178\ub4dc\ub97c \ub530\ub85c \ucd94\uac00\ud569\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>graph_builder = StateGraph(State)\n\ngraph_builder.add_node(\"chatbot\", chatbot)\ngraph_builder.add_node(\"tools\", ToolNode(tools=[tool]))\n</pre> graph_builder = StateGraph(State)  graph_builder.add_node(\"chatbot\", chatbot) graph_builder.add_node(\"tools\", ToolNode(tools=[tool])) <p><code>human</code> \ub178\ub4dc\ub97c \ub9cc\ub4e4\uc5b4 \uc90d\ub2c8\ub2e4.</p> <p><code>human</code> \ub178\ub4dc \ud568\uc218\ub294 interrupt \uac00 \uac78\ub824\uc11c \uc0ac\ub78c\uc5d0\uac8c \uc804\ub2ec \ub420 \ud14c\uc9c0\ub9cc, \ud639\uc2dc\ub77c\ub3c4 \uc0ac\ub78c\uc774 state \ub97c \ubcc0\uacbd\ud558\uc9c0 \uc54a\uc558\ub2e4\uba74, \uadf8 \uc0ac\uc2e4\uc744 \uba54\uc138\uc9c0\uc5d0 \ucd94\uac00\ud558\ub3c4\ub85d \ud558\uac8c\uc2b5\ub2c8\ub2e4. \uadf8\ub9ac\uace0, \ub2e4\uc2dc state \uc5d0\uc11c <code>ask_human</code> \ud50c\ub798\uadf8\ub294 false \ub85c \uaebc\uc90d\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>from langchain_core.messages import AIMessage, ToolMessage\n\n\ndef create_response(response: str, ai_message: AIMessage):\n    return ToolMessage(\n        content=response,\n        tool_call_id=ai_message.tool_calls[0][\"id\"],\n    )\n\n\ndef human_node(state: State):\n    new_messages = []\n    if not isinstance(state[\"messages\"][-1], ToolMessage):\n        # Typically, the user will have updated the state during the interrupt.\n        # If they choose not to, we will include a placeholder ToolMessage to\n        # let the LLM continue.\n        new_messages.append(\n            create_response(\"No response from human.\", state[\"messages\"][-1])\n        )\n    return {\n        # Append the new messages\n        \"messages\": new_messages,\n        # Unset the flag\n        \"ask_human\": False,\n    }\n\n\ngraph_builder.add_node(\"human\", human_node)\n</pre> from langchain_core.messages import AIMessage, ToolMessage   def create_response(response: str, ai_message: AIMessage):     return ToolMessage(         content=response,         tool_call_id=ai_message.tool_calls[0][\"id\"],     )   def human_node(state: State):     new_messages = []     if not isinstance(state[\"messages\"][-1], ToolMessage):         # Typically, the user will have updated the state during the interrupt.         # If they choose not to, we will include a placeholder ToolMessage to         # let the LLM continue.         new_messages.append(             create_response(\"No response from human.\", state[\"messages\"][-1])         )     return {         # Append the new messages         \"messages\": new_messages,         # Unset the flag         \"ask_human\": False,     }   graph_builder.add_node(\"human\", human_node) <p>\uc774\ubc88\uc5d4 \ub178\ub4dc \uc120\ud0dd \ub85c\uc9c1\uc744 \ucd94\uac00\ud558\uaca0\uc2b5\ub2c8\ub2e4. <code>select_next_node</code> \uac00 \ud544\uc694\ud558\uba74 <code>human</code> \ub178\ub4dc\ub85c \ubcf4\ub0b4\uc90d\ub2c8\ub2e4. \uadf8\uac8c \uc544\ub2c8\ub77c\uba74, \uc774\uc804 \ucc98\ub7fc 'tools_condition' \uc73c\ub85c \ubcf4\ub0c5\ub2c8\ub2e4.</p> <p><code>tools_condition</code> \uc740 <code>tool_calls</code> \uc758 \ub2f5\ubcc0\uc744 \ubc1b\uc558\ub294\uc9c0 \ubcf4\uace0, <code>action</code> \ub178\ub4dc\ub85c \ubcf4\ub0b4\uac70\ub098 \uadf8\ub798\ud504\ub97c \ub05d\ub0c5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>def select_next_node(state: State):\n    if state[\"ask_human\"]:\n        return \"human\"\n    # Otherwise, we can route as before\n    return tools_condition(state)\n\n\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    select_next_node,\n    {\"human\": \"human\", \"tools\": \"tools\", \"__end__\": \"__end__\"},\n)\n</pre> def select_next_node(state: State):     if state[\"ask_human\"]:         return \"human\"     # Otherwise, we can route as before     return tools_condition(state)   graph_builder.add_conditional_edges(     \"chatbot\",     select_next_node,     {\"human\": \"human\", \"tools\": \"tools\", \"__end__\": \"__end__\"}, ) <p>\ud56d\uc0c1 \ub418\ub3cc\ub824 \ubcf4\ub0b4\ub294 edge \ub4e4\uc744 \ucd94\uac00\ud574\uc8fc\uace0 \ub9c8\ubb34\ub9ac\ud569\ub2c8\ub2e4. <code>human</code> \ub178\ub4dc \uc804\uc5d0\ub294 \uaf2d interrupt \ud558\uace0\uc694.</p> In\u00a0[\u00a0]: Copied! <pre># The rest is the same\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.add_edge(\"human\", \"chatbot\")\ngraph_builder.add_edge(START, \"chatbot\")\nmemory = MemorySaver()\ngraph = graph_builder.compile(\n    checkpointer=memory,\n    # We interrupt before 'human' here instead.\n    interrupt_before=[\"human\"],\n)\n</pre> # The rest is the same graph_builder.add_edge(\"tools\", \"chatbot\") graph_builder.add_edge(\"human\", \"chatbot\") graph_builder.add_edge(START, \"chatbot\") memory = MemorySaver() graph = graph_builder.compile(     checkpointer=memory,     # We interrupt before 'human' here instead.     interrupt_before=[\"human\"], ) <p>visualize \ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \uc0dd\uacbc\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Image, display\n\ntry:\n    display(Image(graph.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    pass\n</pre> from IPython.display import Image, display  try:     display(Image(graph.get_graph().draw_mermaid_png())) except Exception:     # This requires some extra dependencies and is optional     pass <p>\uc774\uc81c \uadf8\ub798\ud504\ub974\ub974 \uc2e4\ud589\uc2dc\ucf1c\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>user_input = \"AI agent \ub9cc\ub4dc\ub294 \ub370 \ub3c4\uc6c0 \uc880 \uc918\ubd10\ub77c!\"\nconfig = {\"configurable\": {\"thread_id\": \"5\"}}\n# The config is the **second positional argument** to stream() or invoke()!\nevents = graph.stream(\n    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n</pre> user_input = \"AI agent \ub9cc\ub4dc\ub294 \ub370 \ub3c4\uc6c0 \uc880 \uc918\ubd10\ub77c!\" config = {\"configurable\": {\"thread_id\": \"5\"}} # The config is the **second positional argument** to stream() or invoke()! events = graph.stream(     {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\" ) for event in events:     if \"messages\" in event:         event[\"messages\"][-1].pretty_print() <pre>================================ Human Message =================================\n\nAI agent \ub9cc\ub4dc\ub294 \ub370 \ub3c4\uc6c0 \uc880 \uc918\ubd10\ub77c!\n================================== Ai Message ==================================\nTool Calls:\n  RequestAssistance (call_5DIhb5uSzfqjaR527dQuEdZs)\n Call ID: call_5DIhb5uSzfqjaR527dQuEdZs\n  Args:\n    request: AI agent \ub9cc\ub4dc\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574 \uc790\uc138\ud788 \uc124\uba85\ud574 \uc8fc\uc138\uc694. \ud544\uc694\ud55c \uae30\uc220, \ub3c4\uad6c, \uc5b8\uc5b4, \uadf8\ub9ac\uace0 \ud504\ub85c\uc81d\ud2b8\uc758 \uc8fc\uc694 \ub2e8\uacc4\uc5d0 \ub300\ud574 \uc54c\ub824 \uc8fc\uc138\uc694.\n</pre> <p>\ub2e4\uc74c \ub3c4\uad6c\ub85c Human \uc744 \ubf51\uc558\uc2b5\ub2c8\ub2e4. request \ubb38\uad6c\ub3c4 \uc368\uc92c\uace0\uc694, interrupt \uac00 \ub418\uc5c8\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>snapshot = graph.get_state(config)\nsnapshot.next\n</pre> snapshot = graph.get_state(config) snapshot.next Out[\u00a0]: <pre>('human',)</pre> <p>\uc774\uc81c \uc6b0\ub9ac\uac00 Human Expert \uac00 \ub418\uc5b4\uc11c \ub300\ub2f5\uc744 \ud574\uc90d\ub2c8\ub2e4.</p> <ol> <li><code>ToolMessage</code> \uc5d0 \ud560\ub9d0\uc744 \ub2f5\uc544\uc11c \ucd94\uac00\ud569\ub2c8\ub2e4, \uc790\uc5f0\uc2a4\ub798 <code>chatbot</code> \ub178\ub4dc\ub85c \ub118\uc5b4\uac00\uc8e0.</li> <li><code>update_state</code> \ub97c \ud638\ucd9c\uc5d0\uc11c \uba54\uc138\uc9c0\ub97c \uc5c5\ub370\uc774\ud2b8 \ud574\uc8fc\uaca0\uc2b5\ub2c8\ub2e4.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>ai_message = snapshot.values[\"messages\"][-1]\nhuman_response = (\n    \"\ub0b4\uac00 \uc804\ubb38\uac00\uc57c. LangGraph \ub97c \uc0ac\uc6a9\ud558\uba74 AI Agent\ub97c \uc27d\uac8c \uc798 \ub9cc\ub4e4\uc218\uac00 \uc788\uc9c0.\"\n)\ntool_message = create_response(human_response, ai_message)\ngraph.update_state(config, {\"messages\": [tool_message]})\n</pre> ai_message = snapshot.values[\"messages\"][-1] human_response = (     \"\ub0b4\uac00 \uc804\ubb38\uac00\uc57c. LangGraph \ub97c \uc0ac\uc6a9\ud558\uba74 AI Agent\ub97c \uc27d\uac8c \uc798 \ub9cc\ub4e4\uc218\uac00 \uc788\uc9c0.\" ) tool_message = create_response(human_response, ai_message) graph.update_state(config, {\"messages\": [tool_message]}) Out[\u00a0]: <pre>{'configurable': {'thread_id': '5',\n  'checkpoint_ns': '',\n  'checkpoint_id': '1ef75a12-c4a7-69e5-8002-83c61e7b0665'}}</pre> In\u00a0[\u00a0]: Copied! <pre>graph.get_state(config).values[\"messages\"]\n</pre> graph.get_state(config).values[\"messages\"] Out[\u00a0]: <pre>[HumanMessage(content='AI agent \ub9cc\ub4dc\ub294 \ub370 \ub3c4\uc6c0 \uc880 \uc918\ubd10\ub77c!', additional_kwargs={}, response_metadata={}, id='2a2cec0b-b816-4e09-8c05-b7127d0b1903'),\n AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_5DIhb5uSzfqjaR527dQuEdZs', 'function': {'arguments': '{\"request\":\"AI agent \ub9cc\ub4dc\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574 \uc790\uc138\ud788 \uc124\uba85\ud574 \uc8fc\uc138\uc694. \ud544\uc694\ud55c \uae30\uc220, \ub3c4\uad6c, \uc5b8\uc5b4, \uadf8\ub9ac\uace0 \ud504\ub85c\uc81d\ud2b8\uc758 \uc8fc\uc694 \ub2e8\uacc4\uc5d0 \ub300\ud574 \uc54c\ub824 \uc8fc\uc138\uc694.\"}', 'name': 'RequestAssistance'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 153, 'total_tokens': 198, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-02921127-a8e4-4d78-89ba-065ec995e60d-0', tool_calls=[{'name': 'RequestAssistance', 'args': {'request': 'AI agent \ub9cc\ub4dc\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574 \uc790\uc138\ud788 \uc124\uba85\ud574 \uc8fc\uc138\uc694. \ud544\uc694\ud55c \uae30\uc220, \ub3c4\uad6c, \uc5b8\uc5b4, \uadf8\ub9ac\uace0 \ud504\ub85c\uc81d\ud2b8\uc758 \uc8fc\uc694 \ub2e8\uacc4\uc5d0 \ub300\ud574 \uc54c\ub824 \uc8fc\uc138\uc694.'}, 'id': 'call_5DIhb5uSzfqjaR527dQuEdZs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 153, 'output_tokens': 45, 'total_tokens': 198}),\n ToolMessage(content='\ub0b4\uac00 \uc804\ubb38\uac00\uc57c. LangGraph \ub97c \uc0ac\uc6a9\ud558\uba74 AI Agent\ub97c \uc27d\uac8c \uc798 \ub9cc\ub4e4\uc218\uac00 \uc788\uc9c0.', id='bffbbf81-e45b-48ab-a0c4-d41899cce362', tool_call_id='call_5DIhb5uSzfqjaR527dQuEdZs')]</pre> <p>resume \ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc5ed\uc2dc\ub098 stream \uc744 \uc774\uc5b4\uac00\ub294\ub370 \uba54\uc138\uc9c0\ub294 \uc774\ubbf8 \ucd94\uac00\ud588\uc73c\ub2c8 <code>None</code> \uc744 \ub123\uc5b4\uc8fc\uba74 \ub429\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>events = graph.stream(None, config, stream_mode=\"values\")\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n</pre> events = graph.stream(None, config, stream_mode=\"values\") for event in events:     if \"messages\" in event:         event[\"messages\"][-1].pretty_print() <pre>================================= Tool Message =================================\n\n\ub0b4\uac00 \uc804\ubb38\uac00\uc57c. LangGraph \ub97c \uc0ac\uc6a9\ud558\uba74 AI Agent\ub97c \uc27d\uac8c \uc798 \ub9cc\ub4e4\uc218\uac00 \uc788\uc9c0.\n================================= Tool Message =================================\n\n\ub0b4\uac00 \uc804\ubb38\uac00\uc57c. LangGraph \ub97c \uc0ac\uc6a9\ud558\uba74 AI Agent\ub97c \uc27d\uac8c \uc798 \ub9cc\ub4e4\uc218\uac00 \uc788\uc9c0.\n================================== Ai Message ==================================\n\nAI \uc5d0\uc774\uc804\ud2b8\ub97c \ub9cc\ub4dc\ub294 \ub370 LangGraph\ub97c \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:\n\n1. **LangGraph \uc18c\uac1c**: LangGraph\ub294 \uc790\uc5f0\uc5b4 \ucc98\ub9ac(NLP) \ubaa8\ub378\uc744 \uc27d\uac8c \uad6c\ucd95\ud558\uace0 \uc2e4\ud589\ud560 \uc218 \uc788\ub3c4\ub85d \ub3c4\uc640\uc8fc\ub294 \ud504\ub808\uc784\uc6cc\ud06c\uc785\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ubcf5\uc7a1\ud55c AI \uc5d0\uc774\uc804\ud2b8\ub97c \uac04\ud3b8\ud558\uac8c \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n2. **\ud544\uc694\ud55c \uae30\uc220 \ubc0f \ub3c4\uad6c**:\n   - **\ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4**: Python\uc740 NLP \ubc0f AI \uac1c\ubc1c\uc5d0 \ub110\ub9ac \uc0ac\uc6a9\ub429\ub2c8\ub2e4.\n   - **\ub77c\uc774\ube0c\ub7ec\ub9ac**: TensorFlow, PyTorch, Hugging Face Transformers \ub4f1\uacfc \uac19\uc740 \uba38\uc2e0\ub7ec\ub2dd \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n   - **LangGraph**: LangGraph \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc124\uce58\ud558\uace0 \ud65c\uc6a9\ud569\ub2c8\ub2e4.\n\n3. **\uc8fc\uc694 \ub2e8\uacc4**:\n   - **\ubaa9\ud45c \uc124\uc815**: AI \uc5d0\uc774\uc804\ud2b8\uac00 \uc218\ud589\ud560 \uc791\uc5c5\uacfc \uae30\ub2a5\uc744 \uc815\uc758\ud569\ub2c8\ub2e4.\n   - **\ub370\uc774\ud130 \uc218\uc9d1**: \ubaa8\ub378 \ud6c8\ub828\uc5d0 \uc0ac\uc6a9\ud560 \ub370\uc774\ud130\uc14b\uc744 \uc218\uc9d1\ud569\ub2c8\ub2e4.\n   - **\ubaa8\ub378 \uc120\ud0dd**: \uc801\ud569\ud55c NLP \ubaa8\ub378\uc744 \uc120\ud0dd\ud569\ub2c8\ub2e4.\n   - **\ubaa8\ub378 \ud6c8\ub828**: \uc120\ud0dd\ud55c \ubaa8\ub378\uc744 LangGraph\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ud569\ub2c8\ub2e4.\n   - **\ud14c\uc2a4\ud2b8 \ubc0f \ud3c9\uac00**: \ud6c8\ub828\ub41c \ubaa8\ub378\uc744 \ud14c\uc2a4\ud2b8\ud558\uace0 \uc131\ub2a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.\n   - **\ubc30\ud3ec**: \ucd5c\uc885 \ubaa8\ub378\uc744 \uc2e4\uc81c \ud658\uacbd\uc5d0 \ubc30\ud3ec\ud569\ub2c8\ub2e4.\n\n4. **\ucd94\uac00 \uc790\ub8cc**: LangGraph\uc758 \uacf5\uc2dd \ubb38\uc11c\ub098 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \ucc38\uace0\ud558\uc5ec \ub354 \uae4a\uc740 \uc774\ud574\ub97c \ub3d5\uace0 \uad6c\uccb4\uc801\uc778 \uc608\uc81c\ub4e4\uc744 \uc0b4\ud3b4\ubcf4\uc138\uc694.\n\n\uc774\uc640 \uac19\uc740 \ub2e8\uacc4\ub85c AI \uc5d0\uc774\uc804\ud2b8\ub97c \uad6c\ucd95\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub3c4\uc6c0\uc774 \ub354 \ud544\uc694\ud558\uba74 \uad6c\uccb4\uc801\uc778 \uc9c8\ubb38\uc744 \ud574 \uc8fc\uc138\uc694!\n</pre> <p>LangSmith Trace \ub97c \ucc38\uace0\ud574\ubcf4\uba74, Human \ub178\ub4dc\uc5d0\uc11c \uacb0\uacfc\ub97c \uc5bb\uc5b4 \ub300\ub2f5\uc744 \uc774\uc5b4\uac04 \uac83\uc744 \ud655\uc778 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc5ec\uae30\uae4c\uc9c0 LangGraph \uc758 \uae30\ubcf8\uc801\uc778 \uc0ac\uc6a9\ubc95 \uc774\uc5c8\uc2b5\ub2c8\ub2e4. \uacf5\uc2dd \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\ub294 \uacfc\uac70 \uc2dc\uc810\uc73c\ub85c \ub3cc\uc544\uac00\uc11c \uc774\uc5b4\uac00\ub294 \ub0b4\uc6a9\ub3c4 \uc788\uae34 \ud55c\ub370, \uc774 \uc678\uc5d0\ub3c4 \uc218\ub9ce\uc740 \ub514\ud14c\uc77c\ub4e4\uc774 \uc788\uc73c\ubbc0\ub85c, \ubcf8 \ubb38\uc11c\ub294 \uc5ec\uae30\uc11c \ub9c8\uce58\uace0, \ucd94\uac00 \uc790\ub8cc\ub4e4\ub85c \uc774\uc5b4\uc11c \uc18c\uac1c\ud558\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/1_langgraph_start/#langgraph","title":"LangGraph \uc2dc\uc791\ud558\uae30\u00b6","text":"<p>\ub7ad\uadf8\ub798\ud504\ub85c \uac04\ub2e8\ud55c LLM application (\ucc57\ubd07) \uc744 \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4</p> <ul> <li>\uc6f9\uc744 \uc11c\uce58\ud558\uace0 \ub300\ub2f5\ud558\ub294 \ucc57\ubd07</li> <li>\ub300\ud654\uc758 \ub0b4\uc6a9\uc744 \uc800\uc7a5\ud558\uace0 \uc774\uc5b4\uac00\ub294 \ucc57\ubd07</li> <li>Route complex queries to a human for review</li> <li>Use custom state to control its behavior</li> <li>Rewind and explore alternative conversation paths</li> </ul>"},{"location":"tutorial/1_langgraph_start/#setup","title":"Setup\u00b6","text":"<p>\ud544\uc694\ud55c \ud328\ud0a4\uc9c0\ub4e4\uc744 \uc124\uce58\ud569\ub2c8\ub2e4</p>"},{"location":"tutorial/1_langgraph_start/#1-gpt-wrapper","title":"1. \uc815\ub9d0 \uac04\ub2e8\ud55c \ucc57\ubd07 (GPT Wrapper) \ub9cc\ub4e4\uae30\u00b6","text":"<p>LangGraph \ub85c GPT API \ub97c \ubcf4\ub0b4\uace0 \ub2f5\ubcc0 \ubc1b\ub294 \ucc57\ubd07\uc744 \ub9cc\ub4e4\uc5b4 \ubd05\uc2dc\ub2e4.</p> <ul> <li><p><code>StateGraph</code>\ub97c \uba3c\uc800 \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4.</p> <ul> <li><code>StateGraph</code> \uc624\ube0c\uc81d\ud2b8\ub294 \uc6b0\ub9ac\uc758 \ucc57\ubd07\uc744 \"state machine\" \uc73c\ub85c \uc815\uc758\ud569\ub2c8\ub2e4.</li> </ul> </li> <li><p><code>nodes</code> \ub4e4\uc744 \ucd94\uac00\ud569\ub2c8\ub2e4. LLM \uae30\ub2a5\uc744 \uc815\uc758\ud558\uace0,  <code>edges</code> \ub97c \uc5f0\uacb0\ud558\uc5ec \uc5b4\ub5bb\uac8c \ub3d9\uc791\ud560 \uc9c0 \ucd94\uac00\uac00 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> </li> </ul>"},{"location":"tutorial/1_langgraph_start/#2","title":"2. \ucc57\ubd07\uc5d0 \ub3c4\uad6c\ub97c \ucd94\uac00\ud574\uc8fc\uae30\u00b6","text":"<p>\ud604\uc7ac \ucc57\ubd07 (LLM) \uc774 \ub2f9\uc5f0\ud788 \ubcf8\uc778 \"\uae30\uc5b5\" \ub9cc \uac00\uc9c0\uace0\ub294 \ub300\ub2f5\uc744 \ubabb \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub2c8 \uc6f9 \uc11c\uce6d \ub3c4\uad6c\ub97c \uc190\uc5d0 \uc950\uc5b4\uc8fc\uace0 \uac80\uc0c9\ud574\uc624\ub77c\uace0 \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uad00\ub828 \uc815\ubcf4\ub97c \uc54c\uc544\uc11c \uac80\uc0c9\ud574\uc11c \ub354 \uc798 \ub2f5\ubcc0\ud574 \uc8fc\uaca0\uc8e0.</p>"},{"location":"tutorial/1_langgraph_start/#requirements","title":"Requirements\u00b6","text":"<p>\ud544\uc694 \ud328\ud0a4\uc9c0\ub791 API \ud0a4\ub97c \uc14b\uc5c5\ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>Tavily Search Engine \uc640 TAVILY_API_KEY \ub97c \uc14b\uc5c5\ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>Tavily \ub294 \uc6f9 \uc11c\uce6d\uc744 api \ub85c \uc5f0\uacb0\uc2dc\ucf1c\uc8fc\ub294 \ub3c4\uad6c \uc785\ub2c8\ub2e4.</p>"},{"location":"tutorial/1_langgraph_start/#3","title":"3. \ucc57\ubd07\uc5d0 \uba54\ubaa8\ub9ac \ucd94\uac00\ud558\uae30\u00b6","text":"<p>\ucc57\ubd07\uc740 \uc6f9 \uc11c\uce6d\uc774 \uac00\ub2a5\ud558\uc9c0\ub9cc, \ub300\ud654\ub97c \uc774\uc5b4\ub098\uac00\ub824\uba74 \uba54\ubaa8\ub9ac\ub97c \uac00\uc9c0\uace0 \uc788\uc5b4\uc57c \ud558\uaca0\uc8e0. \uba54\ubaa8\ub9ac\ub97c \ucd94\uac00\ud574\uc918\uc11c \uba40\ud2f0\ud134 \ub300\ud654\uc5d0 \uc77c\uad00\uc131\uc744 \ubd80\uc5ec\ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>LangGraph \ub294 persistent checkpointing \uc774\ub77c\ub294 \uac1c\ub150\uc73c\ub85c \uc774 \uba54\ubaa8\ub9ac \ubb38\uc81c\ub97c \ud574\uacb0\ud569\ub2c8\ub2e4. <code>checkpointer</code> \ub97c \uadf8\ub798\ud504 \ucef4\ud30c\uc77c \ud560\ub54c \ub123\uc5b4\uc8fc\uace0, <code>thread_id</code> \ub97c \uadf8\ub798\ud504 \ud638\ucd9c\ud560 \ub54c \uc0ac\uc6a9\ud558\uba74, LangGraph \uac00 \uc790\ub3d9\uc73c\ub85c state \ub97c \uae30\uc5b5\ud558\uace0 \ub118\uaca8\uc90d\ub2c8\ub2e4. \uac19\uc740 <code>thread_id</code> \ub85c \uadf8\ub798\ud504\ub97c \ud638\ucd9c\ud558\uba74, \uc800\uc7a5\ub41c state \ub97c \ub85c\ub529\ud558\uc5ec \uc774\uc5b4\uac11\ub2c8\ub2e4.</p> <p>checkpointing \uc740 \ub2e8\uc21c \uba54\ubaa8\ub9ac\ubcf4\ub2e4\ub294 \ud6e8\uc52c \ub354 \ud30c\uc6cc\ud480\ud55c \ubc29\ubc95\uc785\ub2c8\ub2e4. \ubcf5\uc7a1\ud55c state\ub97c \uc800\uc7a5\ud558\uace0, \uc7ac\uac1c \ud558\ub294 \uae30\ub2a5\uc774\uae30 \ub54c\ubb38\uc5d0,</p> <ul> <li>error recovery</li> <li>human-in-the-loop workflows</li> <li>time travel interactions, and more \uc640 \uac19\uc740 \uae30\ub2a5\ub4e4\uc744 \uad6c\ud604 \uac00\ub2a5\ud558\uac8c \ud569\ub2c8\ub2e4.</li> </ul> <p>\uc6b0\uc120\uc740 \uba40\ud2f0\ud134 \ub300\ud654\ub97c \uc704\ud55c \uba54\ubaa8\ub9ac \uae30\ub2a5\uc73c\ub85c \ubd80\ud130 \uc2dc\uc791\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <p><code>MemorySaver</code> checkpointer \ub97c \ub9cc\ub4e4\uba74\uc11c \uc2dc\uc791\ud558\uc8e0.</p>"},{"location":"tutorial/1_langgraph_start/#4-human-in-the-loop","title":"4. Human-in-the-loop\u00b6","text":"<p>\ub2f9\uc5f0\ud55c \uc774\uc57c\uae30 \uc774\uc9c0\ub9cc, Agent \ub294 unreliable, \ubbff\uc744 \uc218\uac00 \uc5c6\uc5b4\uc694. \ubd80\ud558\uc9c1\uc6d0\uc774\ub77c\uace0 \uc0dd\uac01\ud558\uba74 \ub418\ub294\ub370, \uc0ac\ub78c\ub3c4 \ubd80\ud558\uc9c1\uc6d0\uc774 \uc2e4\uc218\ud560 \uac83\uc744 \ub300\ube44\ud574\uc57c\ud558\uc8e0? \uadf8\ub798\uc11c \uc5b4\ub5a4 \uc77c\uc744 \uc2dc\ud0a4\uace0, \uc911\uac04\uc5d0 \uac1c\uc785\ud574\uc11c \uccb4\ud06c\ub97c \ud574\uc57c\ud569\ub2c8\ub2e4. \uc2b9\uc778 \uacfc\uc815\uc744 \uc2dc\uc2a4\ud15c\uc5d0 \ub123\uc5b4\ub450\ub294 \uac83\uacfc \ube44\uc2b7\ud558\uc8e0.</p> <p>LangGraph \ub294 <code>human-in-the-loop</code> workflows \ub97c \uc5ec\ub7ec\uac00\uc9c0 \ubc29\ubc95\uc73c\ub85c \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \uc774\ubc88 \uc2e4\uc2b5\uc5d0\uc11c\ub294 LangGraph\uc758 <code>interrupt_before</code> \uae30\ub2a5\uc744 \uc0ac\uc6a9\ud574\uc11c \ub178\ub4dc \uc2e4\ud589 \uc804\ub9c8\ub2e4 \ubaa8\ub450 \uba48\ucdb0\uc11c \uc0ac\ub78c\uc774 \uac1c\uc785\ud558\ub3c4\ub85d \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>\ucc38\uace0\ub85c, \uc774 Human-in-the-loop \uae30\ub2a5\uc774 LangGraph \uc774\uc804 LangChain \uc5d0\uc11c\ub294 \uad6c\ud604\uc774 \uc5b4\ub824\uc6cc\uc11c \ub9ce\uc740 \uac1c\ubc1c\uc790\ub4e4\uc758 \ubd88\ub9cc\uc744 \uc0c0\uc5c8\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/1_langgraph_start/#5-state","title":"5. State \ubcc0\uacbd\ud558\uae30\u00b6","text":"<p>\uc55e\uc120 \uc2e4\uc2b5\uc5d0\uc11c\ub294 interrupt \ub97c \uac78\uace0, state\ub97c \ubd24\uc2b5\ub2c8\ub2e4. <code>read</code> \ub9cc \ud588\uc8e0. \uc774\ubc88\uc5d0\ub294 <code>write</code>\ub97c \ud574\uc11c state \ub97c \ub9c8\uc74c\ub300\ub85c \ubcc0\uacbd\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>LangGraph \ub294 manually update state \uac00 \uac00\ub2a5\ud569\ub2c8\ub2e4. agent \uac00 \uc6d0\uce58 \uc54a\ub294 \ub3d9\uc791\uc744 \ubabb\ud558\ub3c4\ub85d \ub9c9\uac70\ub098, \ubcc0\uacbd\uc2dc\ud0a4\uac70\ub098 \ub2e4\uc591\ud558\uac8c \ud65c\uc6a9\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> <p>Checkpointed state \ub97c \uc9c1\uc811 \ubcc0\uacbd \uc2dc\ucf1c\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/1_langgraph_start/#message-overwrite","title":"Message \ub97c \ub367\ubd99\uc774\ub294\uac8c \uc544\ub2c8\ub77c overwrite \ud558\uace0 \uc2f6\ub2e4\uba74?\u00b6","text":"<p><code>add_messages</code> \ud568\uc218\ub294 <code>State</code> \uc5d0 \uba54\uc138\uc9c0\ub97c <code>append</code> \ud558\uc9c0\ub9cc, ID \ub97c \ucc3e\uc544\uc11c \uc77c\uce58\ud558\ub294\uac8c \uc788\uc73c\uba74 overwrite \ub97c \ud569\ub2c8\ub2e4, \uc790\uc138\ud55c \ub3d9\uc791\uc740 \uacf5\uc2dd \ubb38\uc11c \ub9c1\ud06c\ub97c \ucc38\uc870\ud558\uc138\uc694.</p> <p>\uc774\ub97c \uc774\uc6a9\ud558\uba74, \uc6d0\ud558\ub294\ub300\ub85c \ub3d9\uc791\uc744 \ubc14\uafd4\uc904 \uc218 \uc788\uaca0\uc8e0. \ub2e4\uc2dc \uc9c8\ubb38\ubd80\ud130 \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/1_langgraph_start/#6-customizing-state","title":"6. Customizing State\u00b6","text":"<p>\uc9c0\uae08\uae4c\uc9c0\ub294 State \uac00 \ub2e8\uc21c \uba54\uc138\uc9c0\ub9cc \ub2f4\uace0 \uc788\uc5c8\uc8e0? State \uc5d0 \ub354 \ub9ce\uc740 \uc815\ubcf4\ub97c \ub2f4\uc544\uc11c \ubcf5\uc7a1\ud55c \uc77c\uc744 \uc2dc\ucf1c \ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>\uc55e\uc120 \ud30c\ud2b8\uae4c\uc9c0\ub294 \"\ud56d\uc0c1\" interrupt \uac00 \ub418\uc5b4\uc11c \uc0ac\ub78c\uc774 \uac1c\uc785\ud558\ub3c4\ub85d \ud588\uc8e0.  \uc774\ubc88\uc5d0\ub294 \uc0ac\ub78c\uc5d0\uac8c \uc694\uccad\uc744 \ud560\uc9c0, \uc54c\uc544\uc11c \ud560\uc9c0, LLM \uc774 \ud310\ub2e8\ud558\ub3c4\ub85d \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>\uc774\ub97c \uad6c\ud604\ud558\uae30 \uc704\ud55c \uccab\ubc88\uc9f8 \ubc29\ubc95\uc740 \"Human\" \ub178\ub4dc\ub97c \ub9cc\ub4e4\uc5b4\uc11c \uadf8 \ub54c \uba48\ucd94\ub3c4\ub85d\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \"Human\" tool \uc744 LLM \uc774 \ubd80\ub974\uba74 \"Human\" \ub178\ub4dc\ub85c \uadf8\ub798\ud504\uac00 \uac00\uace0, \uba48\ucd94\ub3c4\ub85d \ud558\ub294 \uac83\uc774\uc8e0.</p> <p><code>State</code> \uc5d0 <code>ask_human</code> \uba64\ubc84 \ubcc0\uc218\ub97c \ucd94\uac00\ud558\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/2_langgraph_advanced_topics/","title":"Parallel node execution","text":"In\u00a0[\u00a0]: Copied! <pre>%%capture --no-stderr\n%pip install -U  langgraph tavily-python wikipedia langchain_openai langchain_community langgraph_sdk\n</pre> %%capture --no-stderr %pip install -U  langgraph tavily-python wikipedia langchain_openai langchain_community langgraph_sdk In\u00a0[\u00a0]: Copied! <pre>import os, getpass\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n</pre> import os, getpass  def _set_env(var: str):     if not os.environ.get(var):         os.environ[var] = getpass.getpass(f\"{var}: \")  _set_env(\"OPENAI_API_KEY\") <pre>OPENAI_API_KEY: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Image, display\n\nfrom typing import Any\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    state: str\n\nclass ReturnNodeValue:\n    def __init__(self, node_secret: str):\n        self._value = node_secret\n\n    def __call__(self, state: State) -&gt; Any:\n        print(f\"Adding {self._value} to {state['state']}\")\n        return {\"state\": [self._value]}\n\n# Add nodes\nbuilder = StateGraph(State)\n\n# Initialize each node with node_secret\nbuilder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\nbuilder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\nbuilder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\nbuilder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n\n# Flow\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"b\", \"c\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from IPython.display import Image, display  from typing import Any from typing_extensions import TypedDict  from langgraph.graph import StateGraph, START, END  class State(TypedDict):     # The operator.add reducer fn makes this append-only     state: str  class ReturnNodeValue:     def __init__(self, node_secret: str):         self._value = node_secret      def __call__(self, state: State) -&gt; Any:         print(f\"Adding {self._value} to {state['state']}\")         return {\"state\": [self._value]}  # Add nodes builder = StateGraph(State)  # Initialize each node with node_secret builder.add_node(\"a\", ReturnNodeValue(\"I'm A\")) builder.add_node(\"b\", ReturnNodeValue(\"I'm B\")) builder.add_node(\"c\", ReturnNodeValue(\"I'm C\")) builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))  # Flow builder.add_edge(START, \"a\") builder.add_edge(\"a\", \"b\") builder.add_edge(\"b\", \"c\") builder.add_edge(\"c\", \"d\") builder.add_edge(\"d\", END) graph = builder.compile()  display(Image(graph.get_graph().draw_mermaid_png())) <p>State \uac00 \uacc4\uc18d \ub36e\uc5b4 \uc368\uc9c0\ub294 \uc0c1\ud669\uc785\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>graph.invoke({\"state\": []})\n</pre> graph.invoke({\"state\": []}) <pre>Adding I'm A to []\nAdding I'm B to [\"I'm A\"]\nAdding I'm C to [\"I'm B\"]\nAdding I'm D to [\"I'm C\"]\n</pre> Out[\u00a0]: <pre>{'state': [\"I'm D\"]}</pre> <p>\uc790, \uc774\uc81c <code>b</code> \uc640 <code>c</code> \ub97c \ubcd1\ub82c\ub85c \ubd99\uc774\uace0, <code>d</code> \ub97c run \ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <ul> <li>fan-out from <code>a</code> to <code>b</code> and <code>c</code></li> <li>fan-in to <code>d</code>.</li> </ul> <p>\uc5b4\ub5bb\uac8c \ub420\uae4c\uc694...??</p> In\u00a0[\u00a0]: Copied! <pre>builder = StateGraph(State)\n\n# Initialize each node with node_secret\nbuilder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\nbuilder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\nbuilder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\nbuilder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n\n# Flow\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"d\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> builder = StateGraph(State)  # Initialize each node with node_secret builder.add_node(\"a\", ReturnNodeValue(\"I'm A\")) builder.add_node(\"b\", ReturnNodeValue(\"I'm B\")) builder.add_node(\"c\", ReturnNodeValue(\"I'm C\")) builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))  # Flow builder.add_edge(START, \"a\") builder.add_edge(\"a\", \"b\") builder.add_edge(\"a\", \"c\") builder.add_edge(\"b\", \"d\") builder.add_edge(\"c\", \"d\") builder.add_edge(\"d\", END) graph = builder.compile()  display(Image(graph.get_graph().draw_mermaid_png())) <p>\ub2f9\uc5f0\ud788(?) \uc5d0\ub7ec\uac00 \ub0a9\ub2c8\ub2e4!</p> In\u00a0[\u00a0]: Copied! <pre>from langgraph.errors import InvalidUpdateError\ntry:\n    graph.invoke({\"state\": []})\nexcept InvalidUpdateError as e:\n    print(f\"An error occurred: {e}\")\n</pre> from langgraph.errors import InvalidUpdateError try:     graph.invoke({\"state\": []}) except InvalidUpdateError as e:     print(f\"An error occurred: {e}\") <pre>Adding I'm A to []\nAdding I'm C to [\"I'm A\"]\nAdding I'm B to [\"I'm A\"]\nAn error occurred: At key 'state': Can receive only one value per step. Use an Annotated key to handle multiple values.\n</pre> <p>fan out \uc744 \ud558\uba74, reducer \uc5d0\uc11c \uac19\uc740 channel / key \ub97c write \ud558\ub294\uc9c0 \uaf2d \uccb4\ud06c\ud574\uc57c\ud569\ub2c8\ub2e4.</p> <p>\uc774 \uc0c1\ud669\uc5d0\uc11c\ub294 operator.add \ub97c \uc0ac\uc6a9\ud574\uc11c, State \ub97c \ub36e\uc5b4 \uc4f0\ub294 \uac83\uc774 \uc544\ub2c8\ub77c, append \ud558\ub294 \uac83\uc774 \uac00\uc7a5 \uc26c\uc6b4 \ud574\uacb0 \ubc29\ubc95 \uc774\uaca0\uc8e0.</p> In\u00a0[\u00a0]: Copied! <pre>import operator\nfrom typing import Annotated\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    state: Annotated[list, operator.add]\n\n# Add nodes\nbuilder = StateGraph(State)\n\n# Initialize each node with node_secret\nbuilder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\nbuilder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\nbuilder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\nbuilder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n\n# Flow\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"d\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> import operator from typing import Annotated  class State(TypedDict):     # The operator.add reducer fn makes this append-only     state: Annotated[list, operator.add]  # Add nodes builder = StateGraph(State)  # Initialize each node with node_secret builder.add_node(\"a\", ReturnNodeValue(\"I'm A\")) builder.add_node(\"b\", ReturnNodeValue(\"I'm B\")) builder.add_node(\"c\", ReturnNodeValue(\"I'm C\")) builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))  # Flow builder.add_edge(START, \"a\") builder.add_edge(\"a\", \"b\") builder.add_edge(\"a\", \"c\") builder.add_edge(\"b\", \"d\") builder.add_edge(\"c\", \"d\") builder.add_edge(\"d\", END) graph = builder.compile()  display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[\u00a0]: Copied! <pre>graph.invoke({\"state\": []})\n</pre> graph.invoke({\"state\": []}) <pre>Adding I'm A to []\nAdding I'm B to [\"I'm A\"]Adding I'm C to [\"I'm A\"]\n\nAdding I'm D to [\"I'm A\", \"I'm B\", \"I'm C\"]\n</pre> Out[\u00a0]: <pre>{'state': [\"I'm A\", \"I'm B\", \"I'm C\", \"I'm D\"]}</pre> In\u00a0[\u00a0]: Copied! <pre>builder = StateGraph(State)\n\n# Initialize each node with node_secret\nbuilder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\nbuilder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\nbuilder.add_node(\"b2\", ReturnNodeValue(\"I'm B2\"))\nbuilder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\nbuilder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n\n# Flow\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"b2\")\nbuilder.add_edge([\"b2\", \"c\"], \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> builder = StateGraph(State)  # Initialize each node with node_secret builder.add_node(\"a\", ReturnNodeValue(\"I'm A\")) builder.add_node(\"b\", ReturnNodeValue(\"I'm B\")) builder.add_node(\"b2\", ReturnNodeValue(\"I'm B2\")) builder.add_node(\"c\", ReturnNodeValue(\"I'm C\")) builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))  # Flow builder.add_edge(START, \"a\") builder.add_edge(\"a\", \"b\") builder.add_edge(\"a\", \"c\") builder.add_edge(\"b\", \"b2\") builder.add_edge([\"b2\", \"c\"], \"d\") builder.add_edge(\"d\", END) graph = builder.compile()  display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[\u00a0]: Copied! <pre>graph.invoke({\"state\": []})\n</pre> graph.invoke({\"state\": []}) <pre>Adding I'm A to []\nAdding I'm B to [\"I'm A\"]\nAdding I'm C to [\"I'm A\"]\nAdding I'm B2 to [\"I'm A\", \"I'm B\", \"I'm C\"]\nAdding I'm D to [\"I'm A\", \"I'm B\", \"I'm C\", \"I'm B2\"]\n</pre> Out[\u00a0]: <pre>{'state': [\"I'm A\", \"I'm B\", \"I'm C\", \"I'm B2\", \"I'm D\"]}</pre> In\u00a0[\u00a0]: Copied! <pre>def sorting_reducer(left, right):\n    \"\"\" Combines and sorts the values in a list\"\"\"\n    if not isinstance(left, list):\n        left = [left]\n\n    if not isinstance(right, list):\n        right = [right]\n\n    return sorted(left + right, reverse=False)\n\nclass State(TypedDict):\n    # sorting_reducer will sort the values in state\n    state: Annotated[list, sorting_reducer]\n\n# Add nodes\nbuilder = StateGraph(State)\n\n# Initialize each node with node_secret\nbuilder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\nbuilder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\nbuilder.add_node(\"b2\", ReturnNodeValue(\"I'm B2\"))\nbuilder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\nbuilder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n\n# Flow\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"b2\")\nbuilder.add_edge([\"b2\", \"c\"], \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> def sorting_reducer(left, right):     \"\"\" Combines and sorts the values in a list\"\"\"     if not isinstance(left, list):         left = [left]      if not isinstance(right, list):         right = [right]      return sorted(left + right, reverse=False)  class State(TypedDict):     # sorting_reducer will sort the values in state     state: Annotated[list, sorting_reducer]  # Add nodes builder = StateGraph(State)  # Initialize each node with node_secret builder.add_node(\"a\", ReturnNodeValue(\"I'm A\")) builder.add_node(\"b\", ReturnNodeValue(\"I'm B\")) builder.add_node(\"b2\", ReturnNodeValue(\"I'm B2\")) builder.add_node(\"c\", ReturnNodeValue(\"I'm C\")) builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))  # Flow builder.add_edge(START, \"a\") builder.add_edge(\"a\", \"b\") builder.add_edge(\"a\", \"c\") builder.add_edge(\"b\", \"b2\") builder.add_edge([\"b2\", \"c\"], \"d\") builder.add_edge(\"d\", END) graph = builder.compile()  display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[\u00a0]: Copied! <pre>graph.invoke({\"state\": []})\n</pre> graph.invoke({\"state\": []}) <pre>Adding I'm A to []\nAdding I'm B to [\"I'm A\"]\nAdding I'm C to [\"I'm A\"]\nAdding I'm B2 to [\"I'm A\", \"I'm B\", \"I'm C\"]\nAdding I'm D to [\"I'm A\", \"I'm B\", \"I'm B2\", \"I'm C\"]\n</pre> Out[\u00a0]: <pre>{'state': [\"I'm A\", \"I'm B\", \"I'm B2\", \"I'm C\", \"I'm D\"]}</pre> <p>\uc774\ub807\uac8c \ud558\uba74\uc694... B2 \uac00 \ub098\uc911\uc5d0 \ucd94\uac00\uac00 \ub418\uc5c8\uc9c0\ub9cc, \uadf8\ub798\ub3c4 C \ubcf4\ub2e4 \uc55e\uc73c\ub85c \ub4e4\uc5b4\uac14\uc2b5\ub2c8\ub2e4. \uc774\uac8c \uc88b\uc740 \ubc29\ubc95\uc774\ub0d0\uc5d0 \ub300\ud574\uc11c\ub294 \uc774\uacac\uc774 \uc788\uc744 \uc218 \uc788\uaca0\uc9c0\ub9cc, LangGraph \uc5d0\uc11c \uacf5\uc2dd\uc801\uc73c\ub85c \uc548\ub0b4\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>from langchain_openai import ChatOpenAI\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n</pre> from langchain_openai import ChatOpenAI llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) In\u00a0[\u00a0]: Copied! <pre>class State(TypedDict):\n    question: str\n    answer: str\n    context: Annotated[list, operator.add]\n</pre> class State(TypedDict):     question: str     answer: str     context: Annotated[list, operator.add] <p>Tavily \ub97c \uc774\uc6a9\ud574\uc11c \uc6f9 \uac80\uc0c9\uc744 \ud560 \uac83\uc774\ub2c8 <code>TAVILY_API_KEY</code> \ub97c \uc14b\uc5c5\ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre>import os, getpass\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n_set_env(\"TAVILY_API_KEY\")\n</pre> import os, getpass def _set_env(var: str):     if not os.environ.get(var):         os.environ[var] = getpass.getpass(f\"{var}: \") _set_env(\"TAVILY_API_KEY\") <pre>TAVILY_API_KEY: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[\u00a0]: Copied! <pre>from langchain_core.messages import HumanMessage, SystemMessage\n\nfrom langchain_community.document_loaders import WikipediaLoader\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\ndef search_web(state):\n\n    \"\"\" Retrieve docs from web search \"\"\"\n\n    # Search\n    tavily_search = TavilySearchResults(max_results=3)\n    search_docs = tavily_search.invoke(state['question'])\n\n     # Format\n    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n        [\n            f'&lt;Document href=\"{doc[\"url\"]}\"/&gt;\\n{doc[\"content\"]}\\n&lt;/Document&gt;'\n            for doc in search_docs\n        ]\n    )\n\n    return {\"context\": [formatted_search_docs]}\n\ndef search_wikipedia(state):\n\n    \"\"\" Retrieve docs from wikipedia \"\"\"\n\n    # Search\n    search_docs = WikipediaLoader(query=state['question'],\n                                  load_max_docs=2).load()\n\n     # Format\n    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n        [\n            f'&lt;Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/&gt;\\n{doc.page_content}\\n&lt;/Document&gt;'\n            for doc in search_docs\n        ]\n    )\n\n    return {\"context\": [formatted_search_docs]}\n\ndef generate_answer(state):\n\n    \"\"\" Node to answer a question \"\"\"\n\n    # Get state\n    context = state[\"context\"]\n    question = state[\"question\"]\n\n    # Template\n    answer_template = \"\"\"Answer the question {question} using this context: {context}\"\"\"\n    answer_instructions = answer_template.format(question=question,\n                                                       context=context)\n\n    # Answer\n    answer = llm.invoke([SystemMessage(content=answer_instructions)]+[HumanMessage(content=f\"Answer the question.\")])\n\n    # Append it to state\n    return {\"answer\": answer}\n\n# Add nodes\nbuilder = StateGraph(State)\n\n# Initialize each node with node_secret\nbuilder.add_node(\"search_web\",search_web)\nbuilder.add_node(\"search_wikipedia\", search_wikipedia)\nbuilder.add_node(\"generate_answer\", generate_answer)\n\n# Flow\nbuilder.add_edge(START, \"search_wikipedia\")\nbuilder.add_edge(START, \"search_web\")\nbuilder.add_edge(\"search_wikipedia\", \"generate_answer\")\nbuilder.add_edge(\"search_web\", \"generate_answer\")\nbuilder.add_edge(\"generate_answer\", END)\ngraph = builder.compile()\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from langchain_core.messages import HumanMessage, SystemMessage  from langchain_community.document_loaders import WikipediaLoader from langchain_community.tools.tavily_search import TavilySearchResults  def search_web(state):      \"\"\" Retrieve docs from web search \"\"\"      # Search     tavily_search = TavilySearchResults(max_results=3)     search_docs = tavily_search.invoke(state['question'])       # Format     formatted_search_docs = \"\\n\\n---\\n\\n\".join(         [             f'\\n{doc[\"content\"]}\\n'             for doc in search_docs         ]     )      return {\"context\": [formatted_search_docs]}  def search_wikipedia(state):      \"\"\" Retrieve docs from wikipedia \"\"\"      # Search     search_docs = WikipediaLoader(query=state['question'],                                   load_max_docs=2).load()       # Format     formatted_search_docs = \"\\n\\n---\\n\\n\".join(         [             f'\\n{doc.page_content}\\n'             for doc in search_docs         ]     )      return {\"context\": [formatted_search_docs]}  def generate_answer(state):      \"\"\" Node to answer a question \"\"\"      # Get state     context = state[\"context\"]     question = state[\"question\"]      # Template     answer_template = \"\"\"Answer the question {question} using this context: {context}\"\"\"     answer_instructions = answer_template.format(question=question,                                                        context=context)      # Answer     answer = llm.invoke([SystemMessage(content=answer_instructions)]+[HumanMessage(content=f\"Answer the question.\")])      # Append it to state     return {\"answer\": answer}  # Add nodes builder = StateGraph(State)  # Initialize each node with node_secret builder.add_node(\"search_web\",search_web) builder.add_node(\"search_wikipedia\", search_wikipedia) builder.add_node(\"generate_answer\", generate_answer)  # Flow builder.add_edge(START, \"search_wikipedia\") builder.add_edge(START, \"search_web\") builder.add_edge(\"search_wikipedia\", \"generate_answer\") builder.add_edge(\"search_web\", \"generate_answer\") builder.add_edge(\"generate_answer\", END) graph = builder.compile()  display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[\u00a0]: Copied! <pre>result = graph.invoke({\"question\": \"SKT\uc758 Q2 2024 \uc7ac\ubb34\uc0c1\ud0dc \uc694\uc57d\ud574\uc918\"})\nresult['answer'].content\n</pre> result = graph.invoke({\"question\": \"SKT\uc758 Q2 2024 \uc7ac\ubb34\uc0c1\ud0dc \uc694\uc57d\ud574\uc918\"}) result['answer'].content Out[\u00a0]: <pre>\"SK Telecom's Q2 2024 financial summary is as follows:\\n\\n- Revenue: KRW 4.4224 trillion (up 2.7% compared to the same period last year)\\n- Operating Income: KRW 537.5 billion (up 16% year-over-year)\\n- Net Income: KRW 350.2 billion\\n\\nThese results indicate a positive growth trend for SKT in the second quarter of 2024.\"</pre> In\u00a0[\u00a0]: Copied! <pre>from operator import add\nfrom typing_extensions import TypedDict\nfrom typing import List, Optional, Annotated\n\n# The structure of the logs\nclass Log(TypedDict):\n    id: str\n    question: str\n    docs: Optional[List]\n    answer: str\n    grade: Optional[int]\n    grader: Optional[str]\n    feedback: Optional[str]\n</pre> from operator import add from typing_extensions import TypedDict from typing import List, Optional, Annotated  # The structure of the logs class Log(TypedDict):     id: str     question: str     docs: Optional[List]     answer: str     grade: Optional[int]     grader: Optional[str]     feedback: Optional[str] In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Image, display\nfrom langgraph.graph import StateGraph, START, END\n\n# Failure Analysis Sub-graph\nclass FailureAnalysisState(TypedDict):\n    cleaned_logs: List[Log]\n    failures: List[Log]\n    fa_summary: str\n    processed_logs: List[str]\n\nclass FailureAnalysisOutputState(TypedDict):\n    fa_summary: str\n    processed_logs: List[str]\n\ndef get_failures(state):\n    \"\"\" Get logs that contain a failure \"\"\"\n    cleaned_logs = state[\"cleaned_logs\"]\n    failures = [log for log in cleaned_logs if \"grade\" in log]\n    return {\"failures\": failures}\n\ndef generate_summary(state):\n    \"\"\" Generate summary of failures \"\"\"\n    failures = state[\"failures\"]\n    # Add fxn: fa_summary = summarize(failures)\n    fa_summary = \"Poor quality retrieval of Chroma documentation.\"\n    return {\"fa_summary\": fa_summary, \"processed_logs\": [f\"failure-analysis-on-log-{failure['id']}\" for failure in failures]}\n\nfa_builder = StateGraph(input=FailureAnalysisState,output=FailureAnalysisOutputState)\nfa_builder.add_node(\"get_failures\", get_failures)\nfa_builder.add_node(\"generate_summary\", generate_summary)\nfa_builder.add_edge(START, \"get_failures\")\nfa_builder.add_edge(\"get_failures\", \"generate_summary\")\nfa_builder.add_edge(\"generate_summary\", END)\n\ngraph = fa_builder.compile()\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from IPython.display import Image, display from langgraph.graph import StateGraph, START, END  # Failure Analysis Sub-graph class FailureAnalysisState(TypedDict):     cleaned_logs: List[Log]     failures: List[Log]     fa_summary: str     processed_logs: List[str]  class FailureAnalysisOutputState(TypedDict):     fa_summary: str     processed_logs: List[str]  def get_failures(state):     \"\"\" Get logs that contain a failure \"\"\"     cleaned_logs = state[\"cleaned_logs\"]     failures = [log for log in cleaned_logs if \"grade\" in log]     return {\"failures\": failures}  def generate_summary(state):     \"\"\" Generate summary of failures \"\"\"     failures = state[\"failures\"]     # Add fxn: fa_summary = summarize(failures)     fa_summary = \"Poor quality retrieval of Chroma documentation.\"     return {\"fa_summary\": fa_summary, \"processed_logs\": [f\"failure-analysis-on-log-{failure['id']}\" for failure in failures]}  fa_builder = StateGraph(input=FailureAnalysisState,output=FailureAnalysisOutputState) fa_builder.add_node(\"get_failures\", get_failures) fa_builder.add_node(\"generate_summary\", generate_summary) fa_builder.add_edge(START, \"get_failures\") fa_builder.add_edge(\"get_failures\", \"generate_summary\") fa_builder.add_edge(\"generate_summary\", END)  graph = fa_builder.compile() display(Image(graph.get_graph().draw_mermaid_png())) <pre>&lt;ipython-input-20-47ec520dc6f8&gt;:28: LangGraphDeprecationWarning: Initializing StateGraph without state_schema is deprecated. Please pass in an explicit state_schema instead of just an input and output schema.\n  fa_builder = StateGraph(input=FailureAnalysisState,output=FailureAnalysisOutputState)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Summarization subgraph\nclass QuestionSummarizationState(TypedDict):\n    cleaned_logs: List[Log]\n    qs_summary: str\n    report: str\n    processed_logs: List[str]\n\nclass QuestionSummarizationOutputState(TypedDict):\n    report: str\n    processed_logs: List[str]\n\ndef generate_summary(state):\n    cleaned_logs = state[\"cleaned_logs\"]\n    # Add fxn: summary = summarize(generate_summary)\n    summary = \"Questions focused on usage of ChatOllama and Chroma vector store.\"\n    return {\"qs_summary\": summary, \"processed_logs\": [f\"summary-on-log-{log['id']}\" for log in cleaned_logs]}\n\ndef send_to_slack(state):\n    qs_summary = state[\"qs_summary\"]\n    # Add fxn: report = report_generation(qs_summary)\n    report = \"foo bar baz\"\n    return {\"report\": report}\n\nqs_builder = StateGraph(input=QuestionSummarizationState,output=QuestionSummarizationOutputState)\nqs_builder.add_node(\"generate_summary\", generate_summary)\nqs_builder.add_node(\"send_to_slack\", send_to_slack)\nqs_builder.add_edge(START, \"generate_summary\")\nqs_builder.add_edge(\"generate_summary\", \"send_to_slack\")\nqs_builder.add_edge(\"send_to_slack\", END)\n\ngraph = qs_builder.compile()\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> # Summarization subgraph class QuestionSummarizationState(TypedDict):     cleaned_logs: List[Log]     qs_summary: str     report: str     processed_logs: List[str]  class QuestionSummarizationOutputState(TypedDict):     report: str     processed_logs: List[str]  def generate_summary(state):     cleaned_logs = state[\"cleaned_logs\"]     # Add fxn: summary = summarize(generate_summary)     summary = \"Questions focused on usage of ChatOllama and Chroma vector store.\"     return {\"qs_summary\": summary, \"processed_logs\": [f\"summary-on-log-{log['id']}\" for log in cleaned_logs]}  def send_to_slack(state):     qs_summary = state[\"qs_summary\"]     # Add fxn: report = report_generation(qs_summary)     report = \"foo bar baz\"     return {\"report\": report}  qs_builder = StateGraph(input=QuestionSummarizationState,output=QuestionSummarizationOutputState) qs_builder.add_node(\"generate_summary\", generate_summary) qs_builder.add_node(\"send_to_slack\", send_to_slack) qs_builder.add_edge(START, \"generate_summary\") qs_builder.add_edge(\"generate_summary\", \"send_to_slack\") qs_builder.add_edge(\"send_to_slack\", END)  graph = qs_builder.compile() display(Image(graph.get_graph().draw_mermaid_png())) <pre>&lt;ipython-input-21-720cec5361d1&gt;:24: LangGraphDeprecationWarning: Initializing StateGraph without state_schema is deprecated. Please pass in an explicit state_schema instead of just an input and output schema.\n  qs_builder = StateGraph(input=QuestionSummarizationState,output=QuestionSummarizationOutputState)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Entry Graph\nclass EntryGraphState(TypedDict):\n    raw_logs: List[Log]\n    cleaned_logs: Annotated[List[Log], add] # This will be USED BY in BOTH sub-graphs\n    fa_summary: str # This will only be generated in the FA sub-graph\n    report: str # This will only be generated in the QS sub-graph\n    processed_logs:  Annotated[List[int], add] # This will be generated in BOTH sub-graphs\n</pre> # Entry Graph class EntryGraphState(TypedDict):     raw_logs: List[Log]     cleaned_logs: Annotated[List[Log], add] # This will be USED BY in BOTH sub-graphs     fa_summary: str # This will only be generated in the FA sub-graph     report: str # This will only be generated in the QS sub-graph     processed_logs:  Annotated[List[int], add] # This will be generated in BOTH sub-graphs <p><code>cleaned_logs</code> \ub294 \uc65c reducer (\uc5ec\uae30\uc11c\ub294 add, \uc0c1\ud0dc\ub97c \ubc14\uafc0 \uc218 \uc788\uc73c\ub2c8\uae4c) \uac00 \uc788\uc744\uae4c\uc694.  subgraph \ub85c \uc0c1\ud0dc\uac00 \uc804\ub2ec\ub418\uae30\ub9cc \ud558\uace0 \ubc14\ub00c\uc9c0\ub294 \uc54a\uc744 \ud150\ub370\uc694.</p> <pre><code>cleaned_logs: Annotated[List[Log], add] # This will be USED BY in BOTH sub-graphs\n</code></pre> <p>subgraph\uc758 output\uc740 \uac12\uc744 \ubc14\uafb8\uc9c0 \uc54a\uc558\ub354\ub77c\ub3c4 \ubaa8\ub4e0 keys \ub97c \uac00\uc9c0\uace0 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.</p> <p>subgraph\ub4e4\uc740 \ubcd1\ub82c\uc801\uc73c\ub85c \ub3cc\uc8e0. \uadf8\ub9ac\uace0, \uac19\uc740 key \ub97c \ubc18\ud658\ud558\uae30 \ub54c\ubb38\uc5d0, \ub2e4\uc2dc \ud569\uce60 \ub54c\ub97c \uc704\ud574\uc11c add \uac00 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4.</p> <p>\uc0ac\uc2e4 \ub2e4\ub978 \ubc29\ubc95\uc73c\ub85c\ub3c4 \uad6c\ud604\uc774 \uac00\ub2a5\ud558\uc9c0\ub9cc, \uc0dd\ub7b5\ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> In\u00a0[\u00a0]: Copied! <pre># Entry Graph\nclass EntryGraphState(TypedDict):\n    raw_logs: List[Log]\n    cleaned_logs: List[Log]\n    fa_summary: str # This will only be generated in the FA sub-graph\n    report: str # This will only be generated in the QS sub-graph\n    processed_logs:  Annotated[List[int], add] # This will be generated in BOTH sub-graphs\n\ndef clean_logs(state):\n    # Get logs\n    raw_logs = state[\"raw_logs\"]\n    # Data cleaning raw_logs -&gt; docs\n    cleaned_logs = raw_logs\n    return {\"cleaned_logs\": cleaned_logs}\n\nentry_builder = StateGraph(EntryGraphState)\nentry_builder.add_node(\"clean_logs\", clean_logs)\nentry_builder.add_node(\"question_summarization\", qs_builder.compile())\nentry_builder.add_node(\"failure_analysis\", fa_builder.compile())\n\nentry_builder.add_edge(START, \"clean_logs\")\nentry_builder.add_edge(\"clean_logs\", \"failure_analysis\")\nentry_builder.add_edge(\"clean_logs\", \"question_summarization\")\nentry_builder.add_edge(\"failure_analysis\", END)\nentry_builder.add_edge(\"question_summarization\", END)\n\ngraph = entry_builder.compile()\n\nfrom IPython.display import Image, display\n\n# Setting xray to 1 will show the internal structure of the nested graph\ndisplay(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n</pre> # Entry Graph class EntryGraphState(TypedDict):     raw_logs: List[Log]     cleaned_logs: List[Log]     fa_summary: str # This will only be generated in the FA sub-graph     report: str # This will only be generated in the QS sub-graph     processed_logs:  Annotated[List[int], add] # This will be generated in BOTH sub-graphs  def clean_logs(state):     # Get logs     raw_logs = state[\"raw_logs\"]     # Data cleaning raw_logs -&gt; docs     cleaned_logs = raw_logs     return {\"cleaned_logs\": cleaned_logs}  entry_builder = StateGraph(EntryGraphState) entry_builder.add_node(\"clean_logs\", clean_logs) entry_builder.add_node(\"question_summarization\", qs_builder.compile()) entry_builder.add_node(\"failure_analysis\", fa_builder.compile())  entry_builder.add_edge(START, \"clean_logs\") entry_builder.add_edge(\"clean_logs\", \"failure_analysis\") entry_builder.add_edge(\"clean_logs\", \"question_summarization\") entry_builder.add_edge(\"failure_analysis\", END) entry_builder.add_edge(\"question_summarization\", END)  graph = entry_builder.compile()  from IPython.display import Image, display  # Setting xray to 1 will show the internal structure of the nested graph display(Image(graph.get_graph(xray=1).draw_mermaid_png())) In\u00a0[\u00a0]: Copied! <pre># Dummy logs\nquestion_answer = Log(\n    id=\"1\",\n    question=\"How can I import ChatOllama?\",\n    answer=\"To import ChatOllama, use: 'from langchain_community.chat_models import ChatOllama.'\",\n)\n\nquestion_answer_feedback = Log(\n    id=\"2\",\n    question=\"How can I use Chroma vector store?\",\n    answer=\"To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).\",\n    grade=0,\n    grader=\"Document Relevance Recall\",\n    feedback=\"The retrieved documents discuss vector stores in general, but not Chroma specifically\",\n)\n\nraw_logs = [question_answer,question_answer_feedback]\ngraph.invoke({\"raw_logs\": raw_logs})\n</pre> # Dummy logs question_answer = Log(     id=\"1\",     question=\"How can I import ChatOllama?\",     answer=\"To import ChatOllama, use: 'from langchain_community.chat_models import ChatOllama.'\", )  question_answer_feedback = Log(     id=\"2\",     question=\"How can I use Chroma vector store?\",     answer=\"To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).\",     grade=0,     grader=\"Document Relevance Recall\",     feedback=\"The retrieved documents discuss vector stores in general, but not Chroma specifically\", )  raw_logs = [question_answer,question_answer_feedback] graph.invoke({\"raw_logs\": raw_logs}) Out[\u00a0]: <pre>{'raw_logs': [{'id': '1',\n   'question': 'How can I import ChatOllama?',\n   'answer': \"To import ChatOllama, use: 'from langchain_community.chat_models import ChatOllama.'\"},\n  {'id': '2',\n   'question': 'How can I use Chroma vector store?',\n   'answer': 'To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).',\n   'grade': 0,\n   'grader': 'Document Relevance Recall',\n   'feedback': 'The retrieved documents discuss vector stores in general, but not Chroma specifically'}],\n 'cleaned_logs': [{'id': '1',\n   'question': 'How can I import ChatOllama?',\n   'answer': \"To import ChatOllama, use: 'from langchain_community.chat_models import ChatOllama.'\"},\n  {'id': '2',\n   'question': 'How can I use Chroma vector store?',\n   'answer': 'To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).',\n   'grade': 0,\n   'grader': 'Document Relevance Recall',\n   'feedback': 'The retrieved documents discuss vector stores in general, but not Chroma specifically'}],\n 'fa_summary': 'Poor quality retrieval of Chroma documentation.',\n 'report': 'foo bar baz',\n 'processed_logs': ['summary-on-log-1',\n  'summary-on-log-2',\n  'failure-analysis-on-log-2']}</pre> <p>\uc704\uc640 \ub9c8\ucc2c\uac00\uc9c0\ub85c, LangGraph Studio \ub85c \uc218\ud589\uc744 \ud588\uc2b5\ub2c8\ub2e4. LangSmith\uc5d0 \ucc0d\ud78c \uacb0\uacfc\ub294 \ub9c1\ud06c \uc5d0\uc11c \ud655\uc778 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>from langchain_openai import ChatOpenAI\n\n# Prompts we will use\nsubjects_prompt = \"\"\"Generate a list of 3 sub-topics that are all related to this overall topic: {topic}.\"\"\"\njoke_prompt = \"\"\"Generate a joke about {subject}\"\"\"\nbest_joke_prompt = \"\"\"Below are a bunch of jokes about {topic}. Select the best one! Return the ID of the best one, starting 0 as the ID for the first joke. Jokes: \\n\\n  {jokes}\"\"\"\n\n# LLM\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n</pre> from langchain_openai import ChatOpenAI  # Prompts we will use subjects_prompt = \"\"\"Generate a list of 3 sub-topics that are all related to this overall topic: {topic}.\"\"\" joke_prompt = \"\"\"Generate a joke about {subject}\"\"\" best_joke_prompt = \"\"\"Below are a bunch of jokes about {topic}. Select the best one! Return the ID of the best one, starting 0 as the ID for the first joke. Jokes: \\n\\n  {jokes}\"\"\"  # LLM model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) In\u00a0[\u00a0]: Copied! <pre>import operator\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom pydantic import BaseModel\n\nclass Subjects(BaseModel):\n    subjects: list[str]\n\nclass BestJoke(BaseModel):\n    id: int\n\nclass OverallState(TypedDict):\n    topic: str\n    subjects: list\n    jokes: Annotated[list, operator.add]\n    best_selected_joke: str\n</pre> import operator from typing import Annotated from typing_extensions import TypedDict from pydantic import BaseModel  class Subjects(BaseModel):     subjects: list[str]  class BestJoke(BaseModel):     id: int  class OverallState(TypedDict):     topic: str     subjects: list     jokes: Annotated[list, operator.add]     best_selected_joke: str In\u00a0[\u00a0]: Copied! <pre>def generate_topics(state: OverallState):\n    prompt = subjects_prompt.format(topic=state[\"topic\"])\n    response = model.with_structured_output(Subjects).invoke(prompt)\n    return {\"subjects\": response.subjects}\n</pre> def generate_topics(state: OverallState):     prompt = subjects_prompt.format(topic=state[\"topic\"])     response = model.with_structured_output(Subjects).invoke(prompt)     return {\"subjects\": response.subjects} <p>Send \ub97c \uc0ac\uc6a9\ud574\uc11c \uac01\uac01 joke \uc0dd\uc131 \ub178\ub4dc\uc5d0 \ud1a0\ud53d\uc744 \ubcf4\ub0b4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>\ub178\ub4dc\ub294 task \uc885\ub958\uc774\uace0, \uac01 \ud1a0\ud53d\uc5d0 \ub300\ud574 \ubc18\ubcf5\uc791\uc5c5\uc744 \ud569\ub2c8\ub2e4. \uc774\ub7f4\ub54c \uc544\uc8fc \uc720\uc6a9\ud569\ub2c8\ub2e4. \ub9d0\ud558\uc790\uba74 for \ubb38\uc774\uc8e0.</p> <ul> <li><code>generate_joke</code>: \ub178\ub4dc \uc774\ub984</li> <li><code>{\"subject\": s</code>}: \ubcf4\ub0bc state</li> </ul> <p><code>Send</code> \ub294 \uc544\ubb34 state \ub098 <code>generate_joke</code> \ub178\ub4dc\uc5d0 \ubcf4\ub0bc \uc218 \uc788\uc2b5\ub2c8\ub2e4. <code>OverallState</code> \uc640 align\uc774 \ub420 \ud544\uc694\uac00 \uc5c6\uc2b5\ub2c8\ub2e4.</p> <p><code>generate_joke</code> \ub178\ub4dc\ub294 \uc9c0\ub9cc\uc758 state \uac00 \uc788\uc8e0.</p> In\u00a0[\u00a0]: Copied! <pre>from langgraph.constants import Send\ndef continue_to_jokes(state: OverallState):\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\n</pre> from langgraph.constants import Send def continue_to_jokes(state: OverallState):     return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]] In\u00a0[\u00a0]: Copied! <pre>class JokeState(TypedDict):\n    subject: str\n\nclass Joke(BaseModel):\n    joke: str\n\ndef generate_joke(state: JokeState):\n    prompt = joke_prompt.format(subject=state[\"subject\"])\n    response = model.with_structured_output(Joke).invoke(prompt)\n    return {\"jokes\": [response.joke]}\n</pre> class JokeState(TypedDict):     subject: str  class Joke(BaseModel):     joke: str  def generate_joke(state: JokeState):     prompt = joke_prompt.format(subject=state[\"subject\"])     response = model.with_structured_output(Joke).invoke(prompt)     return {\"jokes\": [response.joke]} In\u00a0[\u00a0]: Copied! <pre>def best_joke(state: OverallState):\n    jokes = \"\\n\\n\".join(state[\"jokes\"])\n    prompt = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)\n    response = model.with_structured_output(BestJoke).invoke(prompt)\n    return {\"best_selected_joke\": state[\"jokes\"][response.id]}\n</pre> def best_joke(state: OverallState):     jokes = \"\\n\\n\".join(state[\"jokes\"])     prompt = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)     response = model.with_structured_output(BestJoke).invoke(prompt)     return {\"best_selected_joke\": state[\"jokes\"][response.id]} In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Image\nfrom langgraph.graph import END, StateGraph, START\n\n# Construct the graph: here we put everything together to construct our graph\ngraph = StateGraph(OverallState)\ngraph.add_node(\"generate_topics\", generate_topics)\ngraph.add_node(\"generate_joke\", generate_joke)\ngraph.add_node(\"best_joke\", best_joke)\ngraph.add_edge(START, \"generate_topics\")\ngraph.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\ngraph.add_edge(\"generate_joke\", \"best_joke\")\ngraph.add_edge(\"best_joke\", END)\n\n# Compile the graph\napp = graph.compile()\nImage(app.get_graph().draw_mermaid_png())\n</pre> from IPython.display import Image from langgraph.graph import END, StateGraph, START  # Construct the graph: here we put everything together to construct our graph graph = StateGraph(OverallState) graph.add_node(\"generate_topics\", generate_topics) graph.add_node(\"generate_joke\", generate_joke) graph.add_node(\"best_joke\", best_joke) graph.add_edge(START, \"generate_topics\") graph.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"]) graph.add_edge(\"generate_joke\", \"best_joke\") graph.add_edge(\"best_joke\", END)  # Compile the graph app = graph.compile() Image(app.get_graph().draw_mermaid_png()) Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre># Call the graph: here we call it to generate a list of jokes\nfor s in app.stream({\"topic\": \"animals\"}):\n    print(s)\n</pre> # Call the graph: here we call it to generate a list of jokes for s in app.stream({\"topic\": \"animals\"}):     print(s) <pre>{'generate_topics': {'subjects': ['Animal Behavior', 'Endangered Species', 'Animal Habitats']}}\n{'generate_joke': {'jokes': [\"Why did the fish blush? Because it saw the ocean's bottom!\"]}}\n{'generate_joke': {'jokes': [\"Why did the dog sit in the shade? Because he didn't want to become a hot dog!\"]}}\n{'generate_joke': {'jokes': ['Why did the endangered species break up with their partner? Because they needed more space to thrive!']}}\n{'best_joke': {'best_selected_joke': \"Why did the dog sit in the shade? Because he didn't want to become a hot dog!\"}}\n</pre>"},{"location":"tutorial/2_langgraph_advanced_topics/#parallel-node-execution","title":"Parallel node execution\u00b6","text":""},{"location":"tutorial/2_langgraph_advanced_topics/#review","title":"Review\u00b6","text":"<p>\uc55e\uc11c\uc11c \ubc30\uc6b4 <code>human-in-the loop</code> \uc740 \ub2e4\uc74c\uacfc \uac19\uc740 3\uac00\uc9c0 \uc0c1\ud669\uc5d0 \uc720\uc6a9\ud558\uac8c \uc0ac\uc6a9\ub429\ub2c8\ub2e4.</p> <p>(1) <code>Approval</code> - Agent\uc758 \uc911\uac04\ub2e8\uacc4\uc5d0 \uac1c\uc785\ud574\uc11c \uc0ac\uc6a9\uc790\uac00 \uacc4\uc18d \ud560\uc9c0 \ub9d0\uc9c0 \uc2b9\uc778\ud558\ub294 \ub2e8\uacc4\ub97c \uac70\uce69\ub2c8\ub2e4.</p> <p>(2) <code>Debugging</code> - \ub418\uac10\uc544\uc11c \ubb38\uc81c\ub97c \uc7ac\ud604\ud569\ub2c8\ub2e4, \ub514\ubc84\uae45\uc5d0 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4, \uac1c\uc778\uc801\uc73c\ub85c\ub294 chatgpt \ud3ec\ud568 \uc218\ub9ce\uc740 Agent\ub4e4\uc774 \ub514\ubc84\uae45\uc774 \uc0ac\uc2e4\uc0c1 \ubd88\uac00\ub2a5\ud55c \uacbd\uc6b0\uac00 \ub9ce\uc740 \uac83\uc774 \ubd88\ub9cc\uc871\uc2a4\ub7ec\uc6e0\ub294\ub370, LangGraph\ub294 \ub514\ubc84\uae45\uc774 \uc798 \ub418\uc5b4\uc11c \uc88b\uc2b5\ub2c8\ub2e4.</p> <p>(3) <code>Editing</code> - \uac15\uc81c\ub85c \uc0ac\uc6a9\uc790\uac00 \uc0c1\ud0dc\ub97c \ubcc0\uacbd\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/2_langgraph_advanced_topics/#goals","title":"Goals\u00b6","text":"<p><code>human-in-the-loop</code> \uacfc <code>memory</code> \ub97c \ubaa8\ub450 \uc0ac\uc6a9\ud560 \uac83\uc774\uace0\uc694.</p> <p>\uc774\uc81c <code>multi-agent</code> workflows \ub85c \ub4e4\uc5b4\uac08 \ucc28\ub840\uc785\ub2c8\ub2e4. \uc55e\uc11c \ubc30\uc6b4 \ub0b4\uc6a9\ub4e4\uc744 \uc751\uc6a9\ud558\ub294 multi-agent research assistant \ub97c \ub9cc\ub4e4\uc5b4 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>multi-agent research assistant \ub97c \uad6c\ud604\ud558\uae30 \uc704\ud574\uc11c LangGraph controllability \uc694\uc18c\ub4e4 \uc911 parallelization \ubd80\ud130 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/2_langgraph_advanced_topics/#fan-out-and-fan-in","title":"Fan out and fan in\u00b6","text":"<p>\uac01 \ub178\ub4dc\uac00 State\ub97c \ub36e\uc5b4\uc4f0\ub294 \uc0c1\ud669\uc744 \uac00\uc815\ud574\ubcf4\uc8e0.</p>"},{"location":"tutorial/2_langgraph_advanced_topics/#waiting-for-nodes-to-finish","title":"Waiting for nodes to finish\u00b6","text":"<p>\uc774\ubc88\uc5d0\ub294 \ud55c\ucabd\uc758 path \uc5d0 \ub178\ub4dc\uac00 \ub354 \ub9ce\uc774 \uc788\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/2_langgraph_advanced_topics/#state-update","title":"state \uc758 update \uc21c\uc11c \uc14b\ud305\u00b6","text":"<p>\uc704 \uacb0\uacfc\ub97c \ubcf4\uba74, B2 \ubcf4\ub2e4 C \uac00 \uba3c\uc800 \ub3c4\ucc29\ud588\uc8e0...? \uc5f0\uacb0\ub41c \ub300\ub85c \ucb49\ucb49 \uc774\uc5b4\ub098\uac00\uc11c \uc5f0\uc0b0\ub418\ub294 \uac83\uc774 \uc21c\uc11c\uc778\uac00 \ubd05\ub2c8\ub2e4. \ubb50 \uc774\uac74 \ub7ad\uadf8\ub798\ud504\uc758 \uadf8\ub798\ud504 \ud0d0\uc0c9\uc774 \uc774\ub7f0 \uc2dd\uc73c\ub85c \uad6c\ud604 \ub418\uc5b4 \uc788\ub294 \uac83\uc774\uace0, \uc774 \ubb38\uc11c\ub97c \ubcf4\uace0 \uacc4\uc2e0 \ubd84\uc774\ub77c\uba74 \ub354 \uc124\uba85\uc740 \uc5c6\uc5b4\ub3c4 \ub418\uaca0\uc8e0.</p> <p>\uc544\ubb34\ud2bc \uadf8\ub798\ud504 \ud0d0\uc0c9 \uc21c\uc11c\ub294 \ucee8\ud2b8\ub864\uc774 \uc548\ub429\ub2c8\ub2e4.</p> <p>\ub300\uc2e0 custom reducer \ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc8e0. (sort state updates)</p>"},{"location":"tutorial/2_langgraph_advanced_topics/#llm","title":"LLM \uc5d0 \uc801\uc6a9\ud558\uae30\u00b6","text":"<p>\uc2e4\uc804\uc73c\ub85c \uac00 \ubd05\uc2dc\ub2e4.</p> <p>\uc704\ud0a4\ub791 \uc6f9\uc5d0\uc11c \ubcd1\ub82c\uc801\uc73c\ub85c \uac80\uc0c9\ud574\uc11c \ub300\ub2f5\uc744 \ud558\ub3c4\ub85d \ud574\ubcf4\uc8e0.</p>"},{"location":"tutorial/2_langgraph_advanced_topics/#langgraph-api","title":"LangGraph API\u00b6","text":"<p>--</p> <p>\u26a0\ufe0f DISCLAIMER</p> <p>LangGraph Studio \uac00 \ud604\uc7ac (24.9.30) \ub294 MACOS \ub9cc \uc9c0\uc6d0\ud569\ub2c8\ub2e4.</p> <p>--</p> <p>\uc628\ub77c\uc778\uc73c\ub85c \ubcf8 \ubb38\uc11c\ub97c \ubcf4\uc2dc\ub294 \ubd84\ub4e4\uc740 \ub098\uc911\uc5d0 \uc81c\uac00 \uc601\uc0c1\uc73c\ub85c \uc81c\uc791\ud574\uc11c \uc62c\ub9ac\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <p></p>"},{"location":"tutorial/2_langgraph_advanced_topics/#sub-graph","title":"Sub-Graph\u00b6","text":"<p>multi-agent \uad6c\ud604\uc744 \uc704\ud574 \ub2e4\uc74c \ub2e4\ub8f0 LangGraph controllable \uc694\uc18c\ub294 Sub-Graph \uc785\ub2c8\ub2e4.</p>"},{"location":"tutorial/2_langgraph_advanced_topics/#state","title":"State\u00b6","text":"<p>Sub-graphs \ub294 State \ub97c \ub098\ub220\uc11c \uac01\uac01 \uad00\ub9ac\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. \uc774\uc81c\uae4c\uc9c0\ub294 State \ub97c \ud558\ub098\ub9cc \uac00\uc9c0\uace0 \uc774\ub807\uac8c \uc800\ub807\uac8c \ub2e4\ub918\uc5c8\uc8e0.</p> <p>multi-agent systems, \ud2b9\ud788 agent \ub4e4\uc744 \ud300\uc73c\ub85c \uc774\ub904\uc11c state \ub97c \uac00\uc9c0\uace0 \uc791\uc5c5 \ud558\ub3c4\ub85d \ub9cc\ub4e4\uc5b4 \uc90d\ub2c8\ub2e4.</p> <p>\uac04\ub2e8\ud55c \uc608\uc2dc\ub97c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <ul> <li>\ub85c\uadf8\ub97c \ubc1b\ub294 \uc2dc\uc2a4\ud15c\uc785\ub2c8\ub2e4.</li> <li>task\ub97c \ub450 \uac1c\ub85c \ub098\ub220\ubcf4\uaca0\uc2b5\ub2c8\ub2e4<ul> <li>\ub85c\uadf8\ub97c \uc694\uc57d\ud558\ub294 task</li> <li>\ubb38\uc81c\ub97c \ucc3e\ub294 task</li> </ul> </li> <li>\uc774 \ub450\uac00\uc9c0 \uc77c\uc744 \uac01\uac01 \ub098\ub220\uc11c \ub530\ub85c\ub530\ub85c agent\uc5d0\uac8c \uc2dc\ud0a4\uace0 \uc2f6\uaca0\uc8e0?</li> </ul> <p>\uac01\uac01 \ub098\ub220\uc11c \uc77c\uc744 \uc2dc\ud0a4\uba74, LLM Application \uc744 \ub450 \uac1c\ub85c \uad6c\ud604\uc744 \ud558\uba74 \ub429\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc644\uc804\ud788 \ub3c5\ub9bd\uc801\uc778 \uc77c\uc774 \uc544\ub2c8\uae30 \ub54c\ubb38\uc5d0 \uc11c\ub85c \uc18c\ud1b5\uc744 \ud558\uba74 \ub354 \uc88b\uaca0\uc8e0. Sub-Graph\ub85c \ub098\ub220\uc11c \uc77c\uc744 \uc2dc\ud0a4\ub824\uace0 \ud558\uace0, \ubb38\uc81c\ub294 \ub450 subgraph \uac04\uc758 \ud1b5\uc2e0\uc785\ub2c8\ub2e4.</p> <p>\ubbf8\ub9ac \ub9d0\ud558\uc790\uba74, \ud1b5\uc2e0\uc740 \uc774\ub807\uac8c \ud574\uacb0\ud569\ub2c8\ub2e4, \uc774 \ud30c\ud2b8\ub294 \uc6d0\ubb38\uc744 \uc815\ud655\ud55c \uc804\ub2ec\uc744 \uc704\ud574 \ubc88\uc5ed\ud558\uc9c0 \uc54a\uace0 \uc4f0\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>done with over-lapping keys:</p> <ul> <li>The sub-graphs can access <code>docs</code> from the parent</li> <li>The parent can access <code>summary/failure_report</code> from the sub-graphs</li> </ul> <p></p>"},{"location":"tutorial/2_langgraph_advanced_topics/#input","title":"Input\u00b6","text":"<p>\ub85c\uadf8\ub97c \ubc1b\uae30 \uc704\ud55c \uc2a4\ud0a4\ub9c8 \ubd80\ud130 \uc7a1\uace0 \uac00\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/2_langgraph_advanced_topics/#sub-graph","title":"Sub Graph\u00b6","text":"<p>sub graph \ubd80\ud130 \ud558\ub098\uc529 \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/2_langgraph_advanced_topics/#sub-graph-graph","title":"Sub Graph\ub4e4\uc744 \ubd80\ubaa8 Graph\uc5d0 \uc774\uc5b4 \ubd99\uc774\uae30\u00b6","text":"<p>\ud569\uccd0\ubcf4\uc8e0.</p> <p><code>EntryGraphState</code> \ub85c \ubd80\ubaa8 Graph\ub97c \ub9cc\ub4e4\uace0\uc694,</p> <p>\ub178\ub4dc\ub85c \ub2e4 \ubd99\uc5ec\uc90d\ub2c8\ub2e4.</p> <pre><code>entry_builder.add_node(\"question_summarization\", qs_builder.compile())\nentry_builder.add_node(\"failure_analysis\", fa_builder.compile())\n</code></pre>"},{"location":"tutorial/2_langgraph_advanced_topics/#map-reduce","title":"Map-Reduce\u00b6","text":"<p>multi-agent \uad6c\ud604\uc744 \uc704\ud574 \ub2e4\uc74c \ub2e4\ub8f0 LangGraph controllable \uc694\uc18c\ub294 Map-Reduce \uc785\ub2c8\ub2e4.</p>"},{"location":"tutorial/2_langgraph_advanced_topics/","title":"\ubb38\uc81c \uc815\uc758\u00b6","text":"<p>Map-reduce \ub294 task \ub97c \ub098\ub204\uace0 \ubcd1\ub82c\uc801\uc73c\ub85c \uc2dc\ud0a4\uace0 \ub3cc\uc544\uc624\ub294 \uacfc\uc815\uc5d0\uc11c \uaf2d \ud544\uc694\ud560 \uc77c\uc785\ub2c8\ub2e4.</p> <p>\ub450 \ud398\uc774\uc988,</p> <p>(1) <code>Map</code> - task\ub97c \uc791\uc740 sub task\ub85c \ub098\ub204\uace0, \uac01\uac01 \ucc98\ub9ac\ud569\ub2c8\ub2e4.</p> <p>(2) <code>Reduce</code> - sub task\uc758 \uacb0\uacfc\ub4e4\uc774 \uc644\ub8cc\ub418\uba74, aggregate \ud569\ub2c8\ub2e4.</p> <p>\uac00 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc608\uc2dc\ub97c \uc7a1\uace0 \uad6c\ud604\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>(1) <code>Map</code> - \uac01 \ud1a0\ud53d\ub4e4\uc5d0 \ub530\ub77c, joke\ub97c \uc5ec\ub7ec\uac1c \ub9cc\ub4e7\ub2c8\ub2e4.</p> <p>(2) <code>Reduce</code> - \uc0dd\uc131\ub41c joke \ub4e4\uc744 \ubaa8\ub450 \ucde8\ud569\ud558\uace0, \uc88b\uc740 \uac83\ub9cc \uace0\ub985\ub2c8\ub2e4.</p> <p>Joke \uc0dd\uc131\uacfc \uc120\ud0dd \ubaa8\ub450 LLM \uc774 \ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/2_langgraph_advanced_topics/#state","title":"State\u00b6","text":""},{"location":"tutorial/2_langgraph_advanced_topics/#joke","title":"\ubcd1\ub82c\uc801\uc778 joke \uc0dd\uc131\u00b6","text":"<p>\uadf8\ub798\ud504 \uba3c\uc800 \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4.</p> <ul> <li>\uc720\uc800\uc5d0\uac8c \ud1a0\ud53d\uc744 \ubc1b\uc2b5\ub2c8\ub2e4.</li> <li>\ud1a0\ud53d\uc744 \ub9ac\uc2a4\ub85c \ub9cc\ub4e4\uace0, joke \uc0dd\uc131 \ub178\ub4dc\uc5d0 \uac01\uac01 \ubcf4\ub0c5\ub2c8\ub2e4.</li> </ul> <p>State \ub294 <code>jokes</code> \ud0a4\ub97c \uac00\uc9c0\uace0, \uc0dd\uc131\ub41c jokes \ub4e4\uc744 \ucde8\ud569\ud569\ub2c8\ub2e4.</p>"},{"location":"tutorial/2_langgraph_advanced_topics/#joke-generation-map","title":"Joke generation (map)\u00b6","text":"<p><code>generate_joke</code> \ub178\ub4dc\ub97c \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4. \uc774 \uce5c\uad6c\uac00 map \uc785\ub2c8\ub2e4.</p> <p><code>OverallState</code> \uc5d0 <code>jokes</code> key \ub85c \uadf8 \uacb0\uacfc\ub97c \uc801\uc5b4\uc918\uc57c \ud569\ub2c8\ub2e4.</p> <p>\uadf8\ub798\uc57c \ub098\uc911\uc5d0, \ub2e4 \ud569\uccd0\uc11c \uc120\ud0dd\uc744 \ud558\uaca0\uc8e0.</p>"},{"location":"tutorial/2_langgraph_advanced_topics/#best-joke-selection-reduce","title":"Best joke selection (reduce)\u00b6","text":"<p>\uc88b\uc740 joke\ub97c pick \ud558\ub294 \ub178\ub4dc\ub3c4 \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4. \uc774\uce5c\uad6c\uac00 reduce \uc8e0.</p>"},{"location":"tutorial/3_MultiAgent_research_assistant/","title":"Research Assistant","text":"In\u00a0[1]: Copied! <pre>%%capture --no-stderr\n%pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python wikipedia\n</pre> %%capture --no-stderr %pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python wikipedia In\u00a0[2]: Copied! <pre>import os, getpass\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n</pre> import os, getpass  def _set_env(var: str):     if not os.environ.get(var):         os.environ[var] = getpass.getpass(f\"{var}: \")  _set_env(\"OPENAI_API_KEY\") <pre>OPENAI_API_KEY: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[3]: Copied! <pre>from langchain_openai import ChatOpenAI\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n</pre> from langchain_openai import ChatOpenAI llm = ChatOpenAI(model=\"gpt-4o\", temperature=0) In\u00a0[4]: Copied! <pre>_set_env(\"LANGCHAIN_API_KEY\")\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\"\n</pre> _set_env(\"LANGCHAIN_API_KEY\") os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\" <pre>LANGCHAIN_API_KEY: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[5]: Copied! <pre>from typing import List\nfrom typing_extensions import TypedDict\nfrom pydantic import BaseModel, Field\n\nclass Analyst(BaseModel):\n    affiliation: str = Field(\n        description=\"Primary affiliation of the analyst.\",\n    )\n    name: str = Field(\n        description=\"Name of the analyst.\"\n    )\n    role: str = Field(\n        description=\"Role of the analyst in the context of the topic.\",\n    )\n    description: str = Field(\n        description=\"Description of the analyst focus, concerns, and motives.\",\n    )\n    @property\n    def persona(self) -&gt; str:\n        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n\nclass Perspectives(BaseModel):\n    analysts: List[Analyst] = Field(\n        description=\"Comprehensive list of analysts with their roles and affiliations.\",\n    )\n\nclass GenerateAnalystsState(TypedDict):\n    topic: str # Research topic\n    max_analysts: int # Number of analysts\n    human_analyst_feedback: str # Human feedback\n    analysts: List[Analyst] # Analyst asking questions\n</pre> from typing import List from typing_extensions import TypedDict from pydantic import BaseModel, Field  class Analyst(BaseModel):     affiliation: str = Field(         description=\"Primary affiliation of the analyst.\",     )     name: str = Field(         description=\"Name of the analyst.\"     )     role: str = Field(         description=\"Role of the analyst in the context of the topic.\",     )     description: str = Field(         description=\"Description of the analyst focus, concerns, and motives.\",     )     @property     def persona(self) -&gt; str:         return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"  class Perspectives(BaseModel):     analysts: List[Analyst] = Field(         description=\"Comprehensive list of analysts with their roles and affiliations.\",     )  class GenerateAnalystsState(TypedDict):     topic: str # Research topic     max_analysts: int # Number of analysts     human_analyst_feedback: str # Human feedback     analysts: List[Analyst] # Analyst asking questions In\u00a0[6]: Copied! <pre>from IPython.display import Image, display\nfrom langgraph.graph import START, END, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n\nanalyst_instructions=\"\"\"You are tasked with creating a set of AI analyst personas. Follow these instructions carefully:\n\n1. First, review the research topic:\n{topic}\n\n2. Examine any editorial feedback that has been optionally provided to guide creation of the analysts:\n\n{human_analyst_feedback}\n\n3. Determine the most interesting themes based upon documents and / or feedback above.\n\n4. Pick the top {max_analysts} themes.\n\n5. Assign one analyst to each theme.\"\"\"\n\ndef create_analysts(state: GenerateAnalystsState):\n\n    \"\"\" Create analysts \"\"\"\n\n    topic=state['topic']\n    max_analysts=state['max_analysts']\n    human_analyst_feedback=state.get('human_analyst_feedback', '')\n\n    # Enforce structured output\n    structured_llm = llm.with_structured_output(Perspectives)\n\n    # System message\n    system_message = analyst_instructions.format(topic=topic,\n                                                            human_analyst_feedback=human_analyst_feedback,\n                                                            max_analysts=max_analysts)\n\n    # Generate question\n    analysts = structured_llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=\"Generate the set of analysts.\")])\n\n    # Write the list of analysis to state\n    return {\"analysts\": analysts.analysts}\n\ndef human_feedback(state: GenerateAnalystsState):\n    \"\"\" No-op node that should be interrupted on \"\"\"\n    pass\n\ndef should_continue(state: GenerateAnalystsState):\n    \"\"\" Return the next node to execute \"\"\"\n\n    # Check if human feedback\n    human_analyst_feedback=state.get('human_analyst_feedback', None)\n    if human_analyst_feedback:\n        return \"create_analysts\"\n\n    # Otherwise end\n    return END\n\n# Add nodes and edges\nbuilder = StateGraph(GenerateAnalystsState)\nbuilder.add_node(\"create_analysts\", create_analysts)\nbuilder.add_node(\"human_feedback\", human_feedback)\nbuilder.add_edge(START, \"create_analysts\")\nbuilder.add_edge(\"create_analysts\", \"human_feedback\")\nbuilder.add_conditional_edges(\"human_feedback\", should_continue, [\"create_analysts\", END])\n\n# Compile\nmemory = MemorySaver()\ngraph = builder.compile(interrupt_before=['human_feedback'], checkpointer=memory)\n\n# View\ndisplay(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n</pre> from IPython.display import Image, display from langgraph.graph import START, END, StateGraph from langgraph.checkpoint.memory import MemorySaver from langchain_core.messages import AIMessage, HumanMessage, SystemMessage  analyst_instructions=\"\"\"You are tasked with creating a set of AI analyst personas. Follow these instructions carefully:  1. First, review the research topic: {topic}  2. Examine any editorial feedback that has been optionally provided to guide creation of the analysts:  {human_analyst_feedback}  3. Determine the most interesting themes based upon documents and / or feedback above.  4. Pick the top {max_analysts} themes.  5. Assign one analyst to each theme.\"\"\"  def create_analysts(state: GenerateAnalystsState):      \"\"\" Create analysts \"\"\"      topic=state['topic']     max_analysts=state['max_analysts']     human_analyst_feedback=state.get('human_analyst_feedback', '')      # Enforce structured output     structured_llm = llm.with_structured_output(Perspectives)      # System message     system_message = analyst_instructions.format(topic=topic,                                                             human_analyst_feedback=human_analyst_feedback,                                                             max_analysts=max_analysts)      # Generate question     analysts = structured_llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=\"Generate the set of analysts.\")])      # Write the list of analysis to state     return {\"analysts\": analysts.analysts}  def human_feedback(state: GenerateAnalystsState):     \"\"\" No-op node that should be interrupted on \"\"\"     pass  def should_continue(state: GenerateAnalystsState):     \"\"\" Return the next node to execute \"\"\"      # Check if human feedback     human_analyst_feedback=state.get('human_analyst_feedback', None)     if human_analyst_feedback:         return \"create_analysts\"      # Otherwise end     return END  # Add nodes and edges builder = StateGraph(GenerateAnalystsState) builder.add_node(\"create_analysts\", create_analysts) builder.add_node(\"human_feedback\", human_feedback) builder.add_edge(START, \"create_analysts\") builder.add_edge(\"create_analysts\", \"human_feedback\") builder.add_conditional_edges(\"human_feedback\", should_continue, [\"create_analysts\", END])  # Compile memory = MemorySaver() graph = builder.compile(interrupt_before=['human_feedback'], checkpointer=memory)  # View display(Image(graph.get_graph(xray=1).draw_mermaid_png())) In\u00a0[7]: Copied! <pre># Input\nmax_analysts = 3\ntopic = \"Llama3, Llama3.1, Llama3.2 \ubaa8\ub378\ub4e4\uc5d0 \ub300\ud55c \ubd84\uc11d\"\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Run the graph until the first interruption\nfor event in graph.stream({\"topic\":topic,\"max_analysts\":max_analysts,}, thread, stream_mode=\"values\"):\n    # Review\n    analysts = event.get('analysts', '')\n    if analysts:\n        for analyst in analysts:\n            print(f\"Name: {analyst.name}\")\n            print(f\"Affiliation: {analyst.affiliation}\")\n            print(f\"Role: {analyst.role}\")\n            print(f\"Description: {analyst.description}\")\n            print(\"-\" * 50)\n</pre> # Input max_analysts = 3 topic = \"Llama3, Llama3.1, Llama3.2 \ubaa8\ub378\ub4e4\uc5d0 \ub300\ud55c \ubd84\uc11d\" thread = {\"configurable\": {\"thread_id\": \"1\"}}  # Run the graph until the first interruption for event in graph.stream({\"topic\":topic,\"max_analysts\":max_analysts,}, thread, stream_mode=\"values\"):     # Review     analysts = event.get('analysts', '')     if analysts:         for analyst in analysts:             print(f\"Name: {analyst.name}\")             print(f\"Affiliation: {analyst.affiliation}\")             print(f\"Role: {analyst.role}\")             print(f\"Description: {analyst.description}\")             print(\"-\" * 50) <pre>Name: Dr. Emily Carter\nAffiliation: OpenAI\nRole: Model Performance Specialist\nDescription: Dr. Carter focuses on the performance metrics and efficiency of AI models. She is particularly interested in the computational efficiency and accuracy of Llama3 series models.\n--------------------------------------------------\nName: Prof. Michael Zhang\nAffiliation: Stanford University\nRole: Ethics and Bias Researcher\nDescription: Prof. Zhang examines the ethical implications and potential biases in AI models. His work with the Llama3 series involves identifying and mitigating biases in the models' outputs.\n--------------------------------------------------\nName: Dr. Sarah Lee\nAffiliation: Google DeepMind\nRole: Innovation and Applications Expert\nDescription: Dr. Lee explores innovative applications and real-world use cases of AI models. She is interested in how the Llama3 series can be applied in various industries and the potential for new technological advancements.\n--------------------------------------------------\n</pre> In\u00a0[8]: Copied! <pre># Get state and look at next node\nstate = graph.get_state(thread)\nstate.next\n</pre> # Get state and look at next node state = graph.get_state(thread) state.next Out[8]: <pre>('human_feedback',)</pre> In\u00a0[9]: Copied! <pre># We now update the state as if we are the human_feedback node\ngraph.update_state(thread, {\"human_analyst_feedback\":\n                            \"\uc2a4\ud0c0\ud2b8\uc5c5\uc758 \uad00\uc810\uc5d0\uc11c Llama \ubaa8\ub378\ub4e4\uc744 \ud65c\uc6a9\ud560 \uc218 \uc788\ub294 \ubc29\uc548\uc5d0 \ub300\ud574 \uc870\uc0ac\ud560 \uc218 \uc788\ub294 \uc0ac\ub78c\uc744 \ucd94\uac00\ud574\uc918\"}, as_node=\"human_feedback\")\n</pre> # We now update the state as if we are the human_feedback node graph.update_state(thread, {\"human_analyst_feedback\":                             \"\uc2a4\ud0c0\ud2b8\uc5c5\uc758 \uad00\uc810\uc5d0\uc11c Llama \ubaa8\ub378\ub4e4\uc744 \ud65c\uc6a9\ud560 \uc218 \uc788\ub294 \ubc29\uc548\uc5d0 \ub300\ud574 \uc870\uc0ac\ud560 \uc218 \uc788\ub294 \uc0ac\ub78c\uc744 \ucd94\uac00\ud574\uc918\"}, as_node=\"human_feedback\") Out[9]: <pre>{'configurable': {'thread_id': '1',\n  'checkpoint_ns': '',\n  'checkpoint_id': '1ef80803-4512-6d04-8002-c5b0b3792ba9'}}</pre> In\u00a0[10]: Copied! <pre># Continue the graph execution\nfor event in graph.stream(None, thread, stream_mode=\"values\"):\n    # Review\n    analysts = event.get('analysts', '')\n    if analysts:\n        for analyst in analysts:\n            print(f\"Name: {analyst.name}\")\n            print(f\"Affiliation: {analyst.affiliation}\")\n            print(f\"Role: {analyst.role}\")\n            print(f\"Description: {analyst.description}\")\n            print(\"-\" * 50)\n</pre> # Continue the graph execution for event in graph.stream(None, thread, stream_mode=\"values\"):     # Review     analysts = event.get('analysts', '')     if analysts:         for analyst in analysts:             print(f\"Name: {analyst.name}\")             print(f\"Affiliation: {analyst.affiliation}\")             print(f\"Role: {analyst.role}\")             print(f\"Description: {analyst.description}\")             print(\"-\" * 50) <pre>Name: Dr. Emily Carter\nAffiliation: OpenAI\nRole: Model Performance Specialist\nDescription: Dr. Carter focuses on the performance metrics and efficiency of AI models. She is particularly interested in the computational efficiency and accuracy of Llama3 series models.\n--------------------------------------------------\nName: Prof. Michael Zhang\nAffiliation: Stanford University\nRole: Ethics and Bias Researcher\nDescription: Prof. Zhang examines the ethical implications and potential biases in AI models. His work with the Llama3 series involves identifying and mitigating biases in the models' outputs.\n--------------------------------------------------\nName: Dr. Sarah Lee\nAffiliation: Google DeepMind\nRole: Innovation and Applications Expert\nDescription: Dr. Lee explores innovative applications and real-world use cases of AI models. She is interested in how the Llama3 series can be applied in various industries and the potential for new technological advancements.\n--------------------------------------------------\nName: Dr. Min-Jae Kim\nAffiliation: AI Research Lab\nRole: Lead AI Researcher\nDescription: Dr. Kim focuses on the technical advancements and performance metrics of the Llama3 series models. His primary concern is to evaluate the models' capabilities, limitations, and potential improvements.\n--------------------------------------------------\nName: Soo-Jin Park\nAffiliation: Tech Startup Incubator\nRole: Startup Strategy Consultant\nDescription: Soo-Jin specializes in advising startups on integrating advanced AI models like Llama3 into their business strategies. She explores practical applications, cost-benefit analyses, and market positioning.\n--------------------------------------------------\nName: Professor Hyeon-Woo Lee\nAffiliation: University of Seoul\nRole: AI Ethics and Policy Expert\nDescription: Professor Lee examines the ethical implications and policy considerations of deploying Llama3 models. His focus includes data privacy, algorithmic bias, and regulatory compliance.\n--------------------------------------------------\n</pre> In\u00a0[11]: Copied! <pre># If we are satisfied, then we simply supply no feedback\nfurther_feedack = None\ngraph.update_state(thread, {\"human_analyst_feedback\":\n                            further_feedack}, as_node=\"human_feedback\")\n</pre> # If we are satisfied, then we simply supply no feedback further_feedack = None graph.update_state(thread, {\"human_analyst_feedback\":                             further_feedack}, as_node=\"human_feedback\") Out[11]: <pre>{'configurable': {'thread_id': '1',\n  'checkpoint_ns': '',\n  'checkpoint_id': '1ef80804-889b-6751-8004-bb0dd29aadcc'}}</pre> In\u00a0[12]: Copied! <pre># Continue the graph execution to end\nfor event in graph.stream(None, thread, stream_mode=\"updates\"):\n    print(\"--Node--\")\n    node_name = next(iter(event.keys()))\n    print(node_name)\n</pre> # Continue the graph execution to end for event in graph.stream(None, thread, stream_mode=\"updates\"):     print(\"--Node--\")     node_name = next(iter(event.keys()))     print(node_name) In\u00a0[13]: Copied! <pre>final_state = graph.get_state(thread)\nanalysts = final_state.values.get('analysts')\n</pre> final_state = graph.get_state(thread) analysts = final_state.values.get('analysts') In\u00a0[14]: Copied! <pre>final_state.next\n</pre> final_state.next Out[14]: <pre>()</pre> In\u00a0[15]: Copied! <pre>for analyst in analysts:\n    print(f\"Name: {analyst.name}\")\n    print(f\"Affiliation: {analyst.affiliation}\")\n    print(f\"Role: {analyst.role}\")\n    print(f\"Description: {analyst.description}\")\n    print(\"-\" * 50)\n</pre> for analyst in analysts:     print(f\"Name: {analyst.name}\")     print(f\"Affiliation: {analyst.affiliation}\")     print(f\"Role: {analyst.role}\")     print(f\"Description: {analyst.description}\")     print(\"-\" * 50) <pre>Name: Dr. Min-Jae Kim\nAffiliation: AI Research Lab\nRole: Lead AI Researcher\nDescription: Dr. Kim focuses on the technical advancements and performance metrics of the Llama3 series models. His primary concern is to evaluate the models' capabilities, limitations, and potential improvements.\n--------------------------------------------------\nName: Soo-Jin Park\nAffiliation: Tech Startup Incubator\nRole: Startup Strategy Consultant\nDescription: Soo-Jin specializes in advising startups on integrating advanced AI models like Llama3 into their business strategies. She explores practical applications, cost-benefit analyses, and market positioning.\n--------------------------------------------------\nName: Professor Hyeon-Woo Lee\nAffiliation: University of Seoul\nRole: AI Ethics and Policy Expert\nDescription: Professor Lee examines the ethical implications and policy considerations of deploying Llama3 models. His focus includes data privacy, algorithmic bias, and regulatory compliance.\n--------------------------------------------------\n</pre> In\u00a0[16]: Copied! <pre>import operator\nfrom typing import  Annotated\nfrom langgraph.graph import MessagesState\n\nclass InterviewState(MessagesState):\n    max_num_turns: int # Number turns of conversation\n    context: Annotated[list, operator.add] # Source docs\n    analyst: Analyst # Analyst asking questions\n    interview: str # Interview transcript\n    sections: list # Final key we duplicate in outer state for Send() API\n\nclass SearchQuery(BaseModel):\n    search_query: str = Field(None, description=\"Search query for retrieval.\")\n</pre> import operator from typing import  Annotated from langgraph.graph import MessagesState  class InterviewState(MessagesState):     max_num_turns: int # Number turns of conversation     context: Annotated[list, operator.add] # Source docs     analyst: Analyst # Analyst asking questions     interview: str # Interview transcript     sections: list # Final key we duplicate in outer state for Send() API  class SearchQuery(BaseModel):     search_query: str = Field(None, description=\"Search query for retrieval.\") In\u00a0[17]: Copied! <pre>question_instructions = \"\"\"You are an analyst tasked with interviewing an expert to learn about a specific topic.\n\nYour goal is boil down to interesting and specific insights related to your topic.\n\n1. Interesting: Insights that people will find surprising or non-obvious.\n\n2. Specific: Insights that avoid generalities and include specific examples from the expert.\n\nHere is your topic of focus and set of goals: {goals}\n\nBegin by introducing yourself using a name that fits your persona, and then ask your question.\n\nContinue to ask questions to drill down and refine your understanding of the topic.\n\nWhen you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\n\nRemember to stay in character throughout your response, reflecting the persona and goals provided to you.\"\"\"\n\ndef generate_question(state: InterviewState):\n    \"\"\" Node to generate a question \"\"\"\n\n    # Get state\n    analyst = state[\"analyst\"]\n    messages = state[\"messages\"]\n\n    # Generate question\n    system_message = question_instructions.format(goals=analyst.persona)\n    question = llm.invoke([SystemMessage(content=system_message)]+messages)\n\n    # Write messages to state\n    return {\"messages\": [question]}\n</pre> question_instructions = \"\"\"You are an analyst tasked with interviewing an expert to learn about a specific topic.  Your goal is boil down to interesting and specific insights related to your topic.  1. Interesting: Insights that people will find surprising or non-obvious.  2. Specific: Insights that avoid generalities and include specific examples from the expert.  Here is your topic of focus and set of goals: {goals}  Begin by introducing yourself using a name that fits your persona, and then ask your question.  Continue to ask questions to drill down and refine your understanding of the topic.  When you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"  Remember to stay in character throughout your response, reflecting the persona and goals provided to you.\"\"\"  def generate_question(state: InterviewState):     \"\"\" Node to generate a question \"\"\"      # Get state     analyst = state[\"analyst\"]     messages = state[\"messages\"]      # Generate question     system_message = question_instructions.format(goals=analyst.persona)     question = llm.invoke([SystemMessage(content=system_message)]+messages)      # Write messages to state     return {\"messages\": [question]} In\u00a0[18]: Copied! <pre>def _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"TAVILY_API_KEY\")\n</pre> def _set_env(var: str):     if not os.environ.get(var):         os.environ[var] = getpass.getpass(f\"{var}: \")  _set_env(\"TAVILY_API_KEY\") <pre>TAVILY_API_KEY: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[19]: Copied! <pre># Web search tool\nfrom langchain_community.tools.tavily_search import TavilySearchResults\ntavily_search = TavilySearchResults(max_results=3)\n</pre> # Web search tool from langchain_community.tools.tavily_search import TavilySearchResults tavily_search = TavilySearchResults(max_results=3) In\u00a0[20]: Copied! <pre># Wikipedia search tool\nfrom langchain_community.document_loaders import WikipediaLoader\n</pre> # Wikipedia search tool from langchain_community.document_loaders import WikipediaLoader <p>\uc704\ud0a4\ub791 \uc6f9\uc744 \uac80\uc0c9\ud558\ub294 \ub178\ub4dc\ub97c \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>\uc560\ub110\ub9ac\uc2a4\ud2b8\uc5d0 \ub2f5\ubcc0\uc744 \uc0dd\uc131\ud574\uc8fc\ub294 \ub178\ub4dc\ub3c4 \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>\uc804\uccb4 \uc778\ud130\ubdf0 \ub0b4\uc6a9\uc744 \uae30\ub85d\ud558\uace0, \uc694\uc57d\ud574\uc11c \uc139\uc158\uc73c\ub85c \ub118\uaca8\uc8fc\ub294 \ub178\ub4dc\ub3c4 \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4.</p> In\u00a0[21]: Copied! <pre>from langchain_core.messages import get_buffer_string\n\n# Search query writing\nsearch_instructions = SystemMessage(content=f\"\"\"You will be given a conversation between an analyst and an expert.\n\nYour goal is to generate a well-structured query for use in retrieval and / or web-search related to the conversation.\n\nFirst, analyze the full conversation.\n\nPay particular attention to the final question posed by the analyst.\n\nConvert this final question into a well-structured web search query\"\"\")\n\ndef search_web(state: InterviewState):\n\n    \"\"\" Retrieve docs from web search \"\"\"\n\n    # Search query\n    structured_llm = llm.with_structured_output(SearchQuery)\n    search_query = structured_llm.invoke([search_instructions]+state['messages'])\n\n    # Search\n    search_docs = tavily_search.invoke(search_query.search_query)\n\n     # Format\n    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n        [\n            f'&lt;Document href=\"{doc[\"url\"]}\"/&gt;\\n{doc[\"content\"]}\\n&lt;/Document&gt;'\n            for doc in search_docs\n        ]\n    )\n\n    return {\"context\": [formatted_search_docs]}\n\ndef search_wikipedia(state: InterviewState):\n\n    \"\"\" Retrieve docs from wikipedia \"\"\"\n\n    # Search query\n    structured_llm = llm.with_structured_output(SearchQuery)\n    search_query = structured_llm.invoke([search_instructions]+state['messages'])\n\n    # Search\n    search_docs = WikipediaLoader(query=search_query.search_query,\n                                  load_max_docs=2).load()\n\n     # Format\n    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n        [\n            f'&lt;Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/&gt;\\n{doc.page_content}\\n&lt;/Document&gt;'\n            for doc in search_docs\n        ]\n    )\n\n    return {\"context\": [formatted_search_docs]}\n\nanswer_instructions = \"\"\"You are an expert being interviewed by an analyst.\n\nHere is analyst area of focus: {goals}.\n\nYou goal is to answer a question posed by the interviewer.\n\nTo answer question, use this context:\n\n{context}\n\nWhen answering questions, follow these guidelines:\n\n1. Use only the information provided in the context.\n\n2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n\n3. The context contain sources at the topic of each individual document.\n\n4. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1].\n\n5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\n\n6. If the source is: &lt;Document source=\"assistant/docs/llama3_1.pdf\" page=\"7\"/&gt;' then just list:\n\n[1] assistant/docs/llama3_1.pdf, page 7\n\nAnd skip the addition of the brackets as well as the Document source preamble in your citation.\"\"\"\n\ndef generate_answer(state: InterviewState):\n\n    \"\"\" Node to answer a question \"\"\"\n\n    # Get state\n    analyst = state[\"analyst\"]\n    messages = state[\"messages\"]\n    context = state[\"context\"]\n\n    # Answer question\n    system_message = answer_instructions.format(goals=analyst.persona, context=context)\n    answer = llm.invoke([SystemMessage(content=system_message)]+messages)\n\n    # Name the message as coming from the expert\n    answer.name = \"expert\"\n\n    # Append it to state\n    return {\"messages\": [answer]}\n\ndef save_interview(state: InterviewState):\n\n    \"\"\" Save interviews \"\"\"\n\n    # Get messages\n    messages = state[\"messages\"]\n\n    # Convert interview to a string\n    interview = get_buffer_string(messages)\n\n    # Save to interviews key\n    return {\"interview\": interview}\n\ndef route_messages(state: InterviewState,\n                   name: str = \"expert\"):\n\n    \"\"\" Route between question and answer \"\"\"\n\n    # Get messages\n    messages = state[\"messages\"]\n    max_num_turns = state.get('max_num_turns',2)\n\n    # Check the number of expert answers\n    num_responses = len(\n        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n    )\n\n    # End if expert has answered more than the max turns\n    if num_responses &gt;= max_num_turns:\n        return 'save_interview'\n\n    # This router is run after each question - answer pair\n    # Get the last question asked to check if it signals the end of discussion\n    last_question = messages[-2]\n\n    if \"Thank you so much for your help\" in last_question.content:\n        return 'save_interview'\n    return \"ask_question\"\n\nsection_writer_instructions = \"\"\"You are an expert technical writer.\n\nYour task is to create a short, easily digestible section of a report based on a set of source documents.\n\n1. Analyze the content of the source documents:\n- The name of each source document is at the start of the document, with the &lt;Document tag.\n\n2. Create a report structure using markdown formatting:\n- Use ## for the section title\n- Use ### for sub-section headers\n\n3. Write the report following this structure:\na. Title (## header)\nb. Summary (### header)\nc. Sources (### header)\n\n4. Make your title engaging based upon the focus area of the analyst:\n{focus}\n\n5. For the summary section:\n- Set up summary with general background / context related to the focus area of the analyst\n- Emphasize what is novel, interesting, or surprising about insights gathered from the interview\n- Create a numbered list of source documents, as you use them\n- Do not mention the names of interviewers or experts\n- Aim for approximately 400 words maximum\n- Use numbered sources in your report (e.g., [1], [2]) based on information from source documents\n\n6. In the Sources section:\n- Include all sources used in your report\n- Provide full links to relevant websites or specific document paths\n- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n- It will look like:\n\n### Sources\n[1] Link or Document name\n[2] Link or Document name\n\n7. Be sure to combine sources. For example this is not correct:\n\n[3] https://ai.meta.com/blog/meta-llama-3-1/\n[4] https://ai.meta.com/blog/meta-llama-3-1/\n\nThere should be no redundant sources. It should simply be:\n\n[3] https://ai.meta.com/blog/meta-llama-3-1/\n\n8. Final review:\n- Ensure the report follows the required structure\n- Include no preamble before the title of the report\n- Check that all guidelines have been followed\"\"\"\n\ndef write_section(state: InterviewState):\n\n    \"\"\" Node to answer a question \"\"\"\n\n    # Get state\n    interview = state[\"interview\"]\n    context = state[\"context\"]\n    analyst = state[\"analyst\"]\n\n    # Write section using either the gathered source docs from interview (context) or the interview itself (interview)\n    system_message = section_writer_instructions.format(focus=analyst.description)\n    section = llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=f\"Use this source to write your section: {context}\")])\n\n    # Append it to state\n    return {\"sections\": [section.content]}\n\n# Add nodes and edges\ninterview_builder = StateGraph(InterviewState)\ninterview_builder.add_node(\"ask_question\", generate_question)\ninterview_builder.add_node(\"search_web\", search_web)\ninterview_builder.add_node(\"search_wikipedia\", search_wikipedia)\ninterview_builder.add_node(\"answer_question\", generate_answer)\ninterview_builder.add_node(\"save_interview\", save_interview)\ninterview_builder.add_node(\"write_section\", write_section)\n\n# Flow\ninterview_builder.add_edge(START, \"ask_question\")\ninterview_builder.add_edge(\"ask_question\", \"search_web\")\ninterview_builder.add_edge(\"ask_question\", \"search_wikipedia\")\ninterview_builder.add_edge(\"search_web\", \"answer_question\")\ninterview_builder.add_edge(\"search_wikipedia\", \"answer_question\")\ninterview_builder.add_conditional_edges(\"answer_question\", route_messages,['ask_question','save_interview'])\ninterview_builder.add_edge(\"save_interview\", \"write_section\")\ninterview_builder.add_edge(\"write_section\", END)\n\n# Interview\nmemory = MemorySaver()\ninterview_graph = interview_builder.compile(checkpointer=memory).with_config(run_name=\"Conduct Interviews\")\n\n# View\ndisplay(Image(interview_graph.get_graph().draw_mermaid_png()))\n</pre> from langchain_core.messages import get_buffer_string  # Search query writing search_instructions = SystemMessage(content=f\"\"\"You will be given a conversation between an analyst and an expert.  Your goal is to generate a well-structured query for use in retrieval and / or web-search related to the conversation.  First, analyze the full conversation.  Pay particular attention to the final question posed by the analyst.  Convert this final question into a well-structured web search query\"\"\")  def search_web(state: InterviewState):      \"\"\" Retrieve docs from web search \"\"\"      # Search query     structured_llm = llm.with_structured_output(SearchQuery)     search_query = structured_llm.invoke([search_instructions]+state['messages'])      # Search     search_docs = tavily_search.invoke(search_query.search_query)       # Format     formatted_search_docs = \"\\n\\n---\\n\\n\".join(         [             f'\\n{doc[\"content\"]}\\n'             for doc in search_docs         ]     )      return {\"context\": [formatted_search_docs]}  def search_wikipedia(state: InterviewState):      \"\"\" Retrieve docs from wikipedia \"\"\"      # Search query     structured_llm = llm.with_structured_output(SearchQuery)     search_query = structured_llm.invoke([search_instructions]+state['messages'])      # Search     search_docs = WikipediaLoader(query=search_query.search_query,                                   load_max_docs=2).load()       # Format     formatted_search_docs = \"\\n\\n---\\n\\n\".join(         [             f'\\n{doc.page_content}\\n'             for doc in search_docs         ]     )      return {\"context\": [formatted_search_docs]}  answer_instructions = \"\"\"You are an expert being interviewed by an analyst.  Here is analyst area of focus: {goals}.  You goal is to answer a question posed by the interviewer.  To answer question, use this context:  {context}  When answering questions, follow these guidelines:  1. Use only the information provided in the context.  2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.  3. The context contain sources at the topic of each individual document.  4. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1].  5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc  6. If the source is: ' then just list:  [1] assistant/docs/llama3_1.pdf, page 7  And skip the addition of the brackets as well as the Document source preamble in your citation.\"\"\"  def generate_answer(state: InterviewState):      \"\"\" Node to answer a question \"\"\"      # Get state     analyst = state[\"analyst\"]     messages = state[\"messages\"]     context = state[\"context\"]      # Answer question     system_message = answer_instructions.format(goals=analyst.persona, context=context)     answer = llm.invoke([SystemMessage(content=system_message)]+messages)      # Name the message as coming from the expert     answer.name = \"expert\"      # Append it to state     return {\"messages\": [answer]}  def save_interview(state: InterviewState):      \"\"\" Save interviews \"\"\"      # Get messages     messages = state[\"messages\"]      # Convert interview to a string     interview = get_buffer_string(messages)      # Save to interviews key     return {\"interview\": interview}  def route_messages(state: InterviewState,                    name: str = \"expert\"):      \"\"\" Route between question and answer \"\"\"      # Get messages     messages = state[\"messages\"]     max_num_turns = state.get('max_num_turns',2)      # Check the number of expert answers     num_responses = len(         [m for m in messages if isinstance(m, AIMessage) and m.name == name]     )      # End if expert has answered more than the max turns     if num_responses &gt;= max_num_turns:         return 'save_interview'      # This router is run after each question - answer pair     # Get the last question asked to check if it signals the end of discussion     last_question = messages[-2]      if \"Thank you so much for your help\" in last_question.content:         return 'save_interview'     return \"ask_question\"  section_writer_instructions = \"\"\"You are an expert technical writer.  Your task is to create a short, easily digestible section of a report based on a set of source documents.  1. Analyze the content of the source documents: - The name of each source document is at the start of the document, with the  In\u00a0[24]: Copied! <pre># Pick one analyst\nanalysts[0]\n</pre> # Pick one analyst analysts[0] Out[24]: <pre>Analyst(affiliation='AI Research Lab', name='Dr. Min-Jae Kim', role='Lead AI Researcher', description=\"Dr. Kim focuses on the technical advancements and performance metrics of the Llama3 series models. His primary concern is to evaluate the models' capabilities, limitations, and potential improvements.\")</pre> In\u00a0[25]: Copied! <pre>from IPython.display import Markdown\nmessages = [HumanMessage(f\"So you said you were writing an article on {topic}?\")]\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\ninterview = interview_graph.invoke({\"analyst\": analysts[0], \"messages\": messages, \"max_num_turns\": 2}, thread)\nMarkdown(interview['sections'][0])\n</pre> from IPython.display import Markdown messages = [HumanMessage(f\"So you said you were writing an article on {topic}?\")] thread = {\"configurable\": {\"thread_id\": \"1\"}} interview = interview_graph.invoke({\"analyst\": analysts[0], \"messages\": messages, \"max_num_turns\": 2}, thread) Markdown(interview['sections'][0]) Out[25]: In\u00a0[26]: Copied! <pre>import operator\nfrom typing import List, Annotated\nfrom typing_extensions import TypedDict\n\nclass ResearchGraphState(TypedDict):\n    topic: str # Research topic\n    max_analysts: int # Number of analysts\n    human_analyst_feedback: str # Human feedback\n    analysts: List[Analyst] # Analyst asking questions\n    sections: Annotated[list, operator.add] # Send() API key\n    introduction: str # Introduction for the final report\n    content: str # Content for the final report\n    conclusion: str # Conclusion for the final report\n    final_report: str # Final report\n</pre> import operator from typing import List, Annotated from typing_extensions import TypedDict  class ResearchGraphState(TypedDict):     topic: str # Research topic     max_analysts: int # Number of analysts     human_analyst_feedback: str # Human feedback     analysts: List[Analyst] # Analyst asking questions     sections: Annotated[list, operator.add] # Send() API key     introduction: str # Introduction for the final report     content: str # Content for the final report     conclusion: str # Conclusion for the final report     final_report: str # Final report In\u00a0[27]: Copied! <pre>from langgraph.constants import Send\n\ndef initiate_all_interviews(state: ResearchGraphState):\n    \"\"\" This is the \"map\" step where we run each interview sub-graph using Send API \"\"\"\n\n    # Check if human feedback\n    human_analyst_feedback=state.get('human_analyst_feedback')\n    if human_analyst_feedback:\n        # Return to create_analysts\n        return \"create_analysts\"\n\n    # Otherwise kick off interviews in parallel via Send() API\n    else:\n        topic = state[\"topic\"]\n        return [Send(\"conduct_interview\", {\"analyst\": analyst,\n                                           \"messages\": [HumanMessage(\n                                               content=f\"So you said you were writing an article on {topic}?\"\n                                           )\n                                                       ]}) for analyst in state[\"analysts\"]]\n\nreport_writer_instructions = \"\"\"You are a technical writer creating a report on this overall topic:\n\n{topic}\n\nYou have a team of analysts. Each analyst has done two things:\n\n1. They conducted an interview with an expert on a specific sub-topic.\n2. They write up their finding into a memo.\n\nYour task:\n\n1. You will be given a collection of memos from your analysts.\n2. Think carefully about the insights from each memo.\n3. Consolidate these into a crisp overall summary that ties together the central ideas from all of the memos.\n4. Summarize the central points in each memo into a cohesive single narrative.\n\nTo format your report:\n\n1. Use markdown formatting.\n2. Include no pre-amble for the report.\n3. Use no sub-heading.\n4. Start your report with a single title header: ## Insights\n5. Do not mention any analyst names in your report.\n6. Preserve any citations in the memos, which will be annotated in brackets, for example [1] or [2].\n7. Create a final, consolidated list of sources and add to a Sources section with the `## Sources` header.\n8. List your sources in order and do not repeat.\n\n[1] Source 1\n[2] Source 2\n\nHere are the memos from your analysts to build your report from:\n\n{context}\"\"\"\n\ndef write_report(state: ResearchGraphState):\n    # Full set of sections\n    sections = state[\"sections\"]\n    topic = state[\"topic\"]\n\n    # Concat all sections together\n    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n\n    # Summarize the sections into a final report\n    system_message = report_writer_instructions.format(topic=topic, context=formatted_str_sections)\n    report = llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=f\"Write a report based upon these memos.\")])\n    return {\"content\": report.content}\n\nintro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}\n\nYou will be given all of the sections of the report.\n\nYou job is to write a crisp and compelling introduction or conclusion section.\n\nThe user will instruct you whether to write the introduction or conclusion.\n\nInclude no pre-amble for either section.\n\nTarget around 100 words, crisply previewing (for introduction) or recapping (for conclusion) all of the sections of the report.\n\nUse markdown formatting.\n\nFor your introduction, create a compelling title and use the # header for the title.\n\nFor your introduction, use ## Introduction as the section header.\n\nFor your conclusion, use ## Conclusion as the section header.\n\nHere are the sections to reflect on for writing: {formatted_str_sections}\"\"\"\n\ndef write_introduction(state: ResearchGraphState):\n    # Full set of sections\n    sections = state[\"sections\"]\n    topic = state[\"topic\"]\n\n    # Concat all sections together\n    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n\n    # Summarize the sections into a final report\n\n    instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_str_sections)\n    intro = llm.invoke([instructions]+[HumanMessage(content=f\"Write the report introduction\")])\n    return {\"introduction\": intro.content}\n\ndef write_conclusion(state: ResearchGraphState):\n    # Full set of sections\n    sections = state[\"sections\"]\n    topic = state[\"topic\"]\n\n    # Concat all sections together\n    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n\n    # Summarize the sections into a final report\n\n    instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_str_sections)\n    conclusion = llm.invoke([instructions]+[HumanMessage(content=f\"Write the report conclusion\")])\n    return {\"conclusion\": conclusion.content}\n\ndef finalize_report(state: ResearchGraphState):\n    \"\"\" The is the \"reduce\" step where we gather all the sections, combine them, and reflect on them to write the intro/conclusion \"\"\"\n    # Save full final report\n    content = state[\"content\"]\n    if content.startswith(\"## Insights\"):\n        content = content.strip(\"## Insights\")\n    if \"## Sources\" in content:\n        try:\n            content, sources = content.split(\"\\n## Sources\\n\")\n        except:\n            sources = None\n    else:\n        sources = None\n\n    final_report = state[\"introduction\"] + \"\\n\\n---\\n\\n\" + content + \"\\n\\n---\\n\\n\" + state[\"conclusion\"]\n    if sources is not None:\n        final_report += \"\\n\\n## Sources\\n\" + sources\n    return {\"final_report\": final_report}\n\n# Add nodes and edges\nbuilder = StateGraph(ResearchGraphState)\nbuilder.add_node(\"create_analysts\", create_analysts)\nbuilder.add_node(\"human_feedback\", human_feedback)\nbuilder.add_node(\"conduct_interview\", interview_builder.compile())\nbuilder.add_node(\"write_report\",write_report)\nbuilder.add_node(\"write_introduction\",write_introduction)\nbuilder.add_node(\"write_conclusion\",write_conclusion)\nbuilder.add_node(\"finalize_report\",finalize_report)\n\n# Logic\nbuilder.add_edge(START, \"create_analysts\")\nbuilder.add_edge(\"create_analysts\", \"human_feedback\")\nbuilder.add_conditional_edges(\"human_feedback\", initiate_all_interviews, [\"create_analysts\", \"conduct_interview\"])\nbuilder.add_edge(\"conduct_interview\", \"write_report\")\nbuilder.add_edge(\"conduct_interview\", \"write_introduction\")\nbuilder.add_edge(\"conduct_interview\", \"write_conclusion\")\nbuilder.add_edge([\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\")\nbuilder.add_edge(\"finalize_report\", END)\n\n# Compile\nmemory = MemorySaver()\ngraph = builder.compile(interrupt_before=['human_feedback'], checkpointer=memory)\ndisplay(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n</pre> from langgraph.constants import Send  def initiate_all_interviews(state: ResearchGraphState):     \"\"\" This is the \"map\" step where we run each interview sub-graph using Send API \"\"\"      # Check if human feedback     human_analyst_feedback=state.get('human_analyst_feedback')     if human_analyst_feedback:         # Return to create_analysts         return \"create_analysts\"      # Otherwise kick off interviews in parallel via Send() API     else:         topic = state[\"topic\"]         return [Send(\"conduct_interview\", {\"analyst\": analyst,                                            \"messages\": [HumanMessage(                                                content=f\"So you said you were writing an article on {topic}?\"                                            )                                                        ]}) for analyst in state[\"analysts\"]]  report_writer_instructions = \"\"\"You are a technical writer creating a report on this overall topic:  {topic}  You have a team of analysts. Each analyst has done two things:  1. They conducted an interview with an expert on a specific sub-topic. 2. They write up their finding into a memo.  Your task:  1. You will be given a collection of memos from your analysts. 2. Think carefully about the insights from each memo. 3. Consolidate these into a crisp overall summary that ties together the central ideas from all of the memos. 4. Summarize the central points in each memo into a cohesive single narrative.  To format your report:  1. Use markdown formatting. 2. Include no pre-amble for the report. 3. Use no sub-heading. 4. Start your report with a single title header: ## Insights 5. Do not mention any analyst names in your report. 6. Preserve any citations in the memos, which will be annotated in brackets, for example [1] or [2]. 7. Create a final, consolidated list of sources and add to a Sources section with the `## Sources` header. 8. List your sources in order and do not repeat.  [1] Source 1 [2] Source 2  Here are the memos from your analysts to build your report from:  {context}\"\"\"  def write_report(state: ResearchGraphState):     # Full set of sections     sections = state[\"sections\"]     topic = state[\"topic\"]      # Concat all sections together     formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])      # Summarize the sections into a final report     system_message = report_writer_instructions.format(topic=topic, context=formatted_str_sections)     report = llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=f\"Write a report based upon these memos.\")])     return {\"content\": report.content}  intro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}  You will be given all of the sections of the report.  You job is to write a crisp and compelling introduction or conclusion section.  The user will instruct you whether to write the introduction or conclusion.  Include no pre-amble for either section.  Target around 100 words, crisply previewing (for introduction) or recapping (for conclusion) all of the sections of the report.  Use markdown formatting.  For your introduction, create a compelling title and use the # header for the title.  For your introduction, use ## Introduction as the section header.  For your conclusion, use ## Conclusion as the section header.  Here are the sections to reflect on for writing: {formatted_str_sections}\"\"\"  def write_introduction(state: ResearchGraphState):     # Full set of sections     sections = state[\"sections\"]     topic = state[\"topic\"]      # Concat all sections together     formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])      # Summarize the sections into a final report      instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_str_sections)     intro = llm.invoke([instructions]+[HumanMessage(content=f\"Write the report introduction\")])     return {\"introduction\": intro.content}  def write_conclusion(state: ResearchGraphState):     # Full set of sections     sections = state[\"sections\"]     topic = state[\"topic\"]      # Concat all sections together     formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])      # Summarize the sections into a final report      instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_str_sections)     conclusion = llm.invoke([instructions]+[HumanMessage(content=f\"Write the report conclusion\")])     return {\"conclusion\": conclusion.content}  def finalize_report(state: ResearchGraphState):     \"\"\" The is the \"reduce\" step where we gather all the sections, combine them, and reflect on them to write the intro/conclusion \"\"\"     # Save full final report     content = state[\"content\"]     if content.startswith(\"## Insights\"):         content = content.strip(\"## Insights\")     if \"## Sources\" in content:         try:             content, sources = content.split(\"\\n## Sources\\n\")         except:             sources = None     else:         sources = None      final_report = state[\"introduction\"] + \"\\n\\n---\\n\\n\" + content + \"\\n\\n---\\n\\n\" + state[\"conclusion\"]     if sources is not None:         final_report += \"\\n\\n## Sources\\n\" + sources     return {\"final_report\": final_report}  # Add nodes and edges builder = StateGraph(ResearchGraphState) builder.add_node(\"create_analysts\", create_analysts) builder.add_node(\"human_feedback\", human_feedback) builder.add_node(\"conduct_interview\", interview_builder.compile()) builder.add_node(\"write_report\",write_report) builder.add_node(\"write_introduction\",write_introduction) builder.add_node(\"write_conclusion\",write_conclusion) builder.add_node(\"finalize_report\",finalize_report)  # Logic builder.add_edge(START, \"create_analysts\") builder.add_edge(\"create_analysts\", \"human_feedback\") builder.add_conditional_edges(\"human_feedback\", initiate_all_interviews, [\"create_analysts\", \"conduct_interview\"]) builder.add_edge(\"conduct_interview\", \"write_report\") builder.add_edge(\"conduct_interview\", \"write_introduction\") builder.add_edge(\"conduct_interview\", \"write_conclusion\") builder.add_edge([\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\") builder.add_edge(\"finalize_report\", END)  # Compile memory = MemorySaver() graph = builder.compile(interrupt_before=['human_feedback'], checkpointer=memory) display(Image(graph.get_graph(xray=1).draw_mermaid_png())) <p>Let's ask an open-ended question about LangGraph.</p> In\u00a0[29]: Copied! <pre># Inputs\nmax_analysts = 3\ntopic = \"Llama3, Llama3.1, Llama3.2 \ubaa8\ub378\ub4e4\uc5d0 \ub300\ud55c \ubd84\uc11d\"\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Run the graph until the first interruption\nfor event in graph.stream({\"topic\":topic,\n                           \"max_analysts\":max_analysts},\n                          thread,\n                          stream_mode=\"values\"):\n\n    analysts = event.get('analysts', '')\n    if analysts:\n        for analyst in analysts:\n            print(f\"Name: {analyst.name}\")\n            print(f\"Affiliation: {analyst.affiliation}\")\n            print(f\"Role: {analyst.role}\")\n            print(f\"Description: {analyst.description}\")\n            print(\"-\" * 50)\n</pre> # Inputs max_analysts = 3 topic = \"Llama3, Llama3.1, Llama3.2 \ubaa8\ub378\ub4e4\uc5d0 \ub300\ud55c \ubd84\uc11d\" thread = {\"configurable\": {\"thread_id\": \"1\"}}  # Run the graph until the first interruption for event in graph.stream({\"topic\":topic,                            \"max_analysts\":max_analysts},                           thread,                           stream_mode=\"values\"):      analysts = event.get('analysts', '')     if analysts:         for analyst in analysts:             print(f\"Name: {analyst.name}\")             print(f\"Affiliation: {analyst.affiliation}\")             print(f\"Role: {analyst.role}\")             print(f\"Description: {analyst.description}\")             print(\"-\" * 50) <pre>Name: Dr. Emily Carter\nAffiliation: Tech Innovators Inc.\nRole: Technology Adoption Specialist\nDescription: Dr. Carter focuses on the strategic benefits and challenges of adopting new technologies in enterprise environments. She is particularly interested in how LangGraph can streamline operations and improve efficiency.\n--------------------------------------------------\nName: Raj Patel\nAffiliation: Data Security Solutions\nRole: Cybersecurity Analyst\nDescription: Raj Patel examines the security implications of adopting new frameworks like LangGraph. His primary concern is ensuring that the integration of LangGraph does not introduce vulnerabilities and that it enhances overall system security.\n--------------------------------------------------\nName: Dr. Maria Gonzalez\nAffiliation: AI Research Lab\nRole: AI Ethics Researcher\nDescription: Dr. Gonzalez explores the ethical considerations of implementing AI frameworks such as LangGraph. She is focused on ensuring that the adoption of LangGraph aligns with ethical standards and promotes fair and unbiased AI practices.\n--------------------------------------------------\nName: Dr. Emily Carter\nAffiliation: OpenAI\nRole: Model Performance Specialist\nDescription: Dr. Carter focuses on the performance metrics and benchmarks of AI models. She is particularly interested in the comparative analysis of Llama3, Llama3.1, and Llama3.2, examining their efficiency, accuracy, and scalability.\n--------------------------------------------------\nName: Prof. John Smith\nAffiliation: MIT\nRole: Ethics and Bias Analyst\nDescription: Prof. Smith's research centers on the ethical implications and potential biases in AI models. He will analyze Llama3, Llama3.1, and Llama3.2 for any inherent biases and ethical concerns, providing insights into their societal impacts.\n--------------------------------------------------\nName: Dr. Alice Wong\nAffiliation: Google Research\nRole: Innovation and Development Expert\nDescription: Dr. Wong is an expert in AI innovation and development. She will explore the technological advancements and novel features introduced in Llama3, Llama3.1, and Llama3.2, highlighting their contributions to the field of AI.\n--------------------------------------------------\n</pre> In\u00a0[30]: Copied! <pre># We now update the state as if we are the human_feedback node\ngraph.update_state(thread, {\"human_analyst_feedback\":\n                                \"Add in the CEO of gen ai native startup\"}, as_node=\"human_feedback\")\n</pre> # We now update the state as if we are the human_feedback node graph.update_state(thread, {\"human_analyst_feedback\":                                 \"Add in the CEO of gen ai native startup\"}, as_node=\"human_feedback\") Out[30]: <pre>{'configurable': {'thread_id': '1',\n  'checkpoint_ns': '',\n  'checkpoint_id': '1ef8083c-f643-6809-8005-779be80ccc9d'}}</pre> In\u00a0[31]: Copied! <pre># Check\nfor event in graph.stream(None, thread, stream_mode=\"values\"):\n    analysts = event.get('analysts', '')\n    if analysts:\n        for analyst in analysts:\n            print(f\"Name: {analyst.name}\")\n            print(f\"Affiliation: {analyst.affiliation}\")\n            print(f\"Role: {analyst.role}\")\n            print(f\"Description: {analyst.description}\")\n            print(\"-\" * 50)\n</pre> # Check for event in graph.stream(None, thread, stream_mode=\"values\"):     analysts = event.get('analysts', '')     if analysts:         for analyst in analysts:             print(f\"Name: {analyst.name}\")             print(f\"Affiliation: {analyst.affiliation}\")             print(f\"Role: {analyst.role}\")             print(f\"Description: {analyst.description}\")             print(\"-\" * 50) <pre>Name: Dr. Emily Carter\nAffiliation: OpenAI\nRole: Model Performance Specialist\nDescription: Dr. Carter focuses on the performance metrics and benchmarks of AI models. She is particularly interested in the comparative analysis of Llama3, Llama3.1, and Llama3.2, examining their efficiency, accuracy, and scalability.\n--------------------------------------------------\nName: Prof. John Smith\nAffiliation: MIT\nRole: Ethics and Bias Analyst\nDescription: Prof. Smith's research centers on the ethical implications and potential biases in AI models. He will analyze Llama3, Llama3.1, and Llama3.2 for any inherent biases and ethical concerns, providing insights into their societal impacts.\n--------------------------------------------------\nName: Dr. Alice Wong\nAffiliation: Google Research\nRole: Innovation and Development Expert\nDescription: Dr. Wong is an expert in AI innovation and development. She will explore the technological advancements and novel features introduced in Llama3, Llama3.1, and Llama3.2, highlighting their contributions to the field of AI.\n--------------------------------------------------\nName: Alex Kim\nAffiliation: Gen AI Native Startup\nRole: CEO\nDescription: Alex is the CEO of a startup that specializes in generative AI technologies. He is focused on the commercial applications and market potential of the Llama3 series models. His primary concern is how these models can be leveraged to create innovative products and services that can disrupt existing markets.\n--------------------------------------------------\nName: Dr. Emily Park\nAffiliation: AI Research Institute\nRole: Lead Research Scientist\nDescription: Dr. Park is a lead research scientist at a prominent AI research institute. Her focus is on the technical advancements and innovations in the Llama3 series models. She is particularly interested in the architectural improvements and performance metrics of Llama3, Llama3.1, and Llama3.2.\n--------------------------------------------------\nName: John Lee\nAffiliation: Tech Media\nRole: Tech Journalist\nDescription: John is a seasoned tech journalist who writes for a leading technology media outlet. His focus is on the broader implications of the Llama3 series models in the tech industry. He is interested in how these models compare to other state-of-the-art AI models and their potential impact on various sectors.\n--------------------------------------------------\n</pre> In\u00a0[32]: Copied! <pre># Confirm we are happy\ngraph.update_state(thread, {\"human_analyst_feedback\":\n                            None}, as_node=\"human_feedback\")\n</pre> # Confirm we are happy graph.update_state(thread, {\"human_analyst_feedback\":                             None}, as_node=\"human_feedback\") Out[32]: <pre>{'configurable': {'thread_id': '1',\n  'checkpoint_ns': '',\n  'checkpoint_id': '1ef8083d-d45d-6946-8007-0b1d6cf236ee'}}</pre> In\u00a0[33]: Copied! <pre># Continue\nfor event in graph.stream(None, thread, stream_mode=\"updates\"):\n    print(\"--Node--\")\n    node_name = next(iter(event.keys()))\n    print(node_name)\n</pre> # Continue for event in graph.stream(None, thread, stream_mode=\"updates\"):     print(\"--Node--\")     node_name = next(iter(event.keys()))     print(node_name) <pre>--Node--\nconduct_interview\n--Node--\nconduct_interview\n--Node--\nconduct_interview\n--Node--\nwrite_conclusion\n--Node--\nwrite_introduction\n--Node--\nwrite_report\n--Node--\nfinalize_report\n</pre> In\u00a0[34]: Copied! <pre>from IPython.display import Markdown\nfinal_state = graph.get_state(thread)\nreport = final_state.values.get('final_report')\nMarkdown(report)\n</pre> from IPython.display import Markdown final_state = graph.get_state(thread) report = final_state.values.get('final_report') Markdown(report) Out[34]: <p>\ud2b8\ub808\uc774\uc2a4\ub97c \ud55c\ubc88 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>https://smith.langchain.com/public/78cc6a69-a654-42b4-8865-e58fb5fd8a3c/r</p> <p></p> <p>3\ubc88\uc758 \uc778\ud130\ubdf0 (conduct_interview) \uc640 report \ub97c write \ud558\ub294 \ub178\ub4dc\ub4e4 \uae4c\uc9c0 \uac01\uac01 \uc798 \uc218\ud589\ub41c \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc791\uc131\ub41c \uacb0\uacfc\ubb3c report\uac00 \uc798 \ub9cc\ub4e4\uc5b4\uc84c\ub0d0. \uace0 \ud558\uba74 \uc560\ub9e4\ud569\ub2c8\ub2e4.</p> <ul> <li>llama3 \uacfc llama3.1 \uc5d0 \ub300\ud55c \uc88b\uc740 \uc815\ubcf4\uac00 \uc798 \ub2f4\uacbc\uc2b5\ub2c8\ub2e4.<ul> <li>\uc6d0 \ub17c\ubb38\uc778 herd of models \ub97c \ubcf4\uc9c0\ub294 \uc54a\uc558\uc9c0\ub9cc, \uadf8\ub798\ub3c4 \uc7ac\ud574\uc11d\ub41c \ube14\ub85c\uadf8\uae00\uc744 \ucc38\uc870\ud588\uc2b5\ub2c8\ub2e4.</li> <li>GQA \ub77c\ub358\uac00 405B \ubaa8\ub378 \ub4f1 \uae30\ud0c0 \uc8fc\uc694 \uc0ac\ud56d\ub4e4\uc740 \uc798 \ub2f4\uacbc\uc501\ub2c8\ub2e4.</li> </ul> </li> <li>\ubb38\uc81c\ub294 llama3.2 \uc5d0 \ub300\ud55c \uc815\ubcf4\uc785\ub2c8\ub2e4.<ul> <li>\uc544\uc8fc \uc791\uc740 \ubaa8\ub378\uc774 \ud3ec\ud568\ub418\uc5c8\ub2e4\ub294\uc810, cross-attention\uc744 vision input \ucd94\uac00\uc5d0 \ud65c\uc6a9\ud588\ub2e4\ub294 \uc810\uc774 \uc804\ud600 \ub2f4\uae30\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.</li> </ul> </li> </ul> <p>\ubb38\uc81c \uc6d0\uc778\uc744 \ubd84\uc11d\ud574\ubcf4\uc790\uba74, \uc81c\uac00 \uc774 \ucf54\ub4dc\ub97c \uc218\ud589\ud558\ub294 \ud604 \uc2dc\uc810 (24.10.02) \uae30\uc900 llama3.2 \ub294 \ub098\uc628\uc9c0 7\uc77c\ub41c \uc544\uc8fc \uc0c8\ub85c\uc6b4 \ubaa8\ub378\uc785\ub2c8\ub2e4. \ud574\ub2f9 \ubaa8\ub378\uc744 \uc7ac\ud574\uc11d\ud55c popular \ud55c \uae00\ub4e4\uc778 \uc6f9\uc0c1\uc5d0 \uc874\uc7ac\ud558\uc9c0 \uc54a\uc544\uc11c \uac80\uc0c9\uc774 \uc548 \ub41c \uac83\uc73c\ub85c \ucd94\uc815\uc774 \ub429\ub2c8\ub2e4.... \u315c\u3160 \uc774\ub97c \ud574\uacb0\ud574\uc57c\uaca0\uc8e0?</p> <p></p> <p>\uadf8\ub7ac\ub354\ub2c8 Llama3.2 \uc5d0 \ub300\ud574 \uacb0\uacfc\ub97c \ub0b4 \ubc49\uc5c8\uace0, \uacf5\uc2dd meta\uc758 Llama3.2 \uc790\ub8cc\ub97c \uc798 \ucc38\uc870\ud588\uc2b5\ub2c8\ub2e4.</p> <p>\uadf8\ub7ec\ub098, Llama 3 \uacfc 3.2 \ub97c \uc81c\ub300\ub85c \uad6c\ubd84\ud558\uc9c0 \ubabb\ud574\uc11c \ub0b4\uc6a9\uc774 \uc5b4\uc9c0\ub7fd\uace0 \ud2c0\ub9b0 \uc18c\ub9ac\uac00 \ub9ce\uc544\uc694. \ud0c8\ub77d\uc785\ub2c8\ub2e4.</p> <p>search \ub2e8\uacc4\uc5d0 human in the loop \uc744 \ub123\uc5b4\uc11c \ud544\uc694 \uc5c6\ub294 \uc790\ub8cc\ub4e4\uc744 \ucc98\ub0b4\uc8fc\uac70\ub098 \ud558\ub294 \ubc29\uc548\uc744 \ucd94\uac00\ud574\uc57c\ud560 \uac83 \uac19\uc2b5\ub2c8\ub2e4.</p> <p>\uacb0\uacfc \ucc38\uc870 \ub9c1\ud06c\ub294 \uc5ec\uae30 \uc788\uc2b5\ub2c8\ub2e4.</p> <ul> <li>https://smith.langchain.com/public/be152826-ae78-4784-a622-97b2c0e3bcdb/r</li> </ul> <p>\uacb0\ub860</p> <p>\uc88b\uc740 research\ub97c \uad6c\ud604\ud560 \uc218 \uc788\uc5c8\uc73c\ub098, \uc644\ubcbd\ud558\uc9c0 \uc54a\ub2e4. \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 \ub2e4\uc74c \ubc29\ubc95\uc744 \ucc3e\uc544\uac00 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>\ub2e4\uc74c \uc8fc\uc81c\ub294 Reflection \uc785\ub2c8\ub2e4.</p>"},{"location":"tutorial/3_MultiAgent_research_assistant/#research-assistant","title":"Research Assistant\u00b6","text":""},{"location":"tutorial/3_MultiAgent_research_assistant/#review","title":"Review\u00b6","text":"<p>\uc544\ub798\uc640 \uac19\uc774 LangGraph \uc758 \uc8fc\uc694 \ucee8\uc149\uc5d0 \ub300\ud574\uc11c \ubc30\uc6e0\uc2b5\ub2c8\ub2e4.</p> <ul> <li>Memory</li> <li>Human-in-the-loop</li> <li>Controllability</li> </ul> <p>\uc774\uc81c 3\uac00\uc9c0\ub97c \ud569\uccd0\uc11c, \uac00\uc7a5 \uc720\uba85\ud55c AI \uc0ac\uc6a9 \ubc95\uc778 \"research automation\"\uc744 \uad6c\ud604\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4</p> <p>\ub9ac\uc11c\uce58\ub294 \uc560\ub110\ub9ac\uc2a4\ud2b8 \uc785\uc7a5\uc5d0\uc11c \ub178\ub3d9\uc9d1\uc57d\uc801\uc778 \uc77c\uc785\ub2c8\ub2e4. AI \ub85c \uc774\ub97c \ub3c4\uc640\uc918 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \ub9ac\uc11c\uce58 \uc791\uc5c5\uc744 \uc704\ud574 \ud574\uacb0\ud574\uc57c\ud560 \ubb38\uc81c\uac00 \uc788\uc2b5\ub2c8\ub2e4. \ubcf4\ud1b5 LLM \uc758 output\uc740 \uc6b0\ub9ac\uc758 \ud604\uc2e4 \ubb38\uc81c\uc640 align \uc774 \uc798 \uc548\ub418\uc5b4 \uc788\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc8e0. \ubed4\ud55c \uc18c\ub9ac\ub97c \ud55c\ub2e4\uac70\ub098, \uc790\ub8cc\uc758 \ucd9c\ucc98\ub97c \uc815\ud655\ud558\uac8c \uc798 \ucc3e\uace0 \ub300\ub2f5\ud558\uc9c0 \ubabb \ud55c\ub2e4\uac70\ub098...</p> <p>research and report generation \ubb38\uc11c\ub97c \ucc38\uc870\ud558\uba74, \uaf64 \uc88b\uc740 workflow \uc608\uc2dc\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ubc29\ubc95\ub860\uc744 LangGraph\ub97c \uc774\uc6a9\ud574\uc11c \uad6c\ud604\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/3_MultiAgent_research_assistant/#goal","title":"Goal\u00b6","text":"<p>\ubaa9\ud45c\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. \ub9ac\uc11c\uce58 \uc791\uc5c5\uc744 \ud574\uc8fc\ub294 \uac00\ubcbc\uc6b4 multi-agent \uc2dc\uc2a4\ud15c\uc744 \uad6c\ud604\ud558\ub294 \uac83 \uc785\ub2c8\ub2e4.</p> <p><code>Source Selection</code></p> <ul> <li>\uc720\uc800\ub294 \uc778\ud48b \uc18c\uc2a4\ub97c \uace8\ub77c\uc90d\ub2c8\ub2e4.</li> </ul> <p><code>Planning</code></p> <ul> <li>\uc0ac\uc6a9\uc790\ub294 \ud1a0\ud53d\uc744 \uc9c0\uc815\ud574\uc8fc\uace0, \uc2dc\uc2a4\ud15c\uc740 AI \uc560\ub110\ub9ac\uc2a4\ud2b8 \"\ud300\" \uc744 \ub9cc\ub4e7\ub2c8\ub2e4. \uc560\ub110\ub9ac\uc2a4\ud2b8\ub4e4\uc740 \uc11c\ube0c\ud1a0\ud53d\ub4e4\uc744 \ub9e1\uc544\uc11c \ubd84\uc11d\ud569\ub2c8\ub2e4.</li> <li><code>Human-in-the-loop</code> \ub2e8\uacc4\uc5d0\uc11c \uc11c\ube0c\ud1a0\ud53d\uc744 \uc0ac\ub78c\uc774 \uc9c0\uc815\ud574 \uc904 \uc608\uc815\uc785\ub2c8\ub2e4. \uc77c\uc744 \ucabc\uac1c\ub294 \uac83\uc740 \uc0ac\ub78c\uc774 \uc9c1\uc811 \ud558\ub294 \uac83\uc774 \ud6a8\uacfc\uc801\uc774\uad70\uc694.</li> </ul> <p><code>LLM Utilization</code></p> <ul> <li>\uac01\uac01\uc758 \uc560\ub110\ub9ac\uc2a4\ud2b8\ub4e4\uc740 \ub610 \ub2e4\ub978 \uc804\ubb38\uac00 AI \ub97c in-depth \uc778\ud130\ubdf0 \ud569\ub2c8\ub2e4. \uc774\ub54c \uc120\ud0dd\ub41c \uc18c\uc2a4\ub97c \ud65c\uc6a9\ud569\ub2c8\ub2e4.</li> <li>\uc778\ud130\ubdf0\ub294 multi-turn conversation \uc785\ub2c8\ub2e4. detailed insights \ub97c \ubf51\uc544 \ub0b4\uae30 \uc704\ud568\uc785\ub2c8\ub2e4. STORM \ud398\uc774\ud37c\ub97c \ucc38\uc870\ud558\uc2dc\uba74 \ub3c4\uc6c0\uc774 \ub420 \uac83\uc785\ub2c8\ub2e4.</li> <li>\uba40\ud2f0\ud134 \ub300\ud654\ub85c \uc774\ub904\uc9c4 \uc778\ud130\ubdf0\ub294 <code>sub-graphs</code> \uac00 \ub420 \uac83\uc785\ub2c8\ub2e4. \uac01 \uc778\ud130\ubdf0\uac00 state\ub97c \ub530\ub85c \uac00\uc9c0\uace0 \uc788\uc5b4\uc57c\uaca0\uc8e0.</li> </ul> <p><code>Research Process</code></p> <ul> <li>\uc804\ubb38\uac00\ub4e4\uc740 \uc560\ub110\ub9ac\uc2a4\ud2b8\uc758 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\uae30 \uc704\ud574 \uc815\ubcf4\ub97c \uc218\uc9d1\ud560\ud150\ub370, \uc774\ub294 <code>parallel</code> \ud558\uac8c \uc774\ub904\uc9d1\ub2c8\ub2e4.</li> <li>\ubaa8\ub4e0 \uc778\ud130\ubdf0\ub4e4\uc740 \ub3d9\uc2dc\uc5d0 \uc77c\uc5b4\ub098\uace0 \ud569\uccd0\uc9c0\uaca0\uc8e0. <code>map-reduce</code> \uc8e0.</li> </ul> <p><code>Output Format</code></p> <ul> <li>\uac01 \uc778\ud130\ubdf0\uc5d0\uc11c \ucde8\ud569\ub41c \uc778\uc0ac\uc774\ud2b8\ub4e4\uc740 \ucd5c\uc885 \ubcf4\uace0\uc11c\ub85c \uac00\uacf5\ub429\ub2c8\ub2e4.</li> <li>Output \ud3ec\ub9f7\uc744 \uc798 \ub9de\ucdb0\uc8fc\uae30 \uc704\ud574\uc11c \ud504\ub86c\ud504\ud305\uc744 \ud574\uc90d\ub2c8\ub2e4.</li> </ul> <p></p>"},{"location":"tutorial/3_MultiAgent_research_assistant/#setup","title":"Setup\u00b6","text":""},{"location":"tutorial/3_MultiAgent_research_assistant/#generate-analysts-human-in-the-loop","title":"Generate Analysts: Human-In-The-Loop\u00b6","text":"<p>\uc560\ub110\ub9ac\uc2a4\ud2b8\ub4e4\uc744 \uc0dd\uc131\ud558\uace0, Human in the loop \uc73c\ub85c \uba48\ucda5\ub2c8\ub2e4. \uc9c1\uc811 \uc0dd\uc131\ub41c \uc560\ub110\ub9ac\uc2a4\ud2b8\ub4e4\uc744 \ub9ac\ubdf0\ud569\ub2c8\ub2e4.</p>"},{"location":"tutorial/3_MultiAgent_research_assistant/","title":"\uc778\ud130\ubdf0 \uc9c4\ud589\ud558\uae30\u00b6","text":""},{"location":"tutorial/3_MultiAgent_research_assistant/","title":"\uc9c8\ubb38 \uc0dd\uc131\u00b6","text":"<p>\uc560\ub110\ub9ac\uc2a4\ud2b8\ub4e4\uc774 \uc804\ubb38\uac00 agent \uc5d0\uac8c \uc9c8\ubb38\uc744 \ud574\uc57c\ud569\ub2c8\ub2e4</p>"},{"location":"tutorial/3_MultiAgent_research_assistant/#parallel","title":"\ub2f5\ubcc0 \uc0dd\uc131: Parallel \ud558\uac8c!\u00b6","text":"<p>\uc804\ubb38\uac00\ub4e4\uc740 \ub2e4\uc591\ud55c \uc18c\uc2a4\uc5d0\uc11c \ub2f5\ubcc0\uc744 \uc704\ud55c \uc815\ubcf4\ub4e4\uc744 \uc218\uc9d1\ud569\ub2c8\ub2e4.</p> <ul> <li>\uc6f9 \ud06c\ub864\ub9c1 (\uac80\uc0c9\uc774 \uc544\ub2c8\ub77c \ud2b9\uc815 \ud398\uc774\uc9c0 \ud30c\uc2f1) <code>WebBaseLoader</code></li> <li>\ubbf8\ub9ac \uc815\ub9ac\ub41c \ubb38\uc11c\ub4e4 RAG</li> <li>\uc6f9 \uc11c\uce58</li> <li>\uc704\ud0a4\ud53c\ub514\uc544 \uc11c\uce58</li> </ul> <p>Tavily \uac19\uc740 \ub2e4\ub978 \uc6f9 \uc11c\uce6d \ub3c4\uad6c\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc8e0.</p>"},{"location":"tutorial/3_MultiAgent_research_assistant/#evaluating-the-technical-advancements-and-performance-metrics-of-llama3-series-models","title":"Evaluating the Technical Advancements and Performance Metrics of Llama3 Series Models\u00b6","text":""},{"location":"tutorial/3_MultiAgent_research_assistant/#summary","title":"Summary\u00b6","text":"<p>The Llama3 series, developed by Meta AI, represents a significant advancement in the field of large language models (LLMs). Positioned as a competitor to OpenAI's GPT series, Llama3 models are designed to excel in various natural language processing (NLP) tasks such as text generation, conversation, and summarization [1]. This report delves into the capabilities, limitations, and potential improvements of the Llama3 series, with a particular focus on the Llama3.1 model.</p> <p>Llama3.1, the latest iteration, boasts an impressive 405 billion parameters, marking a substantial leap from its predecessors [2]. This model is designed to be versatile and powerful, capable of handling a wide range of tasks with improved performance metrics. Notably, Llama3.1 outperforms Llama3 in several benchmarks, including math tasks where it shows a 14% improvement [3]. This enhancement is attributed to its larger context window and better NLP capabilities, making it a more suitable choice for tasks requiring extensive context [4].</p> <p>One of the most surprising insights is the performance of the smaller Llama3 8B model, which, despite being ten times smaller than the Llama2 70B, produces similar results [2]. This indicates that Meta AI has made significant strides in optimizing model efficiency without compromising performance.</p> <p>Key improvements in Llama3.1 include faster performance, broader versatility, and enhanced ease of use. These advancements make it a worthy upgrade for those looking to leverage the power of AI in their projects [5]. However, it's important to note that while Llama3.1 offers numerous benefits, Llama3 still holds its own, particularly in specific use cases where its capabilities are sufficient [5].</p> <p>In summary, the Llama3 series, particularly the Llama3.1 model, showcases Meta AI's commitment to advancing the field of artificial intelligence. The models' capabilities, combined with their performance metrics, make them strong contenders in the competitive landscape of LLMs.</p>"},{"location":"tutorial/3_MultiAgent_research_assistant/#sources","title":"Sources\u00b6","text":"<p>[1] https://kili-technology.com/large-language-models-llms/llama-3-guide-everything-you-need-to-know-about-meta-s-new-model-and-its-data [2] https://medium.com/@soumava.dey.aig/decoding-llama-3-a-quick-overview-of-the-model-7e69abcdbe6a [3] https://medium.com/@getanakin/llama3-vs-llama-comprehensive-review-of-benchmarks-and-pricing-1767fd1bd04a [4] https://medium.com/@kagglepro/llama-3-vs-llama-3-1-which-is-the-better-fit-for-your-ai-projects-a57a052b89b1 [5] http://anakin.ai/blog/llama3-1-vs-llama-3/</p>"},{"location":"tutorial/3_MultiAgent_research_assistant/#map-reduce","title":"\uc778\ud130\ubdf0\ub4e4 \ucde8\ud569, Map-Reduce\u00b6","text":"<p>Map \ub2e8\uacc4\uc5d0\uc11c \uc778\ud130\ubdf0\ub4e4\uc744 <code>Send()</code> API \ub85c \ucb49 \ubcf4\ub0c8\uc2b5\ub2c8\ub2e4.</p> <p>\uc774\uc81c Reduce \ub2e8\uacc4\ub85c \uc778\ud130\ubdf0\ub4e4\uc744 \ucde8\ud569\ud574\uc11c \ubcf4\uace0\uc11c\ub85c \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/3_MultiAgent_research_assistant/#finalize","title":"Finalize\u00b6","text":"<p>\ub9c8\ubb34\ub9ac\ub97c \uc704\ud574 \ubcf4\uace0\uc11c\uc758 \uc778\ud2b8\ub85c, \uacb0\ub860\uc744 \uc791\uc131\ud569\ub2c8\ub2e4.</p>"},{"location":"tutorial/3_MultiAgent_research_assistant/#leveraging-llama-3-for-market-disruption-insights-for-generative-ai-applications","title":"Leveraging Llama 3 for Market Disruption: Insights for Generative AI Applications\u00b6","text":""},{"location":"tutorial/3_MultiAgent_research_assistant/#introduction","title":"Introduction\u00b6","text":"<p>The Llama 3 series, developed by Meta, marks a significant leap in generative AI, offering enhanced performance and new capabilities for commercial applications. This report explores how these models can be harnessed to create innovative products and services that disrupt existing markets. We delve into the architectural advancements and performance metrics of Llama 3, Llama 3.1, and Llama 3.2, highlighting their superior language processing abilities. Additionally, we examine the broader implications of these models in the tech industry, including their state-of-the-art capabilities, multilingual support, and optimization for edge devices.</p> <p>The Llama 3 series, developed by Meta, marks a significant leap in the field of generative AI, offering enhanced performance and new capabilities that can be leveraged for commercial applications. These models, including Llama 3, Llama 3.1, and Llama 3.2, are designed to rival top AI models in various capabilities, such as general knowledge, steerability, math, tool use, and multilingual translation.</p> <p>Llama 3 is a text-generation AI model similar to OpenAI's GPT and Anthropic's Claude models, generating text responses based on given prompts with notable improvements in contextual understanding and logical reasoning [1]. The series includes models with 8 billion and 70 billion parameters, designed to enhance processing power, versatility, and accessibility [2]. Key improvements in Llama 3 include the use of a tokenizer with a vocabulary of 128K tokens, which encodes language more efficiently, leading to substantially improved model performance. Additionally, the adoption of grouped query attention (GQA) across both the 8B and 70B sizes has improved inference efficiency [3].</p> <p>One of the most groundbreaking aspects of Llama 3 is its open-source nature, which democratizes access to advanced AI capabilities and fosters innovation. The release of the 405B model, Llama 3.1, further pushes the boundaries by offering state-of-the-art capabilities in general knowledge, steerability, math, tool use, and multilingual translation [4]. This model is poised to supercharge innovation, providing unprecedented opportunities for growth and exploration [5].</p> <p>Llama 3 introduces several key improvements over its predecessor, Llama 2, starting with its architecture. While maintaining a relatively standard decoder-only transformer architecture, Llama 3 incorporates a tokenizer with a vocabulary of 128K tokens, which encodes language more efficiently and leads to substantially improved model performance [1]. The training data for Llama 3 has also seen a significant increase, with a pretraining corpus expanded by 650%, providing a much richer dataset for the model to learn from [2]. This expansion in training data contributes to Llama 3's enhanced understanding and generation of language.</p> <p>In real-world applications, Llama 3 has demonstrated notable improvements in the speed and accuracy of language tasks. These enhancements are not just theoretical but have been observed in practical scenarios, showcasing Llama 3's superior performance in handling complex language tasks [3]. However, it is worth noting that despite these advancements, Llama 3's performance in competitive benchmarks has shown variability, with win rates dropping from a high 50% to a low 40% in some cases [4]. Llama 3 also excels in multi-step tasks due to refined post-training processes that minimize false rejections, improve response alignment, and generate more diverse answers, making it more robust and versatile in handling a variety of language tasks compared to Llama 2 [5].</p> <p>The Llama 3 series models are designed to support a wide range of languages and tasks, including coding, reasoning, and tool usage, making them highly versatile and applicable across various industries, from healthcare to finance [2][3]. The Llama 3.2 models, with 1B and 3B parameters, are optimized for on-device use cases, supporting a context length of 128K tokens, making them ideal for tasks such as summarization, instruction following, and rewriting on mobile and edge devices. They are also optimized for Qualcomm and MediaTek hardware, ensuring broad compatibility and efficient performance [3].</p> <p>The development of LLMs has seen significant advancements since the introduction of the transformer architecture in 2017. The Llama 3 series builds on this foundation, offering models that are not only powerful but also openly available, which contrasts with the more restricted access of models like GPT-3 and GPT-4 [4]. By fine-tuning the Llama 3 models on data specific to particular industries, it is possible to create custom AI solutions that address unique challenges. For instance, in healthcare, these models can assist in medical image analysis, while in finance, they can be used for predictive analytics and risk assessment [5][6].</p> <p>Meta has emphasized the importance of safety in the development of the Llama 3.1 models. This focus on safety is crucial as these models are deployed in various sensitive applications, ensuring that they operate within ethical guidelines and minimize potential risks [5]. The Llama 3 series models are poised to have a significant impact on the tech industry, offering advanced capabilities and broad applicability across multiple sectors. Their open availability and optimization for edge devices further enhance their potential to drive innovation and address complex challenges in various domains.</p>"},{"location":"tutorial/3_MultiAgent_research_assistant/#conclusion","title":"Conclusion\u00b6","text":"<p>The Llama 3 series, developed by Meta, marks a significant leap in generative AI, offering enhanced performance and new capabilities that can disrupt existing markets. This report has explored the potential of Llama 3 for market disruption, highlighting its open-source nature and advanced features. We delved into the architectural improvements and performance metrics of Llama3, Llama3.1, and Llama3.2, noting substantial advancements over Llama2. Additionally, we examined the broader implications of these models in the tech industry, emphasizing their state-of-the-art capabilities, multilingual support, and optimization for edge devices. The Llama 3 series stands poised to drive innovation and address complex challenges across various sectors.</p>"},{"location":"tutorial/3_MultiAgent_research_assistant/#sources","title":"Sources\u00b6","text":"<p>[1] https://www.datacamp.com/blog/meta-announces-llama-3-the-next-generation-of-open-source-llms [2] https://techcrunch.com/2024/04/18/meta-releases-llama-3-claims-its-among-the-best-open-models-available/ [3] https://ai.meta.com/blog/meta-llama-3/ [4] https://builtin.com/articles/llama-3 [5] https://ai.meta.com/blog/meta-llama-3-1/ [6] https://www.unite.ai/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/ [7] https://kanerika.com/blogs/llama-3-vs-llama-2/ [8] https://www.linkedin.com/pulse/comprehensive-technical-analysis-llama-3-comparison-2-ibad-rehman-kw8pe [9] https://lmsys.org/blog/2024-05-08-llama3/ [10] https://blog.monsterapi.ai/blogs/what-is-llama-3-and-how-it-differs-from-llama-2/ [11] https://arxiv.org/abs/2407.21783 [12] https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/ [13] https://en.wikipedia.org/wiki/Large_language_model [14] https://www.datacamp.com/blog/llama-3-1-405b-meta-ai [15] https://www.indikaai.com/blog/the-llama-3-herd-of-models-a-revolution-in-multilingual-ai-and-beyond</p>"},{"location":"tutorial/3_MultiAgent_research_assistant/#langgraph-studio","title":"LangGraph Studio \ub85c \uc2dc\ub3c4\ud558\uae30\u00b6","text":"<p>\uc704 \uadf8\ub798\ud504 \ucf54\ub4dc\ub97c py \ucf54\ub4dc\ub85c \ub9cc\ub4e4\uc5b4\uc11c studio \ud504\ub85c\uc81d\ud2b8\ub85c \ub123\uc5b4\ub450\uba74 \uc544\ub798\uc640 \uac19\uc774 UI \uc640 \ud568\uaed8 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>py code \ub294 https://github.com/langchain-ai/langchain-academy/tree/main/module-4/studio \ub97c \ucc38\uc870\ud558\uc2dc\uace0\uc694.</p> <p>Llama3.2 \uc5d0 \ub300\ud55c \uc790\ub8cc\uac00 \uc544\uc26c\uc6cc\uc11c \ub2e4\uc2dc \uc870\uc0ac\ub97c \uc2dc\ucf1c\ubd24\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/4_langgraph_adaptive_rag/","title":"Adaptive RAG","text":"In\u00a0[2]: Copied! <pre>%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph  tavily-python\n</pre> %%capture --no-stderr ! pip install -U langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph  tavily-python In\u00a0[3]: Copied! <pre>import getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n</pre> import getpass import os  def _set_env(var: str):     if not os.environ.get(var):         os.environ[var] = getpass.getpass(f\"{var}: \")   _set_env(\"OPENAI_API_KEY\") _set_env(\"TAVILY_API_KEY\") <pre>OPENAI_API_KEY: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nTAVILY_API_KEY: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> <p>LangSmith \ub3c4 \uc14b\uc5c5 \ud558\uaca0\uc2b5\ub2c8\ub2e4. \ucd94\uc801\uc744 \uc704\ud574\uc11c \ud558\ub294 \uac83\uc774\uae30 \ub584\ubb38\uc5d0, \uc548\ud558\uc154\ub3c4 \ubb34\ubc29\ud569\ub2c8\ub2e4.</p> In\u00a0[4]: Copied! <pre>_set_env(\"LANGCHAIN_API_KEY\")\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"langgraph_adaptive_rag\"\n</pre> _set_env(\"LANGCHAIN_API_KEY\") os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" os.environ[\"LANGCHAIN_PROJECT\"] = \"langgraph_adaptive_rag\" <pre>LANGCHAIN_API_KEY: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[5]: Copied! <pre>### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n\"https://jonhpark7966.github.io/LangSmith_Course\",\n\"https://jonhpark7966.github.io/LangSmith_Course/FAQ\",\n\"https://jonhpark7966.github.io/LangSmith_Course/intros/Evaluation_with_dataset\",\n\"https://jonhpark7966.github.io/LangSmith_Course/intros/LLMOps\",\n\"https://jonhpark7966.github.io/LangSmith_Course/intros/LangSmith\",\n\"https://jonhpark7966.github.io/LangSmith_Course/intros/Pricing\",\n\"https://jonhpark7966.github.io/LangSmith_Course/intros/Tracing_concept\",\n\"https://jonhpark7966.github.io/LangSmith_Course/practice/1_LangSmith_Tracing\",\n\"https://jonhpark7966.github.io/LangSmith_Course/practice/2_LangSmith_Evaluation\",\n\"https://jonhpark7966.github.io/LangSmith_Course/tutorial/Datasets\",\n\"https://jonhpark7966.github.io/LangSmith_Course/tutorial/Evaluation\",\n\"https://jonhpark7966.github.io/LangSmith_Course/tutorial/Setup\",\n\"https://jonhpark7966.github.io/LangSmith_Course/tutorial/Tracing_1\",\n\"https://jonhpark7966.github.io/LangSmith_Course/tutorial/Tracing_2\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n</pre> ### Build Index  from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_community.document_loaders import WebBaseLoader from langchain_community.vectorstores import Chroma from langchain_openai import OpenAIEmbeddings   # Set embeddings embd = OpenAIEmbeddings()  # Docs to index urls = [ \"https://jonhpark7966.github.io/LangSmith_Course\", \"https://jonhpark7966.github.io/LangSmith_Course/FAQ\", \"https://jonhpark7966.github.io/LangSmith_Course/intros/Evaluation_with_dataset\", \"https://jonhpark7966.github.io/LangSmith_Course/intros/LLMOps\", \"https://jonhpark7966.github.io/LangSmith_Course/intros/LangSmith\", \"https://jonhpark7966.github.io/LangSmith_Course/intros/Pricing\", \"https://jonhpark7966.github.io/LangSmith_Course/intros/Tracing_concept\", \"https://jonhpark7966.github.io/LangSmith_Course/practice/1_LangSmith_Tracing\", \"https://jonhpark7966.github.io/LangSmith_Course/practice/2_LangSmith_Evaluation\", \"https://jonhpark7966.github.io/LangSmith_Course/tutorial/Datasets\", \"https://jonhpark7966.github.io/LangSmith_Course/tutorial/Evaluation\", \"https://jonhpark7966.github.io/LangSmith_Course/tutorial/Setup\", \"https://jonhpark7966.github.io/LangSmith_Course/tutorial/Tracing_1\", \"https://jonhpark7966.github.io/LangSmith_Course/tutorial/Tracing_2\", ]  # Load docs = [WebBaseLoader(url).load() for url in urls] docs_list = [item for sublist in docs for item in sublist]  # Split text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(     chunk_size=500, chunk_overlap=0 ) doc_splits = text_splitter.split_documents(docs_list)  # Add to vectorstore vectorstore = Chroma.from_documents(     documents=doc_splits,     collection_name=\"rag-chroma\",     embedding=embd, ) retriever = vectorstore.as_retriever() <pre>WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n</pre> In\u00a0[14]: Copied! <pre>### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_openai import ChatOpenAI\n\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to Langsmith, LLM Operations.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"\ub3c4\uc6d0\uacb0\uc758\uac00 \ubb50\uc57c?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"LangSmith\uc5d0 \ub300\ud574 \uc124\uba85\ud574\uc918?\"}))\n</pre> ### Router  from typing import Literal  from langchain_core.prompts import ChatPromptTemplate from langchain_core.pydantic_v1 import BaseModel, Field from langchain_openai import ChatOpenAI   # Data model class RouteQuery(BaseModel):     \"\"\"Route a user query to the most relevant datasource.\"\"\"      datasource: Literal[\"vectorstore\", \"web_search\"] = Field(         ...,         description=\"Given a user question choose to route it to web search or a vectorstore.\",     )   # LLM with function call llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) structured_llm_router = llm.with_structured_output(RouteQuery)  # Prompt system = \"\"\"You are an expert at routing a user question to a vectorstore or web search. The vectorstore contains documents related to Langsmith, LLM Operations. Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\" route_prompt = ChatPromptTemplate.from_messages(     [         (\"system\", system),         (\"human\", \"{question}\"),     ] )  question_router = route_prompt | structured_llm_router print(     question_router.invoke(         {\"question\": \"\ub3c4\uc6d0\uacb0\uc758\uac00 \ubb50\uc57c?\"}     ) ) print(question_router.invoke({\"question\": \"LangSmith\uc5d0 \ub300\ud574 \uc124\uba85\ud574\uc918?\"})) <pre>datasource='web_search'\ndatasource='vectorstore'\n</pre> In\u00a0[10]: Copied! <pre>### Retrieval Grader\n\n\n# Data model\nclass GradeDocuments(BaseModel):\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n\n    binary_score: str = Field(\n        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n    )\n\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nstructured_llm_grader = llm.with_structured_output(GradeDocuments)\n\n# Prompt\nsystem = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\ngrade_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n    ]\n)\n\nretrieval_grader = grade_prompt | structured_llm_grader\nquestion = \"\uc62c\ud574 \ud55c\uad6d \ud504\ub85c\uc57c\uad6c \uc6b0\uc2b9\ud300\uc740?\" #\"\ub7ad\uc2a4\ubbf8\uc2a4\"\ndocs = retriever.get_relevant_documents(question)\ndoc_txt = docs[1].page_content\nprint(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n</pre> ### Retrieval Grader   # Data model class GradeDocuments(BaseModel):     \"\"\"Binary score for relevance check on retrieved documents.\"\"\"      binary_score: str = Field(         description=\"Documents are relevant to the question, 'yes' or 'no'\"     )   # LLM with function call llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) structured_llm_grader = llm.with_structured_output(GradeDocuments)  # Prompt system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n     If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n     It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n     Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\" grade_prompt = ChatPromptTemplate.from_messages(     [         (\"system\", system),         (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),     ] )  retrieval_grader = grade_prompt | structured_llm_grader question = \"\uc62c\ud574 \ud55c\uad6d \ud504\ub85c\uc57c\uad6c \uc6b0\uc2b9\ud300\uc740?\" #\"\ub7ad\uc2a4\ubbf8\uc2a4\" docs = retriever.get_relevant_documents(question) doc_txt = docs[1].page_content print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})) <pre>binary_score='no'\n</pre> In\u00a0[15]: Copied! <pre>### Generate\n\nfrom langchain import hub\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Prompt\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n# LLM\nllm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n\n\n# Post-processing\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\n# Chain\nrag_chain = prompt | llm | StrOutputParser()\n\n# Run\ngeneration = rag_chain.invoke({\"context\": docs, \"question\": question})\nprint(generation)\n</pre> ### Generate  from langchain import hub from langchain_core.output_parsers import StrOutputParser  # Prompt prompt = hub.pull(\"rlm/rag-prompt\")  # LLM llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)   # Post-processing def format_docs(docs):     return \"\\n\\n\".join(doc.page_content for doc in docs)   # Chain rag_chain = prompt | llm | StrOutputParser()  # Run generation = rag_chain.invoke({\"context\": docs, \"question\": question}) print(generation) <pre>\ubaa8\ub974\uaca0\uc2b5\ub2c8\ub2e4.\n</pre> In\u00a0[16]: Copied! <pre>### Hallucination Grader\n\n\n# Data model\nclass GradeHallucinations(BaseModel):\n    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n\n    binary_score: str = Field(\n        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n    )\n\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nstructured_llm_grader = llm.with_structured_output(GradeHallucinations)\n\n# Prompt\nsystem = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\nhallucination_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n    ]\n)\n\nhallucination_grader = hallucination_prompt | structured_llm_grader\nhallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n</pre> ### Hallucination Grader   # Data model class GradeHallucinations(BaseModel):     \"\"\"Binary score for hallucination present in generation answer.\"\"\"      binary_score: str = Field(         description=\"Answer is grounded in the facts, 'yes' or 'no'\"     )   # LLM with function call llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) structured_llm_grader = llm.with_structured_output(GradeHallucinations)  # Prompt system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n      Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\" hallucination_prompt = ChatPromptTemplate.from_messages(     [         (\"system\", system),         (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),     ] )  hallucination_grader = hallucination_prompt | structured_llm_grader hallucination_grader.invoke({\"documents\": docs, \"generation\": generation}) Out[16]: <pre>GradeHallucinations(binary_score='no')</pre> In\u00a0[17]: Copied! <pre>### Answer Grader\n\n\n# Data model\nclass GradeAnswer(BaseModel):\n    \"\"\"Binary score to assess answer addresses question.\"\"\"\n\n    binary_score: str = Field(\n        description=\"Answer addresses the question, 'yes' or 'no'\"\n    )\n\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nstructured_llm_grader = llm.with_structured_output(GradeAnswer)\n\n# Prompt\nsystem = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\nanswer_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n    ]\n)\n\nanswer_grader = answer_prompt | structured_llm_grader\nanswer_grader.invoke({\"question\": question, \"generation\": generation})\n</pre> ### Answer Grader   # Data model class GradeAnswer(BaseModel):     \"\"\"Binary score to assess answer addresses question.\"\"\"      binary_score: str = Field(         description=\"Answer addresses the question, 'yes' or 'no'\"     )   # LLM with function call llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) structured_llm_grader = llm.with_structured_output(GradeAnswer)  # Prompt system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n      Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\" answer_prompt = ChatPromptTemplate.from_messages(     [         (\"system\", system),         (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),     ] )  answer_grader = answer_prompt | structured_llm_grader answer_grader.invoke({\"question\": question, \"generation\": generation}) Out[17]: <pre>GradeAnswer(binary_score='no')</pre> In\u00a0[18]: Copied! <pre>### Question Re-writer\n\n# LLM\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n# Prompt\nsystem = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\nre_write_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\n            \"human\",\n            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n        ),\n    ]\n)\n\nquestion_rewriter = re_write_prompt | llm | StrOutputParser()\nquestion_rewriter.invoke({\"question\": question})\n</pre> ### Question Re-writer  # LLM llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # Prompt system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n      for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\" re_write_prompt = ChatPromptTemplate.from_messages(     [         (\"system\", system),         (             \"human\",             \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",         ),     ] )  question_rewriter = re_write_prompt | llm | StrOutputParser() question_rewriter.invoke({\"question\": question}) Out[18]: <pre>'2023\ub144 \ud55c\uad6d \ud504\ub85c\uc57c\uad6c\uc758 \uc6b0\uc2b9\ud300\uc740 \ub204\uad6c\uc778\uac00\uc694?'</pre> In\u00a0[19]: Copied! <pre>### Search\n\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\nweb_search_tool = TavilySearchResults(k=3)\n</pre> ### Search  from langchain_community.tools.tavily_search import TavilySearchResults  web_search_tool = TavilySearchResults(k=3) In\u00a0[20]: Copied! <pre>from typing import List\n\nfrom typing_extensions import TypedDict\n\n\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        question: question\n        generation: LLM generation\n        documents: list of documents\n    \"\"\"\n\n    question: str\n    generation: str\n    documents: List[str]\n</pre> from typing import List  from typing_extensions import TypedDict   class GraphState(TypedDict):     \"\"\"     Represents the state of our graph.      Attributes:         question: question         generation: LLM generation         documents: list of documents     \"\"\"      question: str     generation: str     documents: List[str] In\u00a0[21]: Copied! <pre>from langchain.schema import Document\n\n\ndef retrieve(state):\n    \"\"\"\n    Retrieve documents\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, documents, that contains retrieved documents\n    \"\"\"\n    print(\"---RETRIEVE---\")\n    question = state[\"question\"]\n\n    # Retrieval\n    documents = retriever.invoke(question)\n    return {\"documents\": documents, \"question\": question}\n\n\ndef generate(state):\n    \"\"\"\n    Generate answer\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation, that contains LLM generation\n    \"\"\"\n    print(\"---GENERATE---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # RAG generation\n    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n\n\ndef grade_documents(state):\n    \"\"\"\n    Determines whether the retrieved documents are relevant to the question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Updates documents key with only filtered relevant documents\n    \"\"\"\n\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # Score each doc\n    filtered_docs = []\n    for d in documents:\n        score = retrieval_grader.invoke(\n            {\"question\": question, \"document\": d.page_content}\n        )\n        grade = score.binary_score\n        if grade == \"yes\":\n            print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        else:\n            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            continue\n    return {\"documents\": filtered_docs, \"question\": question}\n\n\ndef transform_query(state):\n    \"\"\"\n    Transform the query to produce a better question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Updates question key with a re-phrased question\n    \"\"\"\n\n    print(\"---TRANSFORM QUERY---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # Re-write question\n    better_question = question_rewriter.invoke({\"question\": question})\n    return {\"documents\": documents, \"question\": better_question}\n\n\ndef web_search(state):\n    \"\"\"\n    Web search based on the re-phrased question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Updates documents key with appended web results\n    \"\"\"\n\n    print(\"---WEB SEARCH---\")\n    question = state[\"question\"]\n\n    # Web search\n    docs = web_search_tool.invoke({\"query\": question})\n    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n    web_results = Document(page_content=web_results)\n\n    return {\"documents\": web_results, \"question\": question}\n\n\n### Edges ###\n\n\ndef route_question(state):\n    \"\"\"\n    Route question to web search or RAG.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n\n    print(\"---ROUTE QUESTION---\")\n    question = state[\"question\"]\n    source = question_router.invoke({\"question\": question})\n    if source.datasource == \"web_search\":\n        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n        return \"web_search\"\n    elif source.datasource == \"vectorstore\":\n        print(\"---ROUTE QUESTION TO RAG---\")\n        return \"vectorstore\"\n\n\ndef decide_to_generate(state):\n    \"\"\"\n    Determines whether to generate an answer, or re-generate a question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Binary decision for next node to call\n    \"\"\"\n\n    print(\"---ASSESS GRADED DOCUMENTS---\")\n    state[\"question\"]\n    filtered_documents = state[\"documents\"]\n\n    if not filtered_documents:\n        # All documents have been filtered check_relevance\n        # We will re-generate a new query\n        print(\n            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n        )\n        return \"transform_query\"\n    else:\n        # We have relevant documents, so generate answer\n        print(\"---DECISION: GENERATE---\")\n        return \"generate\"\n\n\ndef grade_generation_v_documents_and_question(state):\n    \"\"\"\n    Determines whether the generation is grounded in the document and answers question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Decision for next node to call\n    \"\"\"\n\n    print(\"---CHECK HALLUCINATIONS---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    generation = state[\"generation\"]\n\n    score = hallucination_grader.invoke(\n        {\"documents\": documents, \"generation\": generation}\n    )\n    grade = score.binary_score\n\n    # Check hallucination\n    if grade == \"yes\":\n        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n        # Check question-answering\n        print(\"---GRADE GENERATION vs QUESTION---\")\n        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n        grade = score.binary_score\n        if grade == \"yes\":\n            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n            return \"useful\"\n        else:\n            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n            return \"not useful\"\n    else:\n        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n        return \"not supported\"\n</pre> from langchain.schema import Document   def retrieve(state):     \"\"\"     Retrieve documents      Args:         state (dict): The current graph state      Returns:         state (dict): New key added to state, documents, that contains retrieved documents     \"\"\"     print(\"---RETRIEVE---\")     question = state[\"question\"]      # Retrieval     documents = retriever.invoke(question)     return {\"documents\": documents, \"question\": question}   def generate(state):     \"\"\"     Generate answer      Args:         state (dict): The current graph state      Returns:         state (dict): New key added to state, generation, that contains LLM generation     \"\"\"     print(\"---GENERATE---\")     question = state[\"question\"]     documents = state[\"documents\"]      # RAG generation     generation = rag_chain.invoke({\"context\": documents, \"question\": question})     return {\"documents\": documents, \"question\": question, \"generation\": generation}   def grade_documents(state):     \"\"\"     Determines whether the retrieved documents are relevant to the question.      Args:         state (dict): The current graph state      Returns:         state (dict): Updates documents key with only filtered relevant documents     \"\"\"      print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")     question = state[\"question\"]     documents = state[\"documents\"]      # Score each doc     filtered_docs = []     for d in documents:         score = retrieval_grader.invoke(             {\"question\": question, \"document\": d.page_content}         )         grade = score.binary_score         if grade == \"yes\":             print(\"---GRADE: DOCUMENT RELEVANT---\")             filtered_docs.append(d)         else:             print(\"---GRADE: DOCUMENT NOT RELEVANT---\")             continue     return {\"documents\": filtered_docs, \"question\": question}   def transform_query(state):     \"\"\"     Transform the query to produce a better question.      Args:         state (dict): The current graph state      Returns:         state (dict): Updates question key with a re-phrased question     \"\"\"      print(\"---TRANSFORM QUERY---\")     question = state[\"question\"]     documents = state[\"documents\"]      # Re-write question     better_question = question_rewriter.invoke({\"question\": question})     return {\"documents\": documents, \"question\": better_question}   def web_search(state):     \"\"\"     Web search based on the re-phrased question.      Args:         state (dict): The current graph state      Returns:         state (dict): Updates documents key with appended web results     \"\"\"      print(\"---WEB SEARCH---\")     question = state[\"question\"]      # Web search     docs = web_search_tool.invoke({\"query\": question})     web_results = \"\\n\".join([d[\"content\"] for d in docs])     web_results = Document(page_content=web_results)      return {\"documents\": web_results, \"question\": question}   ### Edges ###   def route_question(state):     \"\"\"     Route question to web search or RAG.      Args:         state (dict): The current graph state      Returns:         str: Next node to call     \"\"\"      print(\"---ROUTE QUESTION---\")     question = state[\"question\"]     source = question_router.invoke({\"question\": question})     if source.datasource == \"web_search\":         print(\"---ROUTE QUESTION TO WEB SEARCH---\")         return \"web_search\"     elif source.datasource == \"vectorstore\":         print(\"---ROUTE QUESTION TO RAG---\")         return \"vectorstore\"   def decide_to_generate(state):     \"\"\"     Determines whether to generate an answer, or re-generate a question.      Args:         state (dict): The current graph state      Returns:         str: Binary decision for next node to call     \"\"\"      print(\"---ASSESS GRADED DOCUMENTS---\")     state[\"question\"]     filtered_documents = state[\"documents\"]      if not filtered_documents:         # All documents have been filtered check_relevance         # We will re-generate a new query         print(             \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"         )         return \"transform_query\"     else:         # We have relevant documents, so generate answer         print(\"---DECISION: GENERATE---\")         return \"generate\"   def grade_generation_v_documents_and_question(state):     \"\"\"     Determines whether the generation is grounded in the document and answers question.      Args:         state (dict): The current graph state      Returns:         str: Decision for next node to call     \"\"\"      print(\"---CHECK HALLUCINATIONS---\")     question = state[\"question\"]     documents = state[\"documents\"]     generation = state[\"generation\"]      score = hallucination_grader.invoke(         {\"documents\": documents, \"generation\": generation}     )     grade = score.binary_score      # Check hallucination     if grade == \"yes\":         print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")         # Check question-answering         print(\"---GRADE GENERATION vs QUESTION---\")         score = answer_grader.invoke({\"question\": question, \"generation\": generation})         grade = score.binary_score         if grade == \"yes\":             print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")             return \"useful\"         else:             print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")             return \"not useful\"     else:         pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")         return \"not supported\" In\u00a0[22]: Copied! <pre>from langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"web_search\", web_search)  # web search\nworkflow.add_node(\"retrieve\", retrieve)  # retrieve\nworkflow.add_node(\"grade_documents\", grade_documents)  # grade documents\nworkflow.add_node(\"generate\", generate)  # generatae\nworkflow.add_node(\"transform_query\", transform_query)  # transform_query\n\n# Build graph\nworkflow.add_conditional_edges(\n    START,\n    route_question,\n    {\n        \"web_search\": \"web_search\",\n        \"vectorstore\": \"retrieve\",\n    },\n)\nworkflow.add_edge(\"web_search\", \"generate\")\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\nworkflow.add_conditional_edges(\n    \"grade_documents\",\n    decide_to_generate,\n    {\n        \"transform_query\": \"transform_query\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"transform_query\", \"retrieve\")\nworkflow.add_conditional_edges(\n    \"generate\",\n    grade_generation_v_documents_and_question,\n    {\n        \"not supported\": \"generate\",\n        \"useful\": END,\n        \"not useful\": \"transform_query\",\n    },\n)\n\n# Compile\napp = workflow.compile()\n</pre> from langgraph.graph import END, StateGraph, START  workflow = StateGraph(GraphState)  # Define the nodes workflow.add_node(\"web_search\", web_search)  # web search workflow.add_node(\"retrieve\", retrieve)  # retrieve workflow.add_node(\"grade_documents\", grade_documents)  # grade documents workflow.add_node(\"generate\", generate)  # generatae workflow.add_node(\"transform_query\", transform_query)  # transform_query  # Build graph workflow.add_conditional_edges(     START,     route_question,     {         \"web_search\": \"web_search\",         \"vectorstore\": \"retrieve\",     }, ) workflow.add_edge(\"web_search\", \"generate\") workflow.add_edge(\"retrieve\", \"grade_documents\") workflow.add_conditional_edges(     \"grade_documents\",     decide_to_generate,     {         \"transform_query\": \"transform_query\",         \"generate\": \"generate\",     }, ) workflow.add_edge(\"transform_query\", \"retrieve\") workflow.add_conditional_edges(     \"generate\",     grade_generation_v_documents_and_question,     {         \"not supported\": \"generate\",         \"useful\": END,         \"not useful\": \"transform_query\",     }, )  # Compile app = workflow.compile() In\u00a0[24]: Copied! <pre>from pprint import pprint\n\n# Run\ninputs = {\n    \"question\": \"2024\ub144 KBO \ud55c\uad6d \ud504\ub85c\uc57c\uad6c \uc6b0\uc2b9\ud300 \ub204\uad74\uae4c?\"\n}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        # Node\n        pprint(f\"Node '{key}':\")\n        # Optional: print full state at each node\n        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n    pprint(\"\\n---\\n\")\n\n# Final generation\npprint(value[\"generation\"])\n</pre> from pprint import pprint  # Run inputs = {     \"question\": \"2024\ub144 KBO \ud55c\uad6d \ud504\ub85c\uc57c\uad6c \uc6b0\uc2b9\ud300 \ub204\uad74\uae4c?\" } for output in app.stream(inputs):     for key, value in output.items():         # Node         pprint(f\"Node '{key}':\")         # Optional: print full state at each node         # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)     pprint(\"\\n---\\n\")  # Final generation pprint(value[\"generation\"]) <pre>---ROUTE QUESTION---\n---ROUTE QUESTION TO WEB SEARCH---\n---WEB SEARCH---\n\"Node 'web_search':\"\n'\\n---\\n'\n---GENERATE---\n---CHECK HALLUCINATIONS---\n---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n---GRADE GENERATION vs QUESTION---\n---DECISION: GENERATION ADDRESSES QUESTION---\n\"Node 'generate':\"\n'\\n---\\n'\n('2024\ub144 KBO \ud55c\uad6d \ud504\ub85c\uc57c\uad6c \uc6b0\uc2b9\ud300\uc5d0 \ub300\ud55c \uc608\uce21\uc740 LG \ud2b8\uc708\uc2a4\uac00 \uac00\uc7a5 \uc720\ub825\ud558\ub2e4\uace0 \ud569\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc2e4\uc81c \uc6b0\uc2b9\ud300\uc740 \uc2dc\uc98c\uc774 \uc9c4\ud589\ub41c \ud6c4\uc5d0\uc57c '\n '\ud655\uc815\ub420 \uac83\uc785\ub2c8\ub2e4. \ud604\uc7ac\ub85c\uc11c\ub294 \uc815\ud655\ud55c \ub2f5\ubcc0\uc744 \ub4dc\ub9b4 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.')\n</pre> <p>Trace:</p> <p>https://smith.langchain.com/public/1a80c8ad-444f-4924-899e-b5ee49637934/r</p> In\u00a0[25]: Copied! <pre># Run\ninputs = {\"question\": \"\ub7ad\uc2a4\ubbf8\uc2a4\ub97c \uc4f0\uba74 \uc7a5\uc810\uc774 \ubb50\uc57c?\"}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        # Node\n        pprint(f\"Node '{key}':\")\n        # Optional: print full state at each node\n        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n    pprint(\"\\n---\\n\")\n\n# Final generation\npprint(value[\"generation\"])\n</pre> # Run inputs = {\"question\": \"\ub7ad\uc2a4\ubbf8\uc2a4\ub97c \uc4f0\uba74 \uc7a5\uc810\uc774 \ubb50\uc57c?\"} for output in app.stream(inputs):     for key, value in output.items():         # Node         pprint(f\"Node '{key}':\")         # Optional: print full state at each node         # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)     pprint(\"\\n---\\n\")  # Final generation pprint(value[\"generation\"]) <pre>---ROUTE QUESTION---\n---ROUTE QUESTION TO RAG---\n---RETRIEVE---\n\"Node 'retrieve':\"\n'\\n---\\n'\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\n---GRADE: DOCUMENT NOT RELEVANT---\n---GRADE: DOCUMENT NOT RELEVANT---\n---GRADE: DOCUMENT RELEVANT---\n---GRADE: DOCUMENT RELEVANT---\n---ASSESS GRADED DOCUMENTS---\n---DECISION: GENERATE---\n\"Node 'grade_documents':\"\n'\\n---\\n'\n---GENERATE---\n---CHECK HALLUCINATIONS---\n---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n---GRADE GENERATION vs QUESTION---\n---DECISION: GENERATION ADDRESSES QUESTION---\n\"Node 'generate':\"\n'\\n---\\n'\n('\ub7ad\uc2a4\ubbf8\uc2a4\ub97c \uc0ac\uc6a9\ud558\uba74 LLM \uc5b4\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \uc131\ub2a5\uc744 \ubaa8\ub2c8\ud130\ub9c1\ud558\uace0, \uc0ac\uc6a9\uc790 \ud53c\ub4dc\ubc31\uc744 \uc218\uc9d1\ud558\uc5ec \uac1c\uc120\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \ubc84\uadf8 \ucd94\uc801\uacfc \uc751\ub2f5 '\n '\uc18d\ub3c4 \ubd84\uc11d\uc744 \ud1b5\ud574 \uc6b4\uc601 \uc911 \ubc1c\uc0dd\ud560 \uc218 \uc788\ub294 \ubb38\uc81c\ub97c \uc2e0\uc18d\ud558\uac8c \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uae30\ub2a5\ub4e4\uc740 A/B \ud14c\uc2a4\ud2b8\uc640 \uac19\uc740 \ubc29\ubc95\uc73c\ub85c '\n '\uc5b4\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \ud488\uc9c8\uc744 \ub192\uc774\ub294 \ub370 \uae30\uc5ec\ud569\ub2c8\ub2e4.')\n</pre> <p>Trace:</p> <p>https://smith.langchain.com/public/42b2be5c-6c36-4019-a163-25fb4390e55d/r</p>"},{"location":"tutorial/4_langgraph_adaptive_rag/#adaptive-rag","title":"Adaptive RAG\u00b6","text":"<p>RAG \ub97c \uc798\ud558\uae30 \uc704\ud55c \ubc29\ubc95\uc785\ub2c8\ub2e4.</p> <ol> <li>query analysis \ub97c \ud574\uc11c RAG\uac00 \ud544\uc694\ud55c\uc9c0 \uba3c\uc800 \ud310\ub2e8\ud558\uace0,</li> <li>active / self-corrective RAG \ub97c \uc774\uc6a9\ud574\uc11c RAG \uc798!! \ud569\ub2c8\ub2e4.</li> </ol> <p>paper\ub97c \ucc38\uace0\ud558\uc2dc\uba74, 3\uac00\uc9c0 \ub8e8\ud2b8\ub85c \ub3d9\uc801\uc73c\ub85c \uad6c\ud604\ud558\ub294 \ubc29\ubc95\ub860\uc774 \uc798 \uc18c\uac1c\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.</p> <ul> <li>No Retrieval</li> <li>Single-shot RAG</li> <li>Iterative RAG</li> </ul> <p>LangGraph \ub85c \uad6c\ud604\uc744 \ud560 \uac83\uc774\uace0\uc694, \uc544\ub798 \ub450\uac00\uc9c0 \ub8e8\ud2b8\ub85c \ub098\ub220\uc11c \ubcf4\ub0bc \uac83\uc785\ub2c8\ub2e4.</p> <ul> <li>Web search</li> <li>Self-corrective RAG</li> </ul>"},{"location":"tutorial/4_langgraph_adaptive_rag/#setup","title":"Setup\u00b6","text":"<p>\ud544\uc694\ud55c \ud328\ud0a4\uc9c0 \uc124\uce58\ud558\uace0 API \ud0a4 \uc14b\uc5c5 \ud558\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/4_langgraph_adaptive_rag/#create-index","title":"Create Index\u00b6","text":"<p>OpenAIEmbedding \uacfc Chroma vectordb \ub97c \uc774\uc6a9\ud574\uc11c \uc6f9\ubb38\uc11c\ub4e4\uc744 \uc900\ube44\ud569\ub2c8\ub2e4. RAG\ub97c \uc704\ud55c \ub370\uc774\ud130 \uc900\ube44 \uc785\ub2c8\ub2e4.</p>"},{"location":"tutorial/4_langgraph_adaptive_rag/#llms","title":"LLMs\u00b6","text":""},{"location":"tutorial/4_langgraph_adaptive_rag/#routing-query-analysis","title":"Routing - Query Analysis\u00b6","text":"<p>Routing \ubd80\ud130 \uc2dc\uc791\ud569\ub2c8\ub2e4. LLM \uc5d0\uac8c \uba3c\uc800 query analysis\ub97c \ub9e1\uae41\ub2c8\ub2e4.</p> <p>RouteQuery DataModel\uc744 \ub9cc\ub4e4\uc5b4 LLM \uc5d0\uac8c structured llm \ud3ec\ub9f7\uc73c\ub85c \uc9c0\uc815\ud574\uc90d\ub2c8\ub2e4. Route \uacb0\uc815\uc740 Prompt \uc5d0 \ub123\uc5b4\uc90d\ub2c8\ub2e4. \uc5ec\uae30\uc5d0 \ubb38\uc11c\uc5d0 \ub300\ud55c \ub0b4\uc6a9, \uc5b4\ub5a4 \uc8fc\uc81c\ub294 RAG \ub85c \uac00\uac8c \ub9cc\ub4e4 \uac83\uc778\uc9c0 \uc798 \uc9c0\uc815\ud574\uc57c\ud569\ub2c8\ub2e4.</p> <p>\uc790\ub3d9\uc73c\ub85c \ud558\uaca0\ub2e4\uba74 RAG \ubb38\uc11c\ub4e4\uc744 \ub2e4\uc2dc \uc694\uc57d\ud574 \ub0b4\ub294 \ubc29\ubc95\ub3c4 \uc788\uaca0\uc9c0\ub9cc, \ubb38\uc11c\uac00 \ucee4\uc9c0\uba74 \ube44\uc6a9\uc774 \ub9ce\uc774 \ub4dc\ub2c8 \uc190\uc73c\ub85c \uc7a1\uc544\uc8fc\ub294 \uac83\uc774 \uc88b\uaca0\ub124\uc694.</p>"},{"location":"tutorial/4_langgraph_adaptive_rag/#retreival-grader","title":"Retreival Grader\u00b6","text":"<p>Retrieve \ub97c \ud574\ubcf4\uace0, \uc810\uc218\ub97c \ub9e4\uae41\ub2c8\ub2e4. \uc55e\uc11c Query\ub97c \ubcf4\uace0 RAG \ub85c \ub3cc\ub9ac\uaca0\ub2e4 \uacb0\uc815\uc744 \ud588\uc9c0\ub9cc, \uc2e4\uc81c Retreived \ub41c \ubb38\uc11c\uac00 \ub9c8\uc74c\uc5d0 \uc548\ub4e4 \uc218\ub3c4 \uc788\uc8e0. Retreived \ubb38\uc11c\uc640 Query \uac00 \ucda9\ubd84\ud788 \ubb38\uc11c\uac00 \uad00\ub828\uc774 \uc788\ub294\uc9c0 \uccb4\ud06c\ub97c \ud574\ubd05\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 \ub2e4\uc2dc LLM \uc5d0\uac8c \ub9e1\uae30\uace0, \ud3c9\uac00\ub294 binary\ub85c 'yes' or 'no' \ub85c \ud310\ub2e8\ud569\ub2c8\ub2e4.</p>"},{"location":"tutorial/4_langgraph_adaptive_rag/#generate","title":"Generate\u00b6","text":"<p>\uc774\uc81c \ud1b5\uc0c1\uc801\uc778 RAG, Retreived \ub41c \ubb38\uc11c\uc640 \ud568\uaed8 \ub300\ub2f5\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \uac04\ub2e8\ud558\uac8c LCEL\ub85c \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/4_langgraph_adaptive_rag/","title":"\ud658\uac01 \ud3c9\uac00\u00b6","text":"<p>\ud639\uc2dc Hallucination\uc774 \uc77c\uc5b4\ub098\uc9c0 \uc54a\uc558\ub294\uc9c0 \ub2e4\uc2dc \ud55c\ubc88 \ud3c9\uac00\ud569\ub2c8\ub2e4. \uc5ed\uc2dc\ub098 binary\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4.</p>"},{"location":"tutorial/4_langgraph_adaptive_rag/","title":"\ub2f5\ubcc0 \ud3c9\uac00\u00b6","text":"<p>\ucd5c\uc885\uc801\uc73c\ub85c \ub2f5\ubcc0\ub3c4 \ud3c9\uac00\ud569\ub2c8\ub2e4.</p>"},{"location":"tutorial/4_langgraph_adaptive_rag/#question","title":"Question \uc7ac\uc791\uc131\u00b6","text":"<p>\uc9c8\ubb38\uc744 \uadf8\ub798\ub3c4 RAG\uc5d0 \ud65c\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uadf8\ub7f0\ub370.... \uc0ac\uc6a9\uc790\uc758 \uc9c8\ubb38\uc740 RAG \uc5d0 \uc801\ud569\ud558\uc9c0 \uc54a\uc740 \ud615\ud0dc\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub354 \uc88b\uc740 retrieval \uc744 \uc704\ud574\uc11c \uc9c8\ubb38\uc744 \ud574\uc11d\ud558\uc5ec \uc798 vector simlarity \uc11c\uce58\uac00 \ub418\ub3c4\ub85d \ub2e4\uc2dc \uc791\uc131\ud574\uc90d\ub2c8\ub2e4.</p>"},{"location":"tutorial/4_langgraph_adaptive_rag/#web-search-tool","title":"Web Search Tool\u00b6","text":"<p>\uc6f9\uc73c\ub85c \uac80\uc0c9\ud558\ub294 \uacbd\uc6b0\ub97c \uc704\ud574 tavily \ub3c4\uad6c\ub97c \uc14b\ud305\ud574\uc90d\ub2c8\ub2e4.</p>"},{"location":"tutorial/4_langgraph_adaptive_rag/#construct-the-graph","title":"Construct the Graph\u00b6","text":"<p>\uc704 \ubaa8\ub4e0 \uae30\ub2a5\ub4e4\uc744 \ud1b5\ud569\ud574\uc11c graph\ub97c \uc791\uc131\ud558\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tutorial/4_langgraph_adaptive_rag/#define-graph-state","title":"Define Graph State\u00b6","text":""},{"location":"tutorial/4_langgraph_adaptive_rag/#define-graph-flow","title":"Define Graph Flow\u00b6","text":""},{"location":"tutorial/4_langgraph_adaptive_rag/#compile-graph","title":"Compile Graph\u00b6","text":""},{"location":"tutorial/4_langgraph_adaptive_rag/#use-graph","title":"Use Graph\u00b6","text":""}]}